<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
                          _   _
     _  _    _     _  _  | | | |   __     ____
    | |/ \  | |_  | |/_| | | | |  /  \   /    \
    | |   | |  _| |  /   | | | | | || | |  ||  |
    | |   | | |_  | |    | | | | | || | |  ||  |
    |_|   | |___| |_|    |_| |_|  \__/   \____/|
                                               |
                                          ____/
    -->
    <title>ntrllog | Computer Networking</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
    <link rel="shortcut icon" type="image/png" href="../pictures/favicon.ico"/>
    <link href="../css/content.css" rel="stylesheet">
  </head>
  <body>
    <div class="dropdown ln-fixed-right">
      <button class="btn btn-secondary dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">ToC</button>
      <ul class="dropdown-menu">
        <li><a class="dropdown-item" href="#whatisit">What is the Internet?</a></li>
        <li><a class="dropdown-item" href="#networkedge">Livin' on the Edge</a></li>
        <li><a class="dropdown-item" href="#networkcore">The Network Core</a></li>
        <li><a class="dropdown-item" href="#delay">Delay, Loss, and Throughput</a></li>
        <li><a class="dropdown-item" href="#protocollayers">Protocol Layers</a></li>
        <li><a class="dropdown-item" href="#networkapplications">Principles of Network Applications</a></li>
        <li><a class="dropdown-item" href="#http">The Web and HTTP</a></li>
        <li><a class="dropdown-item" href="#email">Email</a></li>
        <li><a class="dropdown-item" href="#dns">DNS</a></li>
        <li><a class="dropdown-item" href="#p2p">Peer-to-Peer File Distribution</a></li>
        <li><a class="dropdown-item" href="#cdn">Video Streaming and Content Distribution Networks</a></li>
        <li><a class="dropdown-item" href="#transportlayer">Transport-Layer Services</a></li>
        <li><a class="dropdown-item" href="#multiplexing">Multiplexing and Demultiplexing</a></li>
        <li><a class="dropdown-item" href="#udp">Connectionless Transport: UDP</a></li>
        <li><a class="dropdown-item" href="#rdt">Principles of Reliable Data Transfer</a></li>
        <li><a class="dropdown-item" href="#tcp">Connection-Oriented Transport: TCP</a></li>
        <li><a class="dropdown-item" href="#congestioncontrol">TCP Congestion Control</a></li>
        <li><a class="dropdown-item" href="#networklayer">Overview of Network Layer</a></li>
        <li><a class="dropdown-item" href="#routingalgorithms">Routing Algorithms</a></li>
        <li><a class="dropdown-item" href="#ospf">Intra-AS Routing in the Internet: OSPF</a></li>
        <li><a class="dropdown-item" href="#bgp">Routing Among the ISPs: BGP</a></li>
      </ul>
    </div>
    <div class="container ln-line-height">
      <a href="projects.html"><i class="fas fa-long-arrow-alt-left fa-2x"></i></a>
      <h1>Computer Networking</h1>
      <hr>
      <p>Shortcut to this page: <a href="networking.html">ntrllog.netlify.app/networking</a></p>
      <p>Info provided by <a href="https://gaia.cs.umass.edu/kurose_ross/index.php" target="_blank">Computer Networking: A Top-Down Approach</a> and Professor Polly Huang</p>
      <h2 id="whatisit">What is the Internet?</h2>
      <p>Computers. Laptops. Smartphones. TVs. Watches. And everything else that has the word "smart" in front of it. All of these things that can connect to the Internet are called <b>hosts</b> or <b>end systems</b>. End systems send and receive data through a system of communcation links and packet switches. The data travels through this system as little pieces called packets and they are reassembled when they reach their destination. Packet switches are machines that help transfer these packets. The whole path that a packet takes is called a <b>route</b> or <b>path</b>.</p>
      <div class="ln-box">
        <p>A <b>router</b> is a type of packet switch. The two terms may be used interchangeably (by me).</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/nuts_and_bolts.png">
      </div>
      <div class="ln-box">
        <p>I believe the shape I'll be using for all packet switches is a circle/oval. And links will be just a line connecting the circles.</p>
      </div>
      <p>Access to the Internet is provided by <b>Internet Service Providers (ISPs)</b>. Think ATT and Spectrum for example. ISPs themselves are a network of packet switches and communication links.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/isp.png">
      </div>
      <h3>What is a Protocol?</h3>
      <p>All the devices involved with sending and receiving packets run <b>protocols</b>, which are a set of rules defining:</p>
      <ul>
        <li>how messages should be formatted</li>
        <li>the order of the messages exchanged</li>
        <li>the actions that can be performed when the messages are received or transmitted</li>
      </ul>
      <p>The protocols are defined by <b>Internet standards</b>, which are developed by the Internet Engineering Task Force (IETF). The actual documents are called <b>requests for comments (RFCs)</b>.</p>
      <div class="ln-box">
        <p>When we go to a URL on our browser, the actions that occur are a result of following a protocol (the TCP protocol).</p>
        <ol>
          <li>Computer sends a request to connect to the Web server and waits for a reply</li>
          <li>Web server receives the connection request and returns a "connection okay" message</li>
          <li>Computer sends the name of the Web page we want</li>
          <li>Web server sends it to our computer</li>
        </ol>
      </div>
      <h2 id="networkedge">Livin' on the Edge</h2>
      <p>The word "end system" is used to describe devices that connect to the Internet because they exist at the "edge" of the Internet.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/network_edge.png">
      </div>
      <h3>Access Networks</h3>
      <p>The access network is the network where end systems connect to the first router in the path to the ISP. (There may be many routers in between the ISP and a home. The first router is the one that's actually in the home.) Basically, an access network is a network on the edge of the Internet.</p>
      <h4>Home Access: What's the WiFi Password?</h4>
      <p>The home network will probably be the easiest for everyone to relate to. All our phones, laptops, tablets, computers, and everything else that connect to the WiFi in our homes form a home network.</p>
      <p>Two of the most popular ways to get Internet access are by a <b>digital subscriber line (DSL)</b> (think telephone lines and telephone companies) and cable.</p>
      <h4>Wide-Area Wireless Access: Unlimited Talk, Text, and Web For Only $ Per Month!</h4>
      <p>When our phones aren't connected to WiFi, it uses mobile data, which is Internet provided by cellular network providers through base stations (cell towers?). This is where the terms 3G, 4G, 5G, and LTE come into play. (6G doesn't exist yet at the time of this writing.)</p>
      <h2 id="networkcore">The Network Core</h2>
      <p>The network core is, well, the part of the Internet that is not at the edge. This is the part of the Internet that our router connects to â€” out to where our Internet access comes from.</p>
      <h3>Packet Switching</h3>
      <p>The data that the end systems send and receive are called <b>messages</b>. Messages can be any type of data, such as emails, pictures, or audio files. When going from their source to their destination, messages are broken up into packets, which are transferred from packet switch to packet switch. From packet switch to packet switch, each packet is broken up into bits and sent across the communication link connecting the two packet switches.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/packet_switching.png">
      </div>
      <h4>No Packet Left Behind</h4>
      <p>Most packet switches use <b>store-and-forward transmission</b>. This means that the packet switch waits until it receives the entire packet before it starts sending the first bit of the packet to the next location.</p>
      <div class="ln-box">
        <p>Consider a simple network of two end systems and one packet switch. The source has three packets to send to the destination.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/packet_switching_2.png">
        </div>
        <p>Let's say each packet has `L` bits and that each communication link can transfer `R` bits per second. Then the total time it takes to transfer one packet across one communication link is `L/R` seconds.</p>
        <p>So the time it takes to transfer the packet from source to destination is `2L/R`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/packet_switching_3.png">
        </div>
        <p>How long does it take to transfer all three packets from source to destination? At `L/R` seconds, the packet switch receives all of the first packet. At this time, while the packet switch starts sending bits of the first packet to the destination, it also starts receiving bits of the second packet from the source.</p>
        <p>At `2L/R` seconds, the first packet arrives at the destination and the second packet arrives at the packet switch.</p>
        <p>At `3L/R` seconds, the second packet arrives at the destination and the third packet arrives at the packet switch.</p>
        <p>At `4L/R` seconds, the third packet arrives at the destination.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/packet_switching.gif">
        </div>
        <p>Let's generalize this for `N` communication links and `P` packets.</p>
        <p>Since the time it takes for one packet to travel across one communication link is `L/R`, it takes one packet `NL/R` time to travel across `N` communication links.</p>
        <p>At time `NL/R`, the first packet will reach the destination. If there are `P` packets in total, then there will be `P-1` more times where there is a packet sitting in the last packet switch (i.e., there will be `P-1` more packets passing through the last packet switch). Since it takes `L/R` time to go from the last packet switch to the destination, it will take `(P-1)L/R` time for all the packets to reach the destination. So the total time it takes `P` packets to travel across `N` communication links is `NL/R+(P-1)L/R=(N+P-1)L/R`.</p>
      </div>
      <div class="ln-box">
        <p>Besides store-and-forward, there's also cut-through switching, in which the switch starts sending bits before the whole packet is received.</p>
      </div>
      <h4>Queuing Delays</h4>
      <p>Realistically, a packet switch has more than two links. For each link in the packet switch, there is an <b>output buffer</b> (a.k.a. <b>output queue</b>) where the packets wait if the link is busy. This can happen if the transmission rate of the link is slower than the arrival rate of the packets, i.e., the packets are coming in faster than the packet switch can send them out.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/output_buffer.png">
      </div>
      <h4>Okay, Some Packets Left Behind</h4>
      <p>Since the buffer can only be so big, if there isn't enough room for more incoming packets, some packets will be dropped, resulting in <b>packet loss</b>. Depending on the implementation, either the arriving packet or one of the packets in the buffer is dropped.</p>
      <h4>Forwarding Tables</h4>
      <p>Again, packet switches usually have multiple links. So how does the packet switch know which link to use to send the packet? The answer is that each packet switch has a <b>forwarding table</b> that lists the destination of each link. When a packet arrives, the packet switch examines the packet to see its destination and searches it up in its forwarding table. The table tells the packet switch which link to use to get to the destination.</p>
      <h3>Circuit Switching</h3>
      <p>With packet switching, the packets are sent and if there is a lot of traffic, then oh well. Maybe the data will take a long time to send, or worse, get dropped. To avoid this, we can reserve the resources ahead of time. This way, one connection won't be competing with other connections, i.e., the connection will have the links to itself. (Imagine being the only one on the road and no other cars are allowed to be on the road with you!)</p>
      <p>With circuit switching, each link has a number of circuits. One circuit per link is required for communication between two end systems. So a link with, for example, four circuits can support four different connections at the same time. However, each connection only gets a fraction of the link's transmission rate.</p>
      <div class="ln-box">
        <p>Suppose we have a link with four circuits and a transmission rate of `1` Mbps. Then each circuit can only transfer data at a rate of `1/4` Mbps, which is `250` kbps.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/circuit_switching.png">
      </div>
      <h4>Frequency-Division Multiplexing</h4>
      <p>Multiplexing is sending multiple signals as one big signal.</p>
      <p>Multiple end systems can all use one link to transfer data if there are enough circuits. Each end system will be transferring different data, but they all get transferred over as "one big data" through the link. The way to differentiate which data came from which end system is to divide the link into different frequencies, like the example above.</p>
      <div class="ln-box">
        <p>Radio uses FDM. We can tune in to a specific frequency to listen to the station we want.</p>
      </div>
      <h4>Time-Division Multiplexing</h4>
      <p>With time-division multiplexing, the end systems get to use all of the frequency to transfer data, but only for a limited amount of time.</p>
      <div class="ln-box">
        <p>Suppose there are two end systems (`A` and `B`) communicating with two other end systems using the same link. Let's say they're only allowed `1`-second intervals to send their data. So from `0` to `1` seconds, part of the data from `A` and part of the data from `B` gets sent. From `1` to `2` seconds, part of the data from `A` and part of the data from `B` gets sent. And this repeats for every second.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/tdm.png">
        </div>
      </div>
      <p>More formally, time is divided into frames, and each frame is divided into slots. Once a connection between two end systems is established, one slot in every frame is reserved just for that connection.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/tdm_2.png">
      </div>
      <div class="ln-box">
        <p>If a link transmits `8000` frames per second and each slot consists of `8` bits, then the transmission rate of each circuit is `8000*8=64,000` bits per second `=64` kbps. The transmission rate of the whole link is `8000*8*text(number of slots)` (I think).</p>
      </div>
      <div class="ln-box">
        <p>Suppose that all links in a network use TDM with `24` slots and can transfer data at a rate of `1.536` Mbps. Then each circuit has a transmission rate of `1.536/24=64` kbps.</p>
      </div>
      <div class="ln-box">
        <p>Suppose there is a circuit-switched network with four circuit switches `A`, `B`, `C`, and `D`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/circuit_switching_2.png">
        </div>
        <p>What is the maximum number of connections that can be ongoing in the network at any time?</p>
        <div class="collapse ln-box" id="circuit_switching_q1">
          <p>There can be `13` connections from `A` to `B`, `11` connections from `B` to `C`, `17` connections from `C` to `D`, and `12` connections from `D` to `A`. So the maximum number of connections possible is `13+11+17+12=53`.</p>
          <p>Note that while it's possible to have a connection from `A` to `C`, for example, that wouldn't maximize the number of connections because two circuits would have to be reserved for one connection, as opposed to the answer above where only one circuit has to be reserved for one connection.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#circuit_switching_q1" aria-expanded="false" aria-controls="circuit_switching_q1">Answer</button>
        <p>Suppose that every connection requires two consecutive hops, and calls are connected clockwise. For example, a connection can go from `A` to `C`, from `B` to `D`, from `C` to `A`, and from `D` to `B`. What is the maximum number of connections that can be ongoing in the network at any time?</p>
        <div class="collapse ln-box" id="circuit_switching_q2">
          <p>`ArarrBrarrC`: Since there are only `11` circuits from `B` to `C`, the number of connections possible for `ArarrBrarrC` is `11`.</p>
          <p>`BrarrCrarrD`: Since all `11` circuits are being used for `ArarrBrarrC`, there are no circuits available for `BrarrCrarrD`.</p>
          <p>`CrarrDrarrA`: Since there are only `12` circuits from `D` to `A`, the number of connections possible for `CrarrDrarrA` is `12`.</p>
          <p>`DrarrArarrB`: Since all `12` circuits are being used for `CrarrDrarrA`, there are no circuits available for `DrarrArarrB`.</p>
          <p>So the maximum number of connections possible is `11+12=23`.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#circuit_switching_q2" aria-expanded="false" aria-controls="circuit_switching_q2">Answer</button>
        <p>Suppose that `13` connections are needed from `A` to `C`, and `19` connections are needed from `B` to `D`. Can we route these calls through the four links to accommodate all `32` connections?</p>
        <div class="collapse ln-box" id="circuit_switching_q3">
          <p>For `ArarrC`, we can use the circuits for `ArarrBrarrC` and `ArarrDrarrC`. `ArarrBrarrC` only supports `11` connections, so the remaining `2` connections will need to use `2` of the circuits in `ArarrD`.</p>
          <p>For `BrarrD`, we can't use the circuits for `BrarrC` because they are already being used for `ArarrBrarrC`. So the only circuits available are for `BrarrArarrD`. There are only `10` circuits available for `ArarrD` since `2` of them are being used for `ArarrDrarrC`. So there are only `10` circuits available for `BrarrArarrD`, which is not enough for the `19` connections that are needed for `BrarrD`.</p>
          <p>So the answer is no.</p>
          <p>It doesn't matter how we choose to divide up the connections. Notice that `13` connections are needed from `A` to `C`, but any path from `A` to `C`, whether it's `ArarrDrarrC` or `ArarrBrarrC`, will have all the circuits used up on that path. So the connections from `B` to `D` can only travel in the other direction, and neither direction can support `19` connections on its own.</p>
          <p>(Also, the answer can be obtained by looking at the answer to the second question, but that's boring.)</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#circuit_switching_q3" aria-expanded="false" aria-controls="circuit_switching_q3">Answer</button>
      </div>
      <h4>Switching Teams</h4>
      <p>Packet switching is not good for things that require a continuous connection, like video calls. However, packet switching is, in general, more efficient than circuit switching.</p>
      <div class="ln-box">
        <p>Suppose several users share a `1` Mbps link. But each user is not using the connection all of the time, i.e., there are periods of activity and inactivity. Let's say that a user is actively using the connection 10% of the time, and generates data at `100` kbps when they are active. With circuit switching, this means the link can support `(1 text( Mbps))/(100 text( kbps))=10` users at once. The circuit must be reserved for all `10` users as long as they are connected, even if they're not using it all the time.</p>
        <p>With packet switching, resources are not reserved, so any number of users can use the link. If there are `35` users, the probability that `ge 11` users are actively using the connection at the same time is ~`0.0004`, which means there is a ~`0.9996` chance that there are `le 10` simultaneous users at any time. As long as there are not more than `10` users at a time, there will be no queuing delay (if there are more than `10` users, there will be more data than the `1` Mbps link can handle). So packet switching performs just as well as circuit switching while allowing for more users at the same time.</p>
      </div>
      <div class="ln-box">
        <p>Sticking with the same `1` Mbps link, suppose there are `10` users. One of the users suddenly generates `1,000,000` bits of data, but the other users are inactive. Suppose the link has `10` slots per frame. If all `10` slots are utilized, then the link can send `1,000,000` bits per second (`= 1` Mbps). However, under the rules of circuit switching, only one of the slots per frame will be used for that user. As a result, the link will only transfer `(1,000,000)/10=100,000` bits per second for that user's connection. So it will take `(1,000,000)/(100,000)=10` seconds to transfer all the data.</p>
        <p>With packet switching, since no other users are active, the one user gets to send all `1,000,000` bits of data in `1` second since there will be no queuing delays.</p>
      </div>
      <div class="ln-box">
        <p>Suppose users share a `2` Mbps link. Also suppose each user transmits continuously at `1` Mbps when transmitting, but each user transmits only `20%` of the time.</p>
        <p>When circuit switching is used, how many users can be supported?</p>
        <div class="collapse ln-box" id="switching_q1">
          <p>Only `2` users can be supported since each user transmits at `1` Mbps.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#switching_q1" aria-expanded="false" aria-controls="switching_q1">Answer</button>
        <p>Suppose packet switching is used from now on.</p>
        <p>Why will there be no queuing delay before the link if `le 2` users transmit at the same time?</p>
        <div class="collapse ln-box" id="switching_q2">
          <p>If only `1` user is transmitting, then there will be traffic at a rate of `1` Mbps, which the link can handle. If `2` users are transmitting, then there will be traffic at a rate of `2` Mbps, which the link can also handle.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#switching_q2" aria-expanded="false" aria-controls="switching_q2">Answer</button>
        <p>Why will there be a queuing delay if `3` users transmit at the same time?</p>
        <div class="collapse ln-box" id="switching_q3">
          <p>If `3` users are transmitting, then there will be traffic at a rate of `3` Mbps. But since the link's transmission rate is only `2` Mbps, packets will be coming in faster than they can be transmitted.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#switching_q3" aria-expanded="false" aria-controls="switching_q3">Answer</button>
        <p>What is the probability that a given user is transmitting?</p>
        <div class="collapse ln-box" id="switching_q4">
          <p>`20%`</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#switching_q4" aria-expanded="false" aria-controls="switching_q4">Answer</button>
        <p>Suppose there are `3` users. What is the probability that at any given time, all `3` users are transmitting at the same time?</p>
        <div class="collapse ln-box" id="switching_q5">
          <p>The probability of `1` user transmitting is `20%=0.2`. So the probability of `3` users transmitting is `0.2*0.2*0.2=0.008`.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#switching_q5" aria-expanded="false" aria-controls="switching_q5">Answer</button>
      </div>
      <h3>A Network of Networks</h3>
      <p>We get our Internet access from an ISP. Of course, all of us don't get Internet access from the same ISP. There are different ISPs out there providing Internet access to different networks. But then how do our end systems communicate with other end systems? How do we communicate with servers and visit websites from different parts of the world for example? The answer is that the ISPs themselves are connected with each other. But what exactly does that look like?</p>
      <p>One idea is that we connect every ISP with each other directly.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_1.png">
      </div>
      <p>The problem with this is that it is too costly, because there would be way too many links (too many for me to draw). In computer science terms, there would be `n^2` connections.</p>
      <p>To minimize the number of connections, another idea is that there is a global ISP and all the other ISPs connect to that global one.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_2.png">
      </div>
      <p>This, too, is costly. But to offset the cost, the global ISP would charge the other ISPs (we'll call them access ISPs from this point on) money to connect to the global ISP.</p>
      <p>But if one global ISP becomes profitable, naturally there will be other global ISPs wishing to be profitable too.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_3.png">
      </div>
      <p>This structure is good for the access ISPs because they can choose which global ISP they want to connect to by comparing prices and service quality. So what we have so far is a two-tier hierarchy where global ISPs are at the top and access ISPs are at the bottom.</p>
      <p>The reality is that these global ISPs can't exist in every city in the world. What happens instead is that the access ISPs connect to a <b>regional ISP</b>, which then connects to a global ISP. (We'll now be calling global ISPs by the more correct term <b>tier-1 ISP</b>.) Some examples of tier-1 ISPs are AT&amp;T, Verizon, and T-Mobile.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_4.png">
      </div>
      <div class="ln-box">
        <p>Sometimes, an access ISP can connect directly to a tier-1 ISP. In that case, the access ISP would pay the tier-1 ISP.</p>
      </div>
      <div class="ln-box">
        <p>In some regions, there can be a larger regional ISP consisting of smaller regional ISPs.</p>
      </div>
      <div class="ln-box">
        <p>Access ISPs and regional ISPs can choose to <b>multi-home</b>. That is, they can connect to more than one provider ISP at the same time, getting Internet access from multiple ISPs. While they have to pay each ISP they're connected to, the multi-homed ISPs can achieve better reliability in case one of the provider ISPs has a failure.</p>
      </div>
      <p>This multi-tier hierarchy is closer to the structure of today's Internet, but there are still a few pieces missing.</p>
      <p>Lower-tier ISPs pay higher-tier ISPs based on how much traffic goes through their connection. To avoid sending traffic to the higher-tier ISPs, the lower-tier ISPs can <b>peer</b>. This means that they connect with each other so that the traffic goes between them instead of up to the higher-tier ISP.</p>
      <p>For example, suppose that a computer connected to the green ISP wants to communicate with a computer connected to the blue ISP. Without peering, the traffic would have to go all the way up to the tier-1 ISP and back down from there.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_5.png">
      </div>
      <p>But with peering, they can avoid the cost of going through the regional and tier-1 ISPs by connecting with each other directly. Usually when ISPs peer with each other, they agree to not pay each other for the traffic that comes from peering.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_6.png">
      </div>
      <p>To facilitate this peering, third-party companies can create an <b>Internet Exchange Point (IXP)</b> which ISPs connect to in order to peer with other ISPs.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_7.png">
      </div>
      <p>There's one more piece: <b>content-provider networks</b>. Content providers are companies like Google, Amazon, and Microsoft.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_8.png">
      </div>
      <p>They run their own private networks so that they can bring their services and content closer to users. They do this by having data centers distributed everywhere across the world. Each data center has tons of servers that are all connected to each other.</p>
      <p>And this is why the Internet is a network of networks. (Recall that ISPs themselves are networks of packet switches and links. There's also the edge network from the previous section, and each network in the edge network is a network of routers and links.)</p>
      <h2 id="delay">Delay, Loss, and Throughput</h2>
      <p>In a (developer's) perfect world, data would travel through the Internet instantaneously without any limitations. But, we know things aren't perfect.</p>
      <h3>What's the Holdup?</h3>
      <p>As packets travel along their route, they experience <b>nodal processing delay</b>, <b>queuing delay</b>, <b>transmission delay</b>, and <b>propagation delay</b>. All these delays combined is called <b>total nodal delay</b>.</p>
      <h4>Processing Delay</h4>
      <p>When a packet arrives at a router, the router looks at the packet's header to see which link it should go on. It also checks for any bit-level errors in the packet. The time it takes to do this (microseconds), is the <b>processing delay</b>.</p>
      <h4>Queuing Delay</h4>
      <p>Once the outbound link has been determined, the packet goes to the queue for that link. The time it waits in the queue (microseconds to milliseconds), is the <b>queuing delay</b>.</p>
      <h4>Transmission Delay</h4>
      <p>Once the packet reaches the front of the queue, it is ready to be transferred across the link. Recall that packets travel through links as bits. The time it takes to push all of the bits into the link (microseconds to milliseconds) is the <b>transmission delay</b>. If a packet has `L` bits and the link can transfer `R` bits per second, then the transmission delay is `L/R`.</p>
      <h4>Propagation Delay</h4>
      <p>Now that the bits are on the link, they must get to the other end. The time it takes to get from the beginning of the link to the end of the link (milliseconds) is the <b>propagation delay</b>. This is determined by two things: how fast the bits are able to move and how far they have to go. How fast the bits can move depends on the physical characteristics of the link (e.g., whether it's fiber optics or twisted-pair copper wire) and is somewhere near the speed of light.</p>
      <p>If `d` is the distance the bit has to travel, i.e., the length of the link, and `s` is the propagation speed of the link, then `d/s` is the propagation delay.</p>
      <div class="ln-box">
        <p>Transmission delay and propagation delay sound very similar to each other. So here are some pictures illustrating the difference.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/transmission_delay.png">
        </div>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/propagation_delay.png">
        </div>
        <p>Yeah, somehow the bits go from being on a conveyer belt to being pushed in needlessly large carts. There's no delay from belt to cart though. That part happens magically.</p>
      </div>
      <div class="ln-box">
        <p>Consider a packet of length `L` which begins at end system `A` and travels over three links to a destination end system. These three links are connected by two packet switches. For `i=1,2,3`, let `d_i`, be the length of each link, `s_i` be the propagation speed of each link, and `R_i` be the transmission rate of each link. The packet switch delays each packet by `d_text(proc)`. Assume there are no queuing delays.</p>
        <p>In terms of `d_i`, `s_i`, `R_i`, and `L`, what is the total end-to-end delay for the packet?</p>
        <div class="collapse ln-box" id="delay_q1">
          <p>The total end-to-end delay is the sum of all the delays: processing delay, queuing delay, transmission delay, and propagation delay.</p>
          <p>The processing delay is given to us as `d_text(proc)`. There are two packet switches, so there will be two processing delays.</p>
          <p>We are also given that there are no queuing delays.</p>
          <p>The transmission delay is `L/R_1+L/R_2+L/R_3`.</p>
          <p>The propagation delay is `d_1/s_1+d_2/s_2+d_3/s_3`.</p>
          <p>So the total end-to-end delay is `d_text(proc)+d_text(proc)+L/R_1+L/R_2+L/R_3+d_1/s_1+d_2/s_2+d_3/s_3`.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#delay_q1" aria-expanded="false" aria-controls="delay_q1">Answer</button>
        <p>Suppose now the packet is `1500` bytes, the propagation speed on all three links is `2.5*10^8` m/s, the transmission rates of all three links are `2` Mbps, the packet switch processing delay is `3` ms, the length of the first link is `5000` km, the length of the second link is `4000` km, and the length of the third link is `1000` km. What is the end-to-end delay?</p>
        <div class="collapse ln-box" id="delay_q2">
          <p>From the first question, we know that the end-to-end delay is `d_text(proc)+d_text(proc)+L/R_1+L/R_2+L/R_3+d_1/s_1+d_2/s_2+d_3/s_3`, so now we'll just calculate and plug in those values.</p>
          <p>We're given that `d_text(proc)=3` ms.</p>
          <p>To calculate transmission delay, we'll first convert everything to bits. There are `8` bits in a byte, so the packet length is `1500*8=12,000` bits. For the transmission rate, `2` Mbps = `2,000,000` bits per second. Since all three links have the same transmission rate, `R_1=R_2=R_3`. So `L/R_1=L/R_2=L/R_3=(12,000)/(2,000,000)=0.006` seconds `=6` ms.</p>
          <p>For the propagation delay, we'll first convert everything to meters. The lengths of the links are `5,000,000` m, `4,000,000` m, and `1,000,000` m. Since all three links have the same propagation speed, `s_1=s_2=s_3`. So `d_1/s_1=(5,000,000)/(250,000,000)=0.02` seconds `=20` ms, `d_2/s_2=(4,000,000)/(250,000,000)=0.016` seconds `=16` ms, and `d_3/s_3=(1,000,000)/(250,000,000)=0.004` seconds `=4` ms.</p>
          <p>So the total-end-to-end delay is `3+3+6+6+6+20+16+4=64` ms.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#delay_q2" aria-expanded="false" aria-controls="delay_q2">Answer</button>
      </div>
      <div class="ln-box">
        <p>Consider a highway that has a tollbooth every `100` km. It takes `12` seconds for a tollbooth to service a car. Suppose a caravan of `10` cars are on the highway with each car traveling `100` km/hour. When going from tollbooth to tollbooth, the first car waits at the tollbooth until the the other nine cars arrive. To analogize, the tollbooth is a router, the highway is a link, the cars are bits, and the caravan is a packet.</p>
        <p>How long does it take the caravan to travel from one tollbooth to the next?</p>
        <div class="collapse ln-box" id="caravan_q1">
          <p>Since the tollbooth takes `12` seconds to service a car and there are `10` cars, it takes a total of `120` seconds (or `2` minutes) to service all the cars.</p>
          <p>Since the distance between every tollbooth is `100` km and the cars are moving at `100` km/hour, it takes `1` hour (or `60` minutes) to reach the next tollbooth.</p>
          <p>Since the cars in the caravan travel together, everything depends on the last car. So we'll look at this from the last car's point of view.</p>
          <p>The last car has to wait `2` minutes before it can be serviced by the tollbooth. Once this is done, it takes `60` minutes for it to reach the next tollbooth. So it takes the last car `62` minutes to reach the next tollbooth.</p>
          <p>Here's some animation to visualize:</p>
          <div class="ln-center">
            <img class="img-fluid ln-image-small" src="../pictures/networking/caravan.gif">
          </div>
          <p>At `120` seconds all `10` cars have exited the first tollbooth and are on the highway to the next tollbooth.</p>
          <p>The first car will take `60` minutes to enter the next tollbooth (get off the animation line), but since it waited `12` seconds for the previous tollbooth servicing, it will enter the next tollbooth at `60:12`.</p>
          <div class="ln-center">
            <img class="img-fluid ln-image-small" src="../pictures/networking/caravan_2.gif">
          </div>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#caravan_q1" aria-expanded="false" aria-controls="caravan_q1">Answer</button>
        <p>To analogize some more, the time it takes the tollbooth to service a car is the transmission delay. The time it takes the car to move to the next tollbooth is the propagation delay.</p>
        <p>Now suppose that the cars travel at `1000` km/hour and the tollbooth takes `1` minute to service a car. Will cars arrive at the second tollbooth before all the cars are serviced at the first tollbooth? That is, will there still be cars at the first tollbooth when the first car arrives at the second tollbooth?</p>
        <div class="collapse ln-box" id="caravan_q2">
          <p>Now it takes `10` minutes to service all `10` cars.</p>
          <p>The first car will wait `1` minute for the tollbooth to service it. Then it will take `6` minutes (`(100text(km))/((1000text(km))/(60text(min)))=6`) to reach the next tollbooth. So it will take a total of `7` minutes for the first car to go from one tollbooth to the next.</p>
          <p>But in `7` minutes, the first tollbooth will have only serviced `7` cars. So there will be `3` cars still at the first tollbooth when the first car arrives at the second tollbooth.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#caravan_q2" aria-expanded="false" aria-controls="caravan_q2">Answer</button>
      </div>
      <h3>Queuing Delay and Packet Loss</h3>
      <p>Queuing delay is the most complicated delay since it varies from packet to packet (as implied, processing delay, transmission delay, and propagation delay are the same for every packet). For example, if `10` packets arrive at an empty queue, the first packet will have no queuing delay, but the last packet will have to wait for the first `9` packets to leave the queue.</p>
      <p>The factors that determine queuing delay are the rate at which traffic arrives, the transmission rate of the link, and the type of traffic that arrives (e.g., periodically or in bursts).</p>
      <div class="ln-box">
        <p>Let `a` be the average rate (in packets per second) at which packets arrive. If a packet has `L` bits, then `La` is the average rate (in bits per second) at which bits arrive. `R` is the transmission rate of the link. So `(La)/R` is the ratio of bits coming in vs bits going out; this ratio is called <b>traffic intensity</b>.</p>
        <p>`(La)/R gt 1` means that bits are arriving at the queue faster than they are leaving. In this case, the queuing delay will approach infinity.</p>
        <p>If `(La)/R le 1`, then the type of traffic that arrives has an effect on the queuing delay. For example, if one packet arrives every `L/R` seconds, then the packet will always arrive at an empty queue, so there will be no queuing delay.</p>
        <p>However, if `N` packets arrive in bursts every `NL/R` seconds, then the first packet will experience no queuing delay. But the second packet will experience a queuing delay of `L/R` seconds. The third packet will experience a queuing delay of `2L/R` seconds. Generally, each packet will have a queuing delay of `(n-1)L/R` seconds.</p>
        <p>Regardless of the type of traffic, as traffic intensity gets closer to `1`, the queuing delay gets closer to approaching infinity.</p>
      </div>
      <div class="ln-box">
        <p>Average queuing delay grows exponentially with traffic intensity, i.e., a small increase in traffic intensity results in a large increase in average queuing delay.</p>
      </div>
      <h4>Packet Loss</h4>
      <p>If packets are coming in faster than they are leaving the queue, then the queue will eventually be full and not be able to store any more packets. In this case, some packets will be <b>dropped</b> or <b>lost</b>. As traffic intensity increases, so does the size of the queue, which means the number of packets lost also increases.</p>
      <h3>End-to-End Delay</h3>
      <p>End-to-end delay is the total delay that a packet experiences from source to destination.</p>
      <div class="ln-box">
        <p>Suppose there are `N-1` routers between the source and destination. Then there are `N` links. Also suppose that the network is uncongested so there are practically no queuing delays.</p>
        <p>Then the end-to-end delay is `d_text(end-end)=Nd_text(proc)+Nd_text(trans)+Nd_text(prop)=N(d_text(proc)+d_text(trans)+d_text(prop))`.</p>
      </div>
      <h4>Traceroute</h4>
      <p>Traceroute is a program that allows us to see how packets move and how long they take. We specify the destination we want to send some packets to and the program will print out a list of all the routers the packets went through and how long it took to reach that router.</p>
      <div class="ln-box">
        <p>Suppose there are `N-1` routers between the source and destination. Traceroute will send `N` special packets into the network, one for each router and one for the destination. When the nth router receives the nth packet, it sends a message back to the source. When it receives a message, the source records the time it took to receive the message and where the message came from. Traceroute performs these steps 3 times.</p>
      </div>
      <h3>Throughput</h3>
      <p>Throughput is the rate at which bits are transferred between the source and destination. The <b>instantaneous throughput</b> is the rate at any point during the transfer. The <b>average throughput</b> is the rate over a period of time.</p>
      <div class="ln-box">
        <p>If a file has `F` bits and the transfer takes `T` seconds, then the average throughput of the file transfer is `F/T` bits per second.</p>
      </div>
      <p>Suppose we have two end systems connected by two links and one router. Let `R_s`, `R_c` be the rates of the two links.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/throughput.png">
      </div>
      <p>Suppose `F=32,000,000` bits, `R_s=2` Mbps, and `R_c=1` Mbps.</p>
      <ul>
        <li>After `1` second, `2,000,000` bits will move from the server to the router.</li>
        <li>After `2` seconds, `2,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `3` seconds, `2,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `4` seconds, `2,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
      </ul>
      <p>The file transfer is complete when the client receives `32,000,000` bits, which means `1,000,000` bits will have to move from the router to the client `32` times. Since it takes `1` second to do so, it will take `32` seconds for the file transfer to complete.</p>
      <p>Notice how it didn't matter how fast the bits moved from the server to the router. The time the file took to reach the client only depended on the rate from the router to the client. This doesn't mean the time it takes to reach the client is always equal to the rate from router to client though. Let's flip the numbers around to prove this point.</p>
      <p>Suppose `F=32,000,000` bits, `R_s=1` Mbps, and `R_c=2` Mbps.</p>
      <ul>
        <li>After `1` second, `1,000,000` bits will move from the server to the router.</li>
        <li>After `2` seconds, `1,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `3` seconds, `1,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `4` seconds, `1,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
      </ul>
      <p>Wait, but isn't the rate from router to client `2,000,000` bits per second? Why aren't `2,000,000` bits moving from router to client every second? That's because there are only `1,000,000` bits entering the router every second.</p>
      <p>From this, we can see that the throughput of a file transfer depends on the link that is transferring bits at the lowest rate. This link is called the <b>bottleneck link</b>.</p>
      <div class="ln-box">
        <p>Mathematically, the throughput is `min(R_s,R_c)`.</p>
        <p>The time it takes to transfer the file is `F/(min(R_s,R_c))`.</p>
      </div>
      <div class="ln-box">
        <p>Generally, the throughput for a network with `N` links is `min(R_1,R_2,...,R_n)`.</p>
      </div>
      <p>The bottleneck link is not always the link with the lowest transmission rate. We'll look at another example to show this.</p>
      <p>Suppose there are `10` clients downloading data from `10` servers (each client is connected to a unique server). Suppose they all share a common link with transmission rate `R` somewhere in the network core. We'll denote the transmission rates of the server links as `R_s` and the transmission rates of the client links as `R_c`.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/throughput_2.png">
        <p>(I think sharing a common link means all the server links eventually connect to one router and all the client links eventually connect to a different router. And the link between these two routers are the same.)</p>
      </div>
      <p>Let's assign actual values to these transmission rates. So let `R_s=2` Mbps, `R_c=1` Mbps, and `R=5` Mbps. Since the common link is being used for `10` simultaneous downloads, the link divides its transmission rate equally among the `10` connections. So the common link is transferring bits at a rate of `(5text(Mbps))/10=500` kbps per connection. Which means the throughput for each download is `500` kbps. Even though the common link had the highest transmission rate, it ended up being the bottleneck link.</p>
      <div class="ln-box">
        <p>Links in the network core are almost never the bottleneck link because they are usually over-provisioned with high speed links. Typically, the bottleneck links are in the access network.</p>
      </div>
      <h2 id="protocollayers">Protocol Layers</h2>
      <p>There are many pieces to the Internet, like routers, links, applications ("apps"), protocols, and end systems. These pieces are organized into layers.</p>
      <h3>Layered Architecture: The Internet Is Like an Onion</h3>
      <p>The airline system can be seen as an organization of layers.</p>
      <table class="table">
        <tr>
          <th>Layer</th>
          <th>Action (Leaving)</th>
          <th>Action (Arriving)</th>
        </tr>
        <tr>
          <td>Ticketing</td>
          <td>Purchase</td>
          <td>Complain</td>
        </tr>
        <tr>
          <td>Baggage</td>
          <td>Check</td>
          <td>Claim</td>
        </tr>
        <tr>
          <td>Gates</td>
          <td>Enter</td>
          <td>Exit</td>
        </tr>
        <tr>
          <td>Runway</td>
          <td>Takeoff</td>
          <td>Land</td>
        </tr>
        <tr>
          <td>Airplane</td>
          <td>Routing</td>
          <td>Routing</td>
        </tr>
      </table>
      <p>Every time we board and leave an airplane, we perform the same steps in a certain order. And the action we perform depends on the layer we're at since each layer is responsible for one thing and one thing only. (I.e., each layer provides a different service.) Also, each layer depends on the layer before it. For example, we can only check in our baggage if we have a ticket, or we can only enter the gate once we have checked in our baggage.</p>
      <p>This layering makes it easier to distinguish the different components of an airline system. It also makes it easier to change the implementation of a layer's service without affecting the rest of the layers. For example, the current implementation of the gate layer is to board people by ticket priority and check-in time. But the implementation can be changed to board people by height. This change doesn't affect any of the other layers, and the function of this layer remains the same: getting people on the airplane.</p>
      <h4>Protocol Layering</h4>
      <p>The Internet is made up of 5 layers.</p>
      <table class="table">
        <tr>
          <th>Layer</th>
        </tr>
        <tr>
          <td>5. Application</td>
        </tr>
        <tr>
          <td>4. Transport</td>
        </tr>
        <tr>
          <td>3. Network</td>
        </tr>
        <tr>
          <td>2. Link</td>
        </tr>
        <tr>
          <td>1. Physical</td>
        </tr>
      </table>
      <p>Recall that a protocol is a set of rules that define how things should work. Each protocol belongs to one of the layers.</p>
      <h4>Application Layer</h4>
      <p>The application layer includes all the network applications (basically apps or programs that connect to the Internet).</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
          <th>Details</th>
        </tr>
        <tr>
          <td>HTTP</td>
          <td>sending and receiving Web documents</td>
        </tr>
        <tr>
          <td>SMTP</td>
          <td>sending emails</td>
        </tr>
        <tr>
          <td>FTP</td>
          <td>transferring files</td>
        </tr>
      </table>
      <p>Applications communicate with each other by sending <b>messages</b> to each other.</p>
      <h4>Transport Layer</h4>
      <p>The transport layer is responsible for delivering the application's message to the network core.</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
          <th>Details</th>
        </tr>
        <tr>
          <td>TCP</td>
          <td>
            <p>connection oriented</p>
            <p>guaranteed delivery</p>
          </td>
        </tr>
        <tr>
          <td>UDP</td>
          <td>
            <p>connectionless</p>
            <p>no reliability, no flow control, no congestion control</p>
          </td>
        </tr>
      </table>
      <p>In the transport layer, the packets of information are called <b>segments</b>.</p>
      <h4>Network Layer</h4>
      <p>The network layer is responsible for moving the transport layer's segments from router to router.</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
          <th>Details</th>
        </tr>
        <tr>
          <td>IP</td>
          <td>
            <p>defines how packets must be formatted</p>
            <p>defines the actions that must be taken when sending or receiving packets</p>
          </td>
        </tr>
      </table>
      <p>In the network layer, the packets are called <b>datagrams</b>.</p>
      <h4>Link Layer</h4>
      <p>The link layer is responsible for moving the network layer's datagrams across links.</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
        </tr>
        <tr>
          <td>Ethernet</td>
        </tr>
        <tr>
          <td>WiFi</td>
        </tr>
        <tr>
          <td>DOCSIS</td>
        </tr>
      </table>
      <p>In the link layer, the packets are called <b>frames</b>.</p>
      <h4>Physical Layer</h4>
      <p>The physical layer is responsible for moving the individual bits of the link layer's frames across links.</p>
      <div class="ln-box">
        <h4>OSI Model</h4>
        <p>In the late 1970s (before the Internet became public), there was another model that was proposed by the International Organization for Standardization (ISO) on how to organize computer networks. This model was called the Open Systems Interconnection (OSI) model and it had 7 layers:</p>
        <table class="table">
        <tr>
          <th>Layer</th>
        </tr>
        <tr>
          <td>7. Application</td>
        </tr>
        <tr>
          <td>6. Presentation</td>
        </tr>
        <tr>
          <td>5. Session</td>
        </tr>
        <tr>
          <td>4. Transport</td>
        </tr>
        <tr>
          <td>3. Network</td>
        </tr>
        <tr>
          <td>2. Link</td>
        </tr>
        <tr>
          <td>1. Physical</td>
        </tr>
      </table>
      <p>The layers that also appear in the Internet model work roughly the same way. So we'll just go over the ones that are new.</p>
      <p>The presentation layer makes sure the data is readable and usable. It provides data compression, data encryption, and data description. I guess it's sorta like a translator.</p>
      <p>The session layer makes sure the data goes where it needs to go. It provides data synchronization, checkpointing, and data recovery.</p>
      </div>
      <h3>Encapsulation</h3>
      <p>Whenever we moved across the different layers, we called the packets of information by a different name. That is because the packets change as they move from layer to layer.</p>
      <ol>
        <li>Application layer sends message</li>
        <li>Transport layer adds transport-layer header information to the message, forming a segment</li>
        <li>Network layer adds network-layer header information to the segment, forming a datagram</li>
        <li>Link layer adds link-layer header information to the datagram, forming a frame</li>
      </ol>
      <p>The header information is needed so that the thing (e.g., router, link, end system) receiving the packet knows what it is and what to do with it. Here are some animations to illustrate:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/encapsulation.gif">
      </div>
      <p>End systems implement all 5 layers while routers usually implement only the bottom 3 layers (network, link, physical). (And some packet switches may only implement the bottom 2 layers, link and physical.)</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/encapsulation_2.gif">
      </div>
      <p>Notice that when the packet left the end system, the last header added was the link-layer header, which is the first thing that gets processed when entering the router. When the packet reaches the link layer, the link-layer headers are extracted away. So now the top-most header is the network-layer header, which is what the network layer then processes.</p>
      <p>After going through the network layer, the router determines that the packet has to go on another link again. So now the packet has to travel down the layers. When it goes from the network layer down to the link layer, a new link-layer header is added, so that the next thing (router or end system) can process it.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/encapsulation_3.gif">
      </div>
      <p>Now that the packet has reached the end system, it goes up the layers one last time. Again, at each layer the headers are removed until only the message remains, which the application understands.</p>
      <hr>
      <p>From this point on, we'll go into more detail for each layer, starting with the application layer.</p>
      <hr>
      <h2 id="networkapplications">Principles of Network Applications</h2>
      <p>A network application is a set of apps or programs that connect to and communicate with the Internet. These apps are made by our local software developers using languages like C, Java, or Python.</p>
      <p>Typically, network applications involve multiple programs running on different end systems. For example, we can download Spotify on our phone, tablet, computer, or watch. And Spotify will have a different program running on their servers to provide functionality to the app.</p>
      <h3>Network Application Architectures</h3>
      <p>There are 2 types of network application architectures: client-server and peer-to-peer (P2P).</p>
      <p>In the client-server architecture, there is a server that is always on and running. Whenever clients want data, they make a request to the server, which then provides the clients with what they requested. In addition to always being on, the server has a fixed IP address, so clients can make requests to the server at any time.</p>
      <p>Big companies, like Google, need more than one server to handle all the requests, otherwise that one server would get overwhelmed. So they set up <b>data centers</b>, which house multiple servers. And then they go and set up multiple data centers.</p>
      <p>In peer-to-peer architecture, there are no servers (and thus no clients). Instead, devices communicate directly with other devices â€” called peers â€” to transfer data.</p>
      <p>Let's say you wanted to download a large file (perhaps an online textbook ðŸ¤«). You could go online and download it (this would be client-server), but let's say the file is so large, that it takes too long. Suppose that I happen to have the complete file already downloaded. Using the right application, I could transfer my file directly to your computer. Suppose that someone else also has the file. Then you can receive the file from both of us. And the more people that have the file, the more people you can connect to to receive the file, making the download faster.</p>
      <p>The effectiveness of peer-to-peer relies on the number of peers. The more peers there are, the more computing power there is. This is referred to as <b>self-scalability</b>.</p>
      <h3>Processes Communicating</h3>
      <p>We call them programs. Operating systems call them <b>processes</b>.</p>
      <h4>Client and Server Processes</h4>
      <p>A network application consists of pairs of processes that send messages to each other. A Web broswer (a process) communicates with a Web server (another process). Spotify, the app (a process), communicates with Spotify's servers (another set of processes). The process that is requesting data is the <b>client</b> process and the process that is providing the data is the <b>server</b> process. Despite the naming, the terms "client" and "server" processes apply for both client-server architecture and peer-to-peer architecture.</p>
      <div class="ln-box">
        <p>More formally (and perhaps more accurately), client processes are the ones that initiate communication and server processes are the ones that wait to be contacted.</p>
      </div>
      <h4>The Interface Between the Process and the Computer Network</h4>
      <p>As we saw before, a process in the application layer sends a message to the transport layer. In order to do so, it opens a <b>socket</b>, which is the application layer's door to the transport layer. Sockets are also used to receive messages from the transport layer.</p>
      <div class="ln-box">
        <p>For us developers, sockets are also referred to as the <b>Application Programming Interface (API)</b> between the application and the network. We don't have control over how the socket is implemented; that's handled by the OS.</p>
      </div>
      <h4>Addressing Processes</h4>
      <p>In order for the message to go to the correct place, the sending process needs to specify where the message needs to be delivered to. First, it needs to specify which end system is receiving the message. This can be done by providing the <b>IP address</b> of the end system. Just knowing which end system is not enough though because an end system is running multiple applications while it's on, so we also need to know which application needs the message. Whenever processes open a socket, they must choose a number to "label" the socket. This number is called a <b>port number</b>. So by specifying the IP address and the port number, the message can be delivered to the correct application on the correct end system.</p>
      <h3>Transport Services Available to Applications</h3>
      <p>On the other side of the socket is a transport-layer protocol that delivers the message to the socket of the receiving process. There are several transport-layer protocols an application can choose from. When deciding which one to use, there are several things to consider.</p>
      <h4>Reliable Data Transfer</h4>
      <p>As we saw earlier, packets can get lost. For some situations, like transferring financial documents, packet loss is undesirable. But even though packets can get lost, a protocol can still guarantee that the file will be delivered correctly. If a protocol can provide this guarantee, then that protocol provides <b>reliable data transfer</b>.</p>
      <p>In other situations, like video calls, some packet loss is acceptable. ("I'm sorry, can you repeat that? You cut out for a few seconds.") Those applications are referred to as <b>loss-tolerant applications</b>.</p>
      <h4>Throughput</h4>
      <p>Throughput is the rate at which bits are transferred between the source and destination (Hmm, I feel like I've said these exact words before ðŸ¤”). A transport-layer protocol can guarantee a specified throughput of some amount of bits per second. This would be useful for voice calls, which need a certain throughput in order for the voices to be heard clearly. Applications that have throughput requirements are <b>bandwidth-sensitive applications</b>.</p>
      <p>Applications that don't are <b>elastic applications</b>. File transfers don't need a certain throughput. If more bits can be transferred, then the download will be faster. If fewer bits can be transferred, then the download will be slower.</p>
      <h4>Timing</h4>
      <p>Whereas throughput is the number of bits going through, timing is the delay between each bit. One timing guarantee might be that bits go to the receiving process every 100 milliseconds. For real-time applications, like video calls and online games, it is ideal to have low delay.</p>
      <h4>Security</h4>
      <p>Before transferring the data, a transport protocol can encrypt it.</p>
      <h3>Transport Services Provided by the Internet</h3>
      <p>There are 2 transport protocols an application can use: TCP and UDP.</p>
      <h4>TCP</h4>
      <p>TCP is a connection-oriented protocol. The client and server have to confirm a connection (<b>TCP connection</b>) with each other first before messages can be transferred. And once the application is done sending messages, it must close the connection.</p>
      <p>TCP also provides reliable data transfer. This means data is sent without error and in the proper order â€” there are no missing or duplicate bytes. Part of this is achieved by implementing congestion control. If the network is congested, the sending process will be throttled.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/tcp_comic.png">
      </div>
      <h4>UDP</h4>
      <p>UDP is like the opposite of TCP. It's connectionless. The data transfer is unreliable â€” there's no guarantee all the messages will arrive and if it does arrive, there's no guarantee that the messages will arrive in order. And there's no congestion control. All of these things sound bad, so why does UDP even exist in the first place?</p>
      <p>Well, no congestion control means that UDP can send bits at any rate with no limits*. This is good for voice call applications because they want data to be sent as much as possible and as fast as possible.</p>
      <p>*However, there will be actual limitations to this rate, including link transmission rate and network congestion.</p>
      <div class="ln-box">
        <p>For some reason, most firewalls are configured to block most types of UDP traffic. So applications that use UDP have to be prepared to use TCP as a backup.</p>
      </div>
      <div class="ln-box">
        <table class="table">
          <tr>
            <th>Application</th>
            <th>Application-Layer Protocol</th>
            <th>Transport-Layer Protocol</th>
          </tr>
          <tr>
            <td>Email</td>
            <td>SMTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Remote terminal access</td>
            <td>SSH</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Web</td>
            <td>HTTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>File transfer</td>
            <td>FTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Streaming</td>
            <td>HTTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Internet telephony</td>
            <td>SIP, RTP, etc.</td>
            <td>TCP or UDP</td>
          </tr>
        </table>
      </div>
      <div class="ln-box">
        <p>TCP and UDP both <em>don't</em> guarantee:</p>
        <ul>
          <li>throughput</li>
          <li>timing</li>
          <li>security*</li>
        </ul>
        <p>Despite this, time-sensitive applications still work. This is because they are designed with these lack of guarantees in mind.</p>
        <p>*TCP can be enchanced with <b>Transport Layer Security (TLS)</b> to provide encryption. TLS is not a protocol; it's an add-on to TCP.</p>
      </div>
      <h3>Application-Layer Protocols</h3>
      <p>Application-layer protocols define:</p>
      <ul>
        <li>the types of messages exchanged (e.g., request and response messages)</li>
        <li>the format of the messages
          <ul>
            <li>what fields can go in a message</li>
          </ul>
        </li>
        <li>the meaning of the information in the fields</li>
        <li>rules for determining when and how processes send and respond to messages</li>
      </ul>
      <p>Some protocols are defined in RFCs and are therefore public. While other protocols, like Skype's, are proprietary.</p>
      <h2 id="http">The Web and HTTP</h2>
      <p>The Internet became big in the 1990s with the arrival of the World Wide Web.</p>
      <h3>Some Dry Facts About HTTP</h3>
      <p>A <b>Web page</b> is a document with a bunch of <b>objects</b>, which can be images, videos, JavaScript files, CSS style sheets, etc. (More formally, an object is a file that is addressable by a single URL.) A Web page is usually an HTML file that includes references to these objects. <b>Web browsers</b> request and display Web objects while <b>Web servers</b> store Web objects.</p>
      <p>HTTP stands for <b>HyperText Transfer Protocol</b>, and it is the Web's application-layer protocol. When browsers request a Web page, they send HTTP request messages to the server and the server responds with HTTP response messages that contain the objects.</p>
      <p>HTTP uses TCP as its transport protocol. Port 80 is reserved for HTTP, so when browsers and servers initialize their sockets, they use port 80.</p>
      <p>HTTP is a <b>stateless protocol</b>. This means that the server does not store any information about the exchanges the server has with clients. For example, the server will not know if a particular client has asked for a Web page before or if it's the client's first time asking for the Web page.</p>
      <div class="ln-box">
        <p>The original version of HTTP is called HTTP/1.0. Then there was HTTP/1.1 (1997), HTTP/2 (2015), and HTTP/3 (2022).</p>
      </div>
      <h3>Non-Persistent and Persistent Connections</h3>
      <p>Each time a browser makes a request for a Web page, the browser actually has to make a request for the base HTML file and then each object in the file. In <b>non-persistent connections</b>, the browser opens a TCP connection for one object, then closes the connection after the object is sent. So a TCP connection has to be opened for each object. In <b>persistent connections</b>, the browser opens one TCP connection and requests/receives all the objects using that one connection. After one object is sent, the connection remains open for some period of time, allowing other objects to be sent using the same connection if needed. HTTP/1.1 and later use persistent connections by default.</p>
      <h4>HTTP with Non-Persistent Connections</h4>
      <ol>
        <li>Browser starts a TCP connection to the server on port 80</li>
        <li>Browser sends an HTTP request message to the server</li>
        <li>Server receives the request, gets the base file, puts it in an HTTP response message, and sends the message</li>
        <li>Server closes the TCP connection</li>
        <li>Browser receives the response.</li>
      </ol>
      <p>And these steps repeat for each object in the Web page. So if a Web page has `10` objects, there will be `11` connections made (`1` for the Web page and `10` for each object). If this seems tedious, it's because it is. That's why browsers can be configured to open multiple parallel connections at the same time instead of waiting for one connection to be done before opening another.</p>
      <p>Let's look at one request in action:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/http.gif">
      </div>
      <p>As we can see, it takes some time for the client to receive the file after making a request.</p>
      <p>The <b>round-trip time (RTT)</b> is the time it takes for a packet to travel from client to server. The delays we saw in the "Delay, Loss, and Throughput" section also factor into the RTT.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/rtt.png">
      </div>
      <p>So the total response time is `2` RTTs plus the transmission time of the file. `1` RTT is for establishing the TCP connection and `1` RTT is for receiving the file.</p>
      <p>Besides the delay for sending each object, there is also the burden of resource management. For each TCP connection, buffers and variables must be allocated; this can take up a lot of resources on the server if there are a bunch of connections to manage.</p>
      <h4>HTTP with Persistent Connections</h4>
      <p>Instead of closing the TCP connection after sending the requested object, the server keeps the connection open for a certain period of time so that multiple objects can be sent through the connection.</p>
      <div class="ln-box">
        <p>For both non-persistent and persistent connections, they can also be non-parallel or parallel. In non-parallel connections, the client waits until it receives an object before requesting another one. So each object requires `1` RTT. In parallel connections, the client requests an object as soon as it needs it, even if a previous request for an object has not been completed yet. This means if there are `n` objects, it could request for all `n` objects at once by opening `n` parallel connections. So it could take as little as `1` RTT to get all the objects. (Note that the `1` RTT is just for the objects in the base file. The browser still needs `1` RTT to get the base file itself first.)</p>
        <p>Persistent parallel is the default in HTTP/1.1 (and later) and the example below will show why.</p>
      </div>
      <div class="ln-box">
        <p>Suppose there is a `100`-kilobit page that contains `5` `100`-kilobit images, all of which are stored on a server. Suppose the round-trip time from the server to a client's browser is `200` milliseconds. For simplicity, we'll assume there is only one `100` Mbps link between the client and server; the time it takes to transmit a GET request onto the link is zero. Note that based on the assumptions, the link has a `100` millisecond one-way propagation delay as well as a transmission delay (for the page and images).</p>
        <p>For non-persistent, non-parallel connections, how long is the response time? (The time from when the user requests the URL to the time when the page and its images are displayed?)</p>
        <div class="collapse ln-box" id="http_q1">
          <p>Let's start with the base file. It takes `1` RTT to establish the TCP connection and another `1` RTT to get the base file. So it takes a total of `2` RTTs for the base file. We are given that one RTT is `200` milliseconds, so it takes `200*2=400` milliseconds for the base file to propagate through the link.</p>
          <p>The transmission delay for the base file is `L/R=(100text( kb))/(100text( Mbps))=(100,000text( bits))/(100,000,000text( bits/s))=0.001` seconds `=1` millisecond.</p>
          <p>So the total time it takes to get the base file is `400+1=401` milliseconds.</p>
          <p>Now for the objects. Since it is non-persistent, the TCP connection for the base file will be closed and new ones (`1` each) will be needed for the objects. Since it is non-parallel, the previous TCP connection has to close before a new one opens.</p>
          <p>For one object, it takes `1` RTT to establish the TCP connection, and `1` RTT to get the object. So `2` RTTs for one object. For `5` objects, this is a total of `5*2=10` RTTs. In terms of milliseconds, this is `200*10=2000` milliseconds.</p>
          <p>The transmission delay for one object is the same for the base file. Then for `5` objects, the transmission delay is `1*5=5` milliseconds.</p>
          <p>So the total time it takes to get all of the objects is `2000+5=2005` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `2005+401=2406` milliseconds `=2.406` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q1" aria-expanded="false" aria-controls="http_q1">Answer</button>
        <p>What about for non-persistent, parallel connections?</p>
        <div class="collapse ln-box" id="http_q2">
          <p>Again, we'll start with the base file. It takes `1` RTT to establish the TCP connection and another `1` RTT to get the base file. So `2` RTTs. Same as before, `2*200=400` milliseconds for the base file to propagate.</p>
          <p>The transmission delay is also the same at `1` millisecond.</p>
          <p>So the total time it takes to get the base file is `400+1=401` milliseconds.</p>
          <p>For the objects, since the connections are parallel, all `5` TCP connections will be opened in parallel in `1` RTT and all `5` objects will be retrieved at the same time in `1` RTT. So `2` RTTs for all `5` objects. This means it takes `2*200=400` milliseconds for them to propagate.</p>
          <p>Even though the connections are parallel, the objects are not transmitted onto the link in parallel. (There is only one link, so only one object can be transmitted at a time.) As we saw before, it takes `1` millisecond to transmit `1` object. So for `5` objects, `5` milliseconds.</p>
          <p>So the total time it takes to get all of the objects is `400+5=405` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `401+405=806` milliseconds `=0.806` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q2" aria-expanded="false" aria-controls="http_q2">Answer</button>
        <p>What about for persistent, non-parallel connections?</p>
        <div class="collapse ln-box" id="http_q3">
          <p>We open the TCP connection and wait for the base file to transmit and propagate, which takes `401` milliseconds.</p>
          <p>For the objects, since the connection is persistent, we don't need to open any more TCP connections.</p>
          <p>But since it is non-parallel, we need to get the objects one at a time. `1` RTT for each object means `5` RTTs for all objects, which means `5*200=1000` milliseconds for all objects to propagate.</p>
          <p>And `5` milliseconds to transmit.</p>
          <p>So the total time it takes to get all of the objects is `1000+5=1005` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `401+1005=1406` milliseconds `=1.406` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q3" aria-expanded="false" aria-controls="http_q3">Answer</button>
        <p>What about for persistent, parallel connections?</p>
        <div class="collapse ln-box" id="http_q4">
          <p>The base file takes `401` milliseconds to transmit and propagate.</p>
          <p>We don't need to open any more TCP connections, and all `5` objects will be retrieved at once in `1` RTT. So `200` milliseconds to propagate.</p>
          <p>`5` milliseconds to transmit.</p>
          <p>So the total time it takes to get all of the objects is `200+5=205` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `401+205=606` milliseconds `=0.606` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q4" aria-expanded="false" aria-controls="http_q4">Answer</button>
      </div>
      <h3>HTTP/2</h3>
      <p>Opening multiple connections results in the server having to open and maintain multiple sockets. However, using only one TCP connection introduces a problem called <b>Head of Line (HOL) blocking</b>. Let's say the base file has a large video clip near the top and many small objects below the video. Being near the top, the video will be requested first. Especially if the bottleneck link is bad, the video will take some time to transfer, causing the objects to have to wait for the video. The video at the head of the line blocks the objects behind it. Using parallel connections is one way to solve this problem.</p>
      <p>Standardized in 2015, HTTP/2 introduced a way to allow for using one TCP connection without experiencing HOL blocking.</p>
      <h4>HTTP/2 Framing</h4>
      <p>The solution is to break each object into small frames and interleave them. That way, each response will contain parts of each object.</p>
      <div class="ln-box">
        <p>Let's say the base file has `1` large video clip and `8` small objects. Suppose the video clip is broken up into `1000` frames and each of the small objects is broken up into `2` frames. The first response can contain `9` frames: `1` frame from the video and the first frame of each of the small objects. The second reponse can also contain `9` frames: `1` frames from the video and the last frame of each of the small objects. In just `18` frames, all of the small objects will have been sent. If the frames weren't interleaved, the small objects would be sent after `1016` frames.</p>
        <p>(For some reason, interloven sounds right.)</p>
      </div>
      <h4>Server Pushing</h4>
      <p>Another feature HTTP/2 introduced was server pushing, where the server can analyze the base file, see what objects there are, and send them to the client without needing to wait for the client to ask for them.</p>
      <h3>HTTP Message Format</h3>
      <p>There are two types of HTTP messages: request messages and response messages.</p>
      <h4>Request Messages: Give Me What I Want!</h4>
      <p>Here's an example of what a request message might look like:</p>
      <p><code>GET /projects/page.html HTTP/1.1<br>Host: www.myschool.edu<br>Connection: close<br>User-agent: Mozilla/5.0<br>Accept-language: fr</code></p>
      <p>The first line is the <b>request line</b>. There are three fields: the method field, the URL field, and the HTTP version field. As we can infer, the method is GET, which is the method for requesting objects (because, you know, we want to "get" them).</p>
      <p>The lines below the request line are the <b>header lines</b>. <code>Connection: close</code> means "close the connection after giving me what I requested", i.e., use a non-persistent connection. <code>Connection: keep-alive</code> would be for a persistent connection. The user agent specifies what type of browser is making the request. And <code>Accept-language: fr</code> means get the French version if it exists.</p>
      <div class="ln-box">
        <p>It may look like the header line specifying the host is unnecessary, because the browser needed to know the host in order to establish the TCP connection in the first place. It turns out that it is needed by Web proxy caches, which we'll see later.</p>
      </div>
      <p>There is also an entity body that follows the header lines. It wasn't included in the example above because the entity body is usually empty for GET requests. The entity body is usually used for POST requests, which are used to submit information, like user inputs from a form. The information that needs to be submitted are included in the entity body.</p>
      <div class="ln-box">
        <p>This is a little bit unconventional, but GET requests can also be used to submit information. Instead of going in the entity body, the inputted data is included in the URL with a question mark, like google.com/search?q=what+is+http, where q is the name of the input and the stuff after the '=' is the value of the input.</p>
      </div>
      <h4>Response Messages: Here You Go, Now Leave Me Alone!</h4>
      <p>Here's an example of what a response message might look like:</p>
      <p><code>HTTP/1.1 200 OK<br>Connection: close<br>Date: Sat, 16 Sep 2023 19:23:09 GMT<br>Server: Apache/2.2.3 (CentOS)<br>Last-Modified: Sat, 16 Sep 2023 18:34:09 GMT<br>Content-Length: 6821<br>Content-Type: text/html<br><br>(the requested data...)</code></p>
      <p>The first line is the <b>status line</b>. There are three fields: the protocol version, the status code, and the status message.</p>
      <p>The lines below are the <b>header lines</b>. The <code>Date:</code> header line specifies when the HTTP response was created and sent by the server. The <code>Server:</code> header line specifies the type of the Web server. <code>Last-Modified:</code> is when the object was created or last modified. <code>Content-Length:</code> specifies the object's size in bytes.</p>
      <p>The <b>entity body</b> contains the actual object (the requested data).</p>
      <h3>Cookies! ðŸª</h3>
      <p>HTTP is stateless, so the server doesn't store information about the requests that the clients are making. However, servers sometimes want a way to identify users so that they can restrict access or provide user-specific information when responding to the client. This is where cookies come in. Cookies allow sites to keep track of users*.</p>
      <p>*Technically, cookies keep track of browsers. But users usually use one browser, and that browser is usually used by only one user (unless it's a shared computer). So tracking a browsing session is essentially tracking a user.</p>
      <p>Let's say we go to our favorite shopping site on our favorite browser. The site will create a unique ID (let's say 9876) for this request, store the ID in its database, and send the ID back in the response. The response will include a <code>Set-cookie:</code> header; this tells the browser to create and store a cookie file. For example, the header will look like <code>Set-cookie: 9876</code>, so the browser will store 9876 in a file. Now, as we navigate through our shopping site, the site can ask the browser to include this cookie in every request. Then the site will query its database for this ID to identify the user. If the ID exists in the database, the site will know the user has visited before, and if the ID isn't in the database, the site will consider this a new user visiting for the first time.</p>
      <p>When we say that the site can identify a user, the site doesn't actually know who we are, like our name and stuff. Until we enter that information ourselves. When we enter our information to buy something or to fill out a form, the site can store that information and associate that with the ID in the cookie. So the next time we visit that site, it can look at the ID and the information linked to that ID.</p>
      <h3>Web Caching</h3>
      <p>A <b>Web cache</b> is a server that stores Web pages and objects that have been recently requested.</p>
      <p>When a browser requests an object, it sends the request to a Web cache. If the Web cache has a copy of the requested object, the Web cache sends it to the browser. If it doesn't, the Web cache requests it from the origin server, which sends the object to the Web cache. The Web cache stores a copy of it in its storage and sends that copy to the browser.</p>
      <p>Having a Web cache can significantly speed things up. If the server is physically located far away from the client or if the bottleneck link between the server and client is really bad, then using a Web cache can reduce response times, assuming the Web cache is physically closer to the client or the bottleneck link between the Web cache and client is faster than the bottleneck link between the server and client.</p>
      <div class="ln-box">
        <p>Let's look at a hypothetical access network connected to the Internet. The router in the access network is connected to the router in the Internet by a `15` Mbps link. The links in the access network have a transmission rate of `100` Mbps. Suppose the average object size is `1` Mbits and that the average request rate is `15` requests per second. Also suppose that it takes `2` seconds on average for data to travel between the router in the Internet and the origin servers (we'll informally call this "Internet delay"). And that it takes `0.01` seconds on average for data to travel in the access network.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/web_cache.png">
        </div>
        <p>Recall that traffic intensity is `(La)/R` where `La` is the average rate at which bits are traveling and `R` is the transmission rate of the link.</p>
        <p>On the access network, `15` requests per second and `1` Mbits per request means there are `15` Mbits traveling per second. So the traffic intensity is</p>
        <div class="ln-center">
          <p>`(La)/R=15/100=0.15`</p>
        </div>
        <p>which isn't too bad. In fact, the delay on the access network is negligible for a traffic intensity of `0.15`.</p>
        <p>On the access link, the traffic intensity is</p>
        <div class="ln-center">
          <p>`(La)/R=15/15=1`</p>
        </div>
        <p>which is really bad. The average response time could be in minutes. (Recall that queuing delay increases as traffic intensity approaches `1`.)</p>
        <p>One solution is to upgrade the access link, but that's costly.</p>
        <p>Instead, let's consider placing a Web cache in the access network. Suppose that there is a `40%` chance that the cache can satisfy a request (i.e., the hit rate is `0.4`). This means only `60%` of the requests are going through the access link, so the traffic intensity is `1*0.6=0.6`.</p>
        <p>The average delay is the sum of the access network delay, access link delay, and Internet delay. The access network delay is `0.01` seconds; the access link delay is negligible with a traffic intensity of `lt 0.8`; and the Internet delay is `2` seconds. Since `40%` of the requests stay in the access network and `60%` of the request go to the Internet, the average delay is</p>
        <div class="ln-center">
          <p>`0.4*(0.01+0+0)+0.6*(0.01+0+2)=0.004+1.206=1.21` seconds</p>
        </div>
        <p>If we had upgraded the access link instead of using a Web cache, the average delay would be at least `2` seconds from the Internet delay alone. So we can see that using a Web cache provides faster response times.</p>
      </div>
      <h4>Conditional GET</h4>
      <p>The problem with using a Web cache is that objects in the Web cache can get outdated since the Web cache only stores a copy of the object while the actual object sits at the server and can be modified. The simple fix is for the Web cache to send a request to the server to check the last time the object was modified.</p>
      <p>Let's say the browser sends a request to the Web cache. Suppose the Web cache doesn't have it, so it will send a request to the server. The server will send the object to the Web cache:</p>
      <p><code>HTTP/1.1 200 OK<br>Date: Sun, 17 Sep 2023 19:10:09<br>Server: Apache/1.3.0 (Unix)<br>Last-Modified: Sun, 17 Sep 2023 18:30:09<br>Content-Type: image/gif<br><br>(the requested data...)</code></p>
      <p>Now, the Web cache will send the object to the browser and make a copy of the object. Let's say the browser requests the same object one week later. The Web cache has a copy of the object this time, but it will send a conditional GET to the server first to check if there have been any changes:</p>
      <p><code>GET /fruit/kiwi.gif HTTP/1.1<br>Host: www.exotiquecuisine.com<br>If-modified-since: Sun, 17 Sep 2023 18:30:09</code></p>
      <p>Let's say the object hasn't been modified since then. The server will then send this to the Web cache:</p>
      <p><code>HTTP/1.1 304 Not Modified<br>Date: Sun, 24 Sep 2023 15:39:09<br>Server: Apache/1.3.0 (Unix)</code></p>
      <p>The entity body will be empty since it is pointless to send the object. However, if the object had been modified, the object would have been sent in the entity body.</p>
      <h2 id="email">Email</h2>
      <p>The Internet's mail system has three components: user agents, mail servers, and the Simple Mail Transfer Protocol (SMTP). <b>User agents</b> are applications that allow us to read and write emails, like Gmail and Outlook. <b>Mail servers</b> store emails in <b>mailboxes</b> and send emails to other peoples' mailboxes. Before being sent, emails wait in the sender's mail server's <b>message queue</b>.</p>
      <div class="ln-box">
        <p>We, as individuals, have our own mail servers (in a sense). This may sound a little weird (what? I'm running a server?). When we create an email account on, say, Google, Google manages a mail server for us. This mail server is shared with other Google users. (Note, the mail server is shared, not the mailbox.)</p>
      </div>
      <h3>SMTP</h3>
      <p><b>SMTP</b> is the application-layer protocol for email applications. Specifically, it is the protocol for <em>sending</em> emails (not receiving). It uses TCP as the underlying transport-layer protocol (which is probably not surprising). When a mail server sends mail, it is an SMTP client. When a mail server receives mail, it is an SMTP server.</p>
      <div class="ln-box">
        <p>SMTP requires emails to be encoded using 7-bit ASCII. This restriction is a result of SMTP being around since the 1980s, when people weren't sending a lot of emails, much less emails with large attachments like images and videos.</p>
      </div>
      <ol>
        <li>Person 1 uses their user agent to write and send an email</li>
        <li>Person 1's user agent sends the email to Person 1's mail server, which places the email in its message queue</li>
        <li>Person 1's mail server opens a TCP connection to Person 2's mail server</li>
        <li>After TCP's whole handshaking thing, Person 1's mail server sends the email</li>
        <li>Person 2's mail server receives the email and puts it in Person 2's mailbox</li>
        <li>Person 2 uses their user agent to read the email</li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/email.png">
      </div>
      <div class="ln-box">
        <p>Person 1's user agent sends emails to Person 1's mail server, which then sends emails to Person 2's mail server. It may seem more efficient for Person 1's user agent to send emails directly to Person 2's mail server. This won't work though because user agents don't have a way to deal with Person 2's mail server being unreachable for whatever reason. Mail servers, which are always on and always connected to the Internet, keep retrying every `n` minutes until it works.</p>
      </div>
      <p>SMTP uses persistent connections. So if a mail server has multiple emails to send to the same mail server, then it can use the same TCP connection to send all the emails.</p>
      <div class="ln-box">
        <p>In HTTP, each object is sent in its own response message. In SMTP, all of the objects are placed into one email.</p>
      </div>
      <div class="ln-box">
        <p>After the TCP handshake, there is another SMTP handshake that takes place between the mail servers before they can send and receive emails.</p>
      </div>
      <h3>Mail Message Formats</h3>
      <p>An email consists of a header and a body. The header is kinda like the metadata of the email. An example:</p>
      <p><code>From: me@gmail.com<br>To: you@gmail.com<br>Subject: Thanks for reading this</code></p>
      <p>And, of course, the body contains the message.</p>
      <h3>Mail Access Protocols</h3>
      <p>As mentioned eariler, SMTP is used only for sending emails. For receiving emails, there are two main protocols. Web-based applications, like Google's Gmail website and app, use HTTP. Some other clients, like Outlook, use <b>Internet Mail Access Protocol (IMAP)</b>.</p>
      <p>Both of these protocols also allow us to manage our emails. For example, we can move emails into folders, delete emails, and mark emails as important.</p>
      <div class="ln-box">
        <p>Mail servers only use SMTP to send emails. User agents on the other hand, can use SMTP or HTTP to send emails to the mail server.</p>
      </div>
      <h2 id="dns">DNS</h2>
      <p>As we probably know, computers like to work with numbers. So naturally, servers are uniquely identified by a set of numbers (IP address). However, we don't type a bunch of numbers if we want to go to Google â€” we type "google.com" (does anyone actually type this to Google something?). "google.com" is what's known as a <b>hostname</b>, which is a human-readable name for a server.</p>
      <h3>Services Provided by DNS</h3>
      <p>The Internet's <b>domain name system (DNS)</b> is what allows us to type human-readable names to go to websites. It converts human-readable hostnames to IP addresses.</p>
      <div class="ln-box">
        <p>DNS is a distributed database deployed on a bunch of <b>DNS servers</b>.</p>
        <p>It is also an application-layer protocol that allows hosts to query the database. It uses UDP.</p>
      </div>
      <p>Here's how it works:</p>
      <ol>
        <li>There is a DNS application running on our phone/computer</li>
        <li>The browser sends the hostname to the DNS application</li>
        <li>The DNS application uses the hostname to query a DNS server for the IP address</li>
        <li>The DNS application receives the IP address and sends it to the browser</li>
        <li>The browser uses the IP address to open a TCP connection</li>
      </ol>
      <p>From this, we can see yet another source of delay: translating the hostname to an IP address.</p>
      <p>DNS also provides additional related services. Sometimes, a hostname can be complicated, like "relay1.west-coast.enterprise.com". An alias can be set up so that we can type "enterprise.com" instead of that long hostname. The long (real) hostname is called the <b>canonical hostname</b>. This is called <b>host aliasing</b>. Something similar is available for mail servers too. "gmail.com" is more likely an alias than an actual hostname for a mail server (which could be something like "relay1.west-coast.gmail.com"). This is <b>mail server aliasing</b>.</p>
      <p>Companies often have several servers running their website. Each server has its own IP address, but those IP addresses should all point to the same website. The DNS database stores this set of IP addresses and rotates between the IP addresses so that traffic is distributed evenly across all servers. So DNS also provides <b>load distribution</b>.</p>
      <h3>How DNS Works</h3>
      <p>DNS is a distributed database organized in a hierarchy. At the top are <b>root DNS servers</b>. This is where the translation starts. The DNS application queries the root DNS server, which then looks at the top-level domain (e.g., com, org, edu, gov, net). It does this because there are dedicated DNS servers for each top-level domain (i.e., there are DNS servers for all .com websites, DNS servers for all .edu websites, etc.). Appropriately, they're called <b>top-level domain (TLD) servers</b>. So the root server will tell the DNS application which TLD server to contact. The TLD server will send back to the DNS application the IP address of the <b>authoritative DNS server</b>, which is where the actual IP address of the website sits. So finally, the DNS application will contact the authoritative DNS server for the IP address of the website.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/dns.png">
      </div>
      <p>There's also a <b>local DNS server</b>, which aren't part of the hierarchy for some reason. The local DNS server is the entry point into the DNS hierarchy; the DNS application first contacts the local DNS server, which then contacts the root DNS server. Local DNS servers are managed by ISPs.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/dns_2.png">
        <p>1. Local DNS! What is google.com's IP address?</p>
        <p>2. Root DNS! What is google.com's IP address?</p>
        <p>3. Go ask a com TLD server. Here's the IP address of one.</p>
        <p>4. TLD DNS! What is google.com's IP address?</p>
        <p>5. Go ask one of Google's authoritative servers. Here's the IP address of one.</p>
        <p>6. Authoritative DNS! What is google.com's IP address?</p>
        <p>7. The IP address is 127.0.0.1.</p>
        <p>8. I finally know google.com's IP address. It's 127.0.0.1.</p>
      </div>
      <div class="ln-box">
        <p>In the example above, the queries from the local DNS server (2, 4, and 6) are <b>iterative queries</b>. This means that the responsibility of asking the next query is placed on the server that asked the query.</p>
        <p>There are also <b>recursive queries</b>, in which the responsibility of asking the next query is placed on the server who got asked. In the example above, 1 is a recursive query.</p>
        <p>Here's an example where all the queries are recursive.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/dns_3.png">
          <p>1. Local DNS! What is google.com's IP address?</p>
          <p>2. Root DNS! What is google.com's IP address?</p>
          <p>3. TLD DNS! What is google.com's IP address?</p>
          <p>4. Authoritative DNS! What is google.com's IP address?</p>
          <p>5. TLD DNS! The IP address is 127.0.0.1.</p>
          <p>6. Root DNS! The IP address is 127.0.0.1.</p>
          <p>7. Local DNS! The IP address is 127.0.0.1.</p>
          <p>8. I finally know google.com's IP address. It's 127.0.0.1.</p>
        </div>
        <p>In practice though, most queries are iterative.</p>
      </div>
      <h4>DNS Caching</h4>
      <p>As we saw earlier, there are a lot of queries that get made when going through the hierarchy. This traversal can be skipped if the DNS server stores the IP address of whatever was queried. For example, a local DNS server can store the IP address of a website, so that if another host wants to visit the same website, the local DNS server doesn't have to ask all those other DNS servers; it just returns the IP address it stored. This cache isn't good for very long though (typically 2 days).</p>
      <div class="ln-box">
        <p>Especially for .com websites, root DNS servers are often bypassed since the local DNS server should typically have the IP address of the appropriate TLD server in its cache. How many .com websites do we (as a whole) go to in a day?</p>
      </div>
      <h3>DNS Records and Messages</h3>
      <p>The entries in the database are called <b>resource records</b>. A resource record is a four-tuple that looks like this: <code>(Name, Value, Type, TTL)</code>. <code>TTL</code> stands for "time to live", which indicates when the record should be removed from a cache. The <code>name</code> and <code>value</code> depend on what the <code>type</code> is. (We'll ignore the TTL in the examples below.)</p>
      <p>Type A records are hostname-to-IP mappings. <code>(relay1.abc.def.com, 145.37.93.126, A)</code>. The <code>name</code> is the hostname and the <code>value</code> is the IP address for that hostname.</p>
      <p>Type NS records are domain-to-DNS-server mappings. <code>(abc.com, dns.abc.com, NS)</code>. The <code>name</code> is a domain and the <code>value</code> is the hostname of a DNS server.</p>
      <p>CNAME records are alias-to-canonical-hostname mappings. <code>(abc.com, relay1.abc.com, CNAME)</code>. The <code>name</code> is an alias and the <code>value</code> is the canonical hostname.</p>
      <p>MX records are the equivalent of CNAME records but for mail servers. <code>(abc.com, mail.abc.com, MX)</code>.</p>
      <div class="ln-box">
        <p>Type AAAA records are also hostname-to-IP mappings, but for IPv6.</p>
      </div>
      <h4>Inserting Records into the DNS Database</h4>
      <p>If we want to register a domain name, say "ourwebsite.com", we do so through a <b>registrar</b>. A registrar makes sure the domain is unique and inserts the appropriate records into the DNS database.</p>
      <div class="ln-box">
        <p>The registrar also needs the names and IP addresses of the primary and secondary authoritative DNS servers. Once that is provided, the registrar inserts a Type NS record and a Type A record into the appropriate TLD server. For example, <code>(ourwebsite.com, dns1.ourwebsite.com, NS)</code> and <code>(dns1.ourwebsite.com, 199.199.199.199, A)</code> are inserted into the TLD servers for .com websites. So when a (local) DNS server contacts a TLD server, the TLD server will return both the Type NS record and the Type A record.</p>
      </div>
      <div class="ln-box">
        <p><code>nslookup</code> is a tool for performing DNS queries.</p>
        <p>Here's a sample output for <code>nslookup google.com</code>:</p>
        <p><code>Server: dns-cac-lb-01.rr.com</code><br><code>Address: 3002:1997:f01:1::1</code><br><br><code>Non-authoritative answer:</code><br><code>Name: google.com</code></br><code>Addresses: 2607:f8b0:4007:80f::200e</code><br><code>172.217.14.110</code></p>
        <p>The <code>Server</code> is the DNS server to which the query is sent. If no DNS server is specified in the command, then it is the default DNS server, which is usually the local DNS server.</p>
        <p><code>Non-authoritative answer</code> means the answer is not from an authoritative DNS server. What happened was that the local DNS server queried a non-authoritative server that had the answer in its cache.</p>
        <p>Here, there happened to be two answers returned. <code>2607:f8b0:4007:80f::200e</code> and <code>172.217.14.110</code> are two separate servers for google.com.</p>
        <hr>
        <p>Here's a sample output for <code>nslookup -type=NS google.com</code>:</p>
        <p><code>Server: dns-cac-lb-01.rr.com</code><br><code>Address: 3002:1997:f01:1::1</code><br><br><code>Non-authoritative answer:</code><br><code>google.com nameserver = ns2.google.com</code><br><code>google.com nameserver = ns1.google.com</code><br><br><code>ns1.google.com internet address = 216.239.32.10</code><br><code>ns2.google.com internet address = 216.239.34.10</code></p>
        <p>This time, we're asking for Type NS records, so we'll see hostnames instead of IP addresses in the (first two lines of the) answer. Specifically, these are the hostnames for Google's authoritative DNS servers.</p>
        <p>In the second half of the answer are Type A records for the authoritative DNS servers. Even though we didn't ask for Type A records, the local DNS server gave it to us for free.</p>
        <hr>
        <p>Here's a sample output for <code>nslookup google.com ns2.google.com</code>:</p>
        <p><code>Server: ns2.google.com</code><br><code>Address: 2001:4860:4802:34::a</code><br><br><code>Name: google.com</code></br><code>Addresses: 2607:f8b0:4007:809::200e</code><br><code>142.250.176.14</code></p>
        <p>This command is sending the query to a specified DNS server (ns2.google.com) instead of to the local DNS server. Note that there's no <code>Non-authoritative answer</code>; this makes sense because we're querying an authoritative DNS server for google.com.</p>
        <p>We might expect the address of the DNS server to match the address from the previous Type NS query, since they're both ns2.google.com. Depending on configurations, queries will return either an IPv4 or IPv6 address. So for all we (I) know, they may be the same server. Also, there are other reasons why IP addresses might not match in general: cache expiration, round robin (refer to DNS load distribution), and server scaling are a few.</p>
      </div>
      <div class="ln-box">
        <p><code>ipconfig</code> (on Windows) is a tool that allows us to see IP addresses of things.</p>
        <p>Here's a sample partial output for <code>ipconfig /all</code>:</p>
        <p><code>Wireless LAN adapter Wi-Fi:</code><br><code>IPv6 Address. . . . . . . . . . . : 3912:6001:f122:5b21:b5c7:284d:6b93:3a12(Preferred)</code><br><code>Temporary IPv6 Address. . . . . . : 3912:6001:f122:5b21:e6a2:778a:3c12:ab92(Preferred)</code><br><code>IPv4 Address. . . . . . . . . . . : 173.146.1.189(Preferred)</code><br><code>DNS Servers . . . . . . . . . . . : 3002:1997:f01:1::1</code></p>
        <p>There will likely be many network interfaces shown, but since I'm connected to the Internet via WiFi, the WiFi one is the relevant one. Note that the IP address of the DNS server matches the IP address from <code>nslookup</code>.</p>
      </div>
      <h2 id="p2p">Peer-to-Peer File Distribution</h2>
      <p>We saw this earlier, but in peer-to-peer applications, there are no servers sending data to clients. Instead, devices connect to each other and are called peers. Specifically for file distribution, peers send/receive portions of files to/from other peers.</p>
      <div class="ln-box">
        <p>It may be weird to think that only portions of files get distributed. While it's possible for a peer to have the whole file in storage, the peer may choose/have to only share a portion of the file with other peers for whatever reason. The peers who only got a portion of the file will share whatever they got with other peers who don't have it yet.</p>
      </div>
      <div class="ln-box">
        <h3>Scalability of P2P Architectures</h3>
        <p>Suppose there is one server that contains a file with `F` bits. There are `N` peers that want to download the file. Let `u_s` be the upload rate of the server and `d_i` be the download rate of the `i^(th)` peer.</p>
        <p>First we'll look at the <b>distribution time</b> (the time it takes for all `N` peers to get the file) in the client-server architecture, which we'll denote `D_(cs)`. The distribution time depends on two things: how long it takes to distribute the file and how long it takes to download the file.</p>
        <p>To distribute the file, the server has to transfer `F` bits to `N` peers, so the server has to transfer a total of `NF` bits. The server's upload rate is `u_s`, so the distribution time is at least `(NF)/u_s`.</p>
        <p>To download the file, peers will download the file at different rates, so let `d_(min)` be the download rate of the slowest peer, i.e., `d_(min)=min{d_1, d_2, ..., d_n}`. This peer will take `F/d_(min)` seconds to receive the file, so the distribution time is also at least `F/d_(min)`.</p>
        <p>Combining these two, the distribution time is</p>
        <div class="ln-center">
          <p>`D_(cs)gemax{(NF)/u_s, F/d_(min)}`</p>
        </div>
        <p>For large `N`, `(NF)/u_s` will be larger than `F/d_(min)`, so the distribution time can be determined by `(NF)/u_s`. This means the distribution time will be higher the more peers there are. Mathematically, the distribution time increases linearly with the number of peers.</p>
        <p>Now we'll look at the distribution time for peer-to-peer architecture, which we'll denote as `D_(P2P)`. Since peers can now upload, let `u_i` be the upload rate of the `i^(th)` peer.</p>
        <p>The server only has to transfer `F` bits once instead of to each peer. This is because the peers can transfer the bits among themselves. With an upload rate of `u_s`, the distribution time is at least `F/u_s`.</p>
        <p>Again, the peer with the lowest download rate will affect distribution time, so the distribution time will also be at least `F/d_(min)`.</p>
        <p>Looking at the system as a whole, there are `NF` bits that need to be transferred. With everyone uploading, the total upload rate is `u_text(total)=u_s+u_1+...+u_N=u_s+sum_(i=1)^Nu_i`. So the distribution time is also at least `(NF)/(u_s+sum_(i=1)^Nu_i)`.</p>
        <p>Combining these three, the distribution time is</p>
        <div class="ln-center">
          <p>`D_(P2P)gemax{F/u_s,F/d_(min),(NF)/(u_s+sum_(i=1)^Nu_i)}`</p>
        </div>
        <p>This time, for large `N`, the distribution time doesn't necessarily increase as `N` increases. While having more peers does mean there are more bits that need to be transferred, it also means that there are more peers to help with uploading.</p>
      </div>
      <h4>BitTorrent</h4>
      <p>BitTorrent is a P2P protocol for file distribution. A group of peers uploading and downloading a particular file is called a torrent. A file is broken up into equal-size chunks (typically 256 kilobytes), so peers in a torrent upload and download files in chunks.</p>
      <div class="ln-box">
        <p>Peers who have not received the whole file yet are called leechers. After a peer has received the whole file, they can choose to leave or stay in the torrent. Those who stay are called seeders. These are the good guys since they help other peers get the file. Those who leave are selfish.</p>
      </div>
      <p>Each torrent has a tracker, which is responsible for keeping track of peers in the torrent. When a new peer joins a torrent, the tracker randomly selects a subset of existing peers and sends their IP addresses to the new peer. The new peer then tries to establish TCP connections to these existing peers. We'll refer to succcessfully connected peers as "neighboring peers". The tracker continuously sends IP addresses to new and existing peers as peers leave and join the torrent.</p>
      <p>Let's say we join a torrent. While downloading the file, we will periodically ask our neighboring peers for a list of chunks they have. Let's say this is what the list looks like:</p>
      <ul>
        <li>Peer A has chunks 1, 3, 5, 7, 9</li>
        <li>Peer B has chunks 1, 2, 3</li>
        <li>Peer C has chunks 1, 3, 10</li>
        <li>Peer D has chunks 1, 5, 7, 9, 10</li>
      </ul>
      <p>Suppose we only have chunks 1 and 3 so far. For deciding which peer to request chunks from, BitTorrent uses a technique called <b>rarest first</b>, meaning that we find the chunk that has the fewest copies and download that first. So in this example, 2 is the rarest chunk since only Peer B has it. The idea is that we want to distribute the rarest chunks first to equalize all the chunks, which makes the file easier to download. If only one peer is holding the rarest chunk and that peer leaves the torrent, then no one else in the torrent can download the whole file since they're missing that rarest chunk.</p>
      <p>While downloading chunks, we will also have peers asking us for chunks that we have so far. BitTorrent will prioritize the peers who are giving us data at the highest rate. Let's say we respond to the top 4 peers; these peers are <b>unchoked</b>. However, if we only focus on our top 4, then other peers will ignore us when we want more chunks (since we're not on their top 4). So we will also randomly choose a peer to respond to; this peer is <b>optimistically unchoked</b>. The idea is that we want to (optimistically) become someone else's unchoked peer by being nice to them (giving them our chunks) so that they will be nice to us (and give us their chunks). This also helps new peers who don't have the power to upload at high rates, since they will occasionally receive chunks as an optimistically unchoked peer.</p>
      <p>This process of prioritizing the top peers is called tit-for-tat.</p>
      <h2 id="cdn">Video Streaming and Content Distribution Networks</h2>
      <h3>Internet Video</h3>
      <p>A video is a bunch of images displayed really quickly (typically 24 or 30 images per second). Images are made up of pixels, where a pixel contains the image's color at that pixel's location. And computers see pixels as a bunch of bits.</p>
      <p>Videos can be compressed by reducing the number of pixels in the images. Reducing the number of pixels reduces the number of bits, thereby reducing the bit rate of the video. Bit rate can be thought of as the number of bits being shown per second. The more bits there are, the more pixels there are, which means a better video quality. Compression is used to create different versions of the same video, with each version having a different quality.</p>
      <div class="ln-box">
        <p>There are (at least?) two types of compression algorithms.</p>
        <p>Spatial compression: If there's an image with a bunch of blue pixels (think sky or ocean), then instead of storing all of those pixels, we can store just two things: the color of the pixel and how many times it is repeated consecutively.</p>
        <p>Temporal compression: Consider an image `i`. The image `i+1` is unlikely to be drastically different from image `i`. So instead of sending the whole image, we can send just the differences between image `i` and image `i+1`.</p>
      </div>
      <p>In order for a video to be able to be played continuously, the average throughput of the network must be at least as large as the bit rate. In other words, the network must be able to transfer bits as fast as the video can show them.</p>
      <h3>HTTP Streaming and DASH</h3>
      <p>Streaming is playing videos without downloading the whole video first. Since a video is likely too large to send all at once, the server storing the video sends it to the client in chunks at a time. The client plays whatever chunks it has received and this repeats until the whole video is sent.</p>
      <p>The problem with this is that all clients receive the same video, regardless of the strength of their Internet connection (i.e. bandwidth). So clients with low bandwidth will struggle to play videos with high bit rates. This is where DASH comes in. In <b>Dynamic Adaptive Streaming over HTTP</b>, videos are compressed into different versions, so that clients with high bandwidth can play videos with high bit rates and clients with low bandwidth can play videos with low bit rates. This also works if the client's bandwidth fluctuates; when the bandwidth is high, the client will get the bytes from the high-quality video and when the bandwidth is low, the client will switch to getting bytes from the lower-quality video.</p>
      <p>The client knows about the different versions of the video because there is a <b>manifest file</b> on the server. The manifest file has the URL and bit rate of all the versions of the video.</p>
      <h3>Content Distribution Networks</h3>
      <p>Let's say an Internet video company (like YouTube) has a huge data center where they store and stream all their videos. Of course, there are several problems with having just one data center:</p>
      <ul>
        <li>The data center can't possibly be conveniently located for everyone in the world. The farther away a client is, the more links there are for the bytes to go through. And the more links there are, the more likely it is for the bytes to hit a bottleneck link, reducing throughput.</li>
        <li>Popular videos will also be sent over and over again, which is costly and inefficient because the company has to pay their ISP for sending the same bytes over and over again.</li>
        <li>And finally, a single data center is a single point of failure. If something goes wrong with that data center, no one can watch anything.</li>
      </ul>
      <p>It's probably obvious at this point, but <b>Content Distribution Networks (CDNs)</b> are networks of distributed servers and data centers, which we'll call clusters. These clusters all have copies of the videos so that clients can connect to the best CDN for them.</p>
      <p>There are two main strategies for placing clusters. The first is to <b>enter deep</b>, where the clusters are "deep" into the access network. With many clusters placed in access ISPs, they are physically located closer to us, reducing the number of links and routers. The drawback to this strategy is that there are many clusters that need to be deployed and managed.</p>
      <p>The second strategy is to <b>bring home</b>, where the clusters "bring the ISPs home" by sitting closer to the network core in Internet exchange points (IXPs). This is easier to manage because there are fewer clusters, but there are more links and routers, which means potentially higher delay and lower throughput.</p>
      <p>The clusters typically don't have copies of every single video. If the cluster needs a video it doesn't have, it pulls the video from the server or another cluster.</p>
      <h4>CDN Operation</h4>
      <p>Suppose a video company called NetCinema uses a third-party CDN company called KingCDN. Let's say we wanted to watch a video whose URL is "http://video.netcinema.com/6Y7B23V".</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/cdn.png">
      </div>
      <ol>
        <li>Our computer sends a DNS query for video.netcinema.com</li>
        <li>The local DNS server contacts an authoritative DNS server for NetCinema.</li>
        <li>The authoritative DNS server sees the word "video" in the hostname and returns a hostname in the KingCDN's domain, e.g., a1105.kingcdn.com</li>
        <li>The local DNS server contacts an authoritative DNS server for KingCDN to get the IP address of a1105.kingcdn.com</li>
        <li>The authoritative DNS server for KingCDN returns the IP address</li>
        <li>The local DNS server sends the IP address back to our browser</li>
        <li>Our browser sets up the TCP connection and plays the video</li>
      </ol>
      <p>Notice that our browser doesn't set up the TCP connection with the video company; it sets up the TCP connection with the CDN. The CDN intercepted the request so that it could direct the browser to one of its clusters.</p>
      <h4>Cluster Selection Strategies</h4>
      <p>So how do CDNs determine what the best cluster is for a client? One simple way is to pick the cluster that is geographically closest to the client's local DNS. This works most of the time, but sometimes, clusters can be physically close to a client and have a ton of links and routers between them. Also, some clients can use local DNSs located far away from them, so the cluster that's closest to the local DNS isn't close to the client.</p>
      <p>The better approach is to do some research. CDNs can perform <b>real-time measurements</b> of delay and performance between clusters and clients by sending pings to local DNSs. This doesn't always work though since local DNSs can be configured to ignore these pings.</p>
      <hr>
      <p>Now to the transport layer!</p>
      <hr>
      <h2 id="transportlayer">Transport-Layer Services</h2>
      <p>The transport layer provides <b>logical communication</b> between applications. Logical communication means that, from the application's point of view, it is directly connected to another application. Of course, we know that physically, there are many routers and links between the applications, but logically, it can be seen as one big direct connection. The transport layer abstracts away all the hardware and details to the application layer.</p>
      <h3>Relationship Between Transport and Network Layers</h3>
      <p>The transport layer provides logical communication between <em>processes</em> running on different hosts. The network layer provides logical communication between <em>hosts</em>. We'll distinguish the two with an analogy.</p>
      <p>Suppose there are two families: one in California and one in New York. These people are old-fashioned and communicate to each other via snail mail. In each family, there is one person responsible for giving the mail to the postal service and giving the mail to the appropriate family member.</p>
      <p>Since the postal service moves mail from house to house, the postal service provides logical communication between the two houses. Since the dedicated mail person in each family moves mail from person to person, the dedicated mail people provide logical communication between the families. The postal service is analogous to the network layer and the dedicated mail person is analogous to the transport layer.</p>
      <p>The services provided by the transport layer is constrained by the services provided by the network layer. Going back to the analogy, if the postal service takes as long as it wants to deliver mail, then the dedicated mail person can't guarantee how long it will take to receive mail. However, the transport layer can provide some guarantees of its own regardless of whatever happens in the network layer. For example, the transport layer can guarantee reliable data transfer even if the network layer loses or corrupts data.</p>
      <h3>Overview of the Transport Layer in the Internet</h3>
      <p>Remember TCP and UDP? These are the two transport-layer protocols provided by the Internet to the application layer. The responsibility of these two protocols is to extend the network layer's delivery service. In other words, once the network layer has done its job delivering data from host to host, it is now the transport layer's job to continue the data delivery to the appropriate process. Extending host-to-host delivery to process-to-process delivery is called transport-layer multiplexing and demultiplexing.</p>
      <h2 id="multiplexing">Multiplexing and Demultiplexing</h2>
      <p>Recall that data in the transport layer are called segments. The process of delivering the segment to the correct socket is called <b>demultiplexing</b> (network layer `rarr` transport layer `rarr` application layer). The process of collecting a message from a socket, encapsulating it into a segment, and sending the segment to the network layer is called <b>multiplexing</b> (application layer `rarr` transport layer `rarr` network layer).</p>
      <p>Each socket has a unique identifier (port number) and each segment has a header specifying which socket it needs to go to. Specifically, the header contains the <b>source port number field</b> and the <b>destination port number field</b>. A port number can be any number from 0 to 65535. Ports 0 to 1023 are <b>well-known port numbers</b> and are reserved for common application protocols.</p>
      <h4>Connectionless Multiplexing and Demultiplexing</h4>
      <p>Suppose Host A wants to send data to Host B through a UDP connection. The transport layer in Host A creates a segment with the application message, source port number, and destination port number and sends this segment into the network layer. (Multiplexing complete.) The network layer sends it to Host B's network layer, which sends it to Host B's transport layer. The transport layer looks at the destination port number and sends the segment to the appropriate socket. (Demultiplexing complete.)</p>
      <div class="ln-box">
        <p>The transport layer didn't even look at the source port number at any time. So why is that included in the segment header? If Host B wants to send data back to Host A, then it can look at the source port number to know where to send data to.</p>
      </div>
      <h4>Connection-Oriented Multiplexing and Demultiplexing</h4>
      <p>The process is pretty much the same for TCP sockets. In the TCP case, the transport layer looks at the source IP address, source port number, destination IP address, and destination port number to identify the correct socket. This is because TCP is connection-oriented, so it matters where the data is coming from.</p>
      <p>In the UDP case, the transport layer only looked at the destination port number (and the destination IP address, which I didn't mention to simplify things) to identify the correct socket.</p>
      <div class="ln-box">
        <p>If two UDP segments have different source ports (or different source IP addresses) and the same destination port (and same IP address), then the two segments will be sent to the same UDP socket.</p>
        <p>If two TCP segments have different source ports (or different source IP addresses) and the same destination port (and same IP address), then the two segments will be sent to different TCP sockets.</p>
      </div>
      <h2 id="udp">Connectionless Transport: UDP</h2>
      <p>UDP is so bare bones that it does about as much as the underlying network layer protocol: IP. Pretty much the only thing it does that IP doesn't do is multiplexing and demultiplexing.</p>
      <p>Even though UDP doesn't guarantee reliable data transfer, there are several reasons why an application would want to use UDP:</p>
      <ul>
        <li>They want to bypass TCP's congestion-control mechanism. They don't really care that some data gets lost.</li>
        <li>They want to bypass TCP's handshaking. They want their data to get there as fast as possible.</li>
        <li>They want to support more active clients. UDP doesn't need to maintain connection state.</li>
      </ul>
      <div class="ln-box">
        <p>Even though UDP isn't always reliable, an application using UDP can still have reliable data transfer by adding reliability mechanisms to the application.</p>
      </div>
      <h3>UDP Segment Structure</h3>
      <p>A UDP segment consists of five pieces:</p>
      <ul>
        <li>source port number (2 bytes)</li>
        <li>destination port number (2 bytes)</li>
        <li>length (2 bytes)
          <ul>
            <li>specifies the size of the whole segment (header + application data)</li>
          </ul>
        </li>
        <li>checksum (2 bytes)</li>
        <li>application data</li>
      </ul>
      <table class="table table-bordered text-center">
        <tr>
          <td>Source Port #</td>
          <td>Destination Port #</td>
        </tr>
        <tr>
          <td>Length</td>
          <td>Checksum</td>
        </tr>
        <tr>
          <td colspan="2">Application Data</td>
        </tr>
      </table>
      <h3>UDP Checksum</h3>
      <p>The checksum is used for error checking. Specifically, the checksum is used to see if any of the bits got messed up while getting transferred.</p>
      <div class="ln-box">
        <p>One's complement is an operation performed on binary numbers. Getting the one's complement of a binary number involves flipping every bit, i.e., `1 rarr 0` and `0 rarr 1`. For example, consider the binary number `1100`. The one's complement of that number is `0011`. This operation is called the one's complement because adding the two numbers together results in a number with all `1`s.</p>
      </div>
      <p>Before sending the segment, UDP has to compute the checksum. It does this by converting the segment into 16-bit numbers, adding all the 16-bit numbers, and performing the one's complement. To simplify things, we will look at 4-bit numbers instead. Suppose a segment is converted into three 4-bit numbers:</p>
      <div class="ln-center">
        <p>`0110`</p>
        <p>`0101`</p>
        <p>`1010`</p>
      </div>
      <p>The sum of the first two are</p>
      <div class="ln-center">
        <p>`0110`</p>
        <p>`0101`<br>______</p>
        <p>`1011`</p>
      </div>
      <p>Now adding this with the third number:</p>
      <div class="ln-center">
        <p>`1011`</p>
        <p>`1010`<br>______</p>
        <p>`10101`</p>
      </div>
      <p>There is an overflow here: adding two 4-bit numbers resulted in a 5-bit number. Whenever overflow happens, a "wraparound operation" is applied, where the leftmost bit is taken out and added to the resulting 4-bit number:</p>
      <div class="ln-center">
        <p>`0101`</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`1`<br>______</p>
        <p>`0110`</p>
      </div>
      <p>Now we need to get the one's complement of this sum, which is `1001`. `1001` is the checksum of the segment.</p>
      <p>When UDP on the receiving end gets the segment, it needs to verify that none of the bits got corrupted. Like the UDP sender, the UDP receiver will also add all the 4-bit numbers together. Adding the three 4-bit numbers (with the wrapround) should result in `0110`. Now, instead of performing the one's complement, the UDP receiver will add this with the checksum in the header, which is `1001`. The result should be `1111`.</p>
      <p>In general, if there are no errors, then all the bits will be `1`s*. If there is a `0` somewhere, then UDP knows there is something wrong with this segment. In this case, UDP can either discard the segment or send it to the application with a warning.</p>
      <div class="ln-box">
        <p>*Notice that this is different from saying, "if all the bits are `1`s, then UDP knows that there are no errors."</p>
      </div>
      <h2 id="rdt">Principles of Reliable Data Transfer</h2>
      <p>Here, we'll build out the mechanisms behind a reliable data transfer protocol.</p>
      <div class="ln-box">
        <p>The ideas here apply to a reliable data transfer protocol in general, not just for the transport layer. So the underlying channel doesn't necessarily have to be the network layer (and the layer above doesn't have to be the application layer). Also, instead of "segments", encapsulated data will be generically called "packets".</p>
      </div>
      <p>Suppose our layer is in between an upper layer and a lower layer. Each layer has a channel responsible for sending and receiving data (we'll be building the channel in our layer, so that's why there's no channel for our layer in the drawing). The upper layer calls <code>rdt_send()</code> when it wants to send data into our layer. In our channel is a sender that gets the data and calls <code>udt_send()</code> (the "u" stands for "unreliable") to send the data into the underlying channel. The receiver in our channel gets the data from the underlying channel by calling <code>rdt_rcv()</code> and sends it to the upper layer by calling <code>deliver_data()</code>.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/rdt.png">
      </div>
      <h3>Building a Reliable Data Transfer Protocol</h3>
      <div class="ln-box">
        <p>A finite-state machine is a model that shows what a machine does on certain actions. A machine starts in a certain state and changes its state as certain events take place.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/fsm.png">
        </div>
        <p>The dashed arrow points to the initial state of the machine. A solid arrow indicates a transition from one state to another. For every transition, there is a cause and effect. The cause is the event that triggers the transition (think of it as "why is this transition happening?") and the effect is the actions that are taken when the event occurs.</p>
        <p>&amp;&amp; is shorthand for "and" and || is shorthand for "or". So "cause: event1 &amp;&amp; event2" means that event1 and event2 are causing the transition. And "cause: event1 &amp;&amp; (event2 || event3)" means that event1 and either event2 or event3 are causing the transition.</p>
      </div>
      <h4>Reliable Data Transfer over a Perfectly Reliable Channel: <code>rdt1.0</code></h4>
      <p>First, we'll assume the underlying channel is reliable. This makes things pretty trivial since the underlying channel pretty much did the job for us (the job being reliable data transfer).</p>
      <p>On the sender's side:</p>
      <ol>
        <li>take in the data from the upper layer by calling <code>rdt_send(data)</code></li>
        <li>create a packet by calling <code>make_pkt(data)</code></li>
        <li>send the packet into the underlying channel by calling <code>udt_send(packet)</code></li>
      </ol>
      <p>On the receiver's side:</p>
      <ol>
        <li>take in the packet from the underlying channel by calling <code>rdt_rcv(packet)</code></li>
        <li>extract the data from the packet by calling <code>extract(packet)</code></li>
        <li>send the data to the receiver by calling <code>deliver_data(data)</code></li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt1.png">
      </div>
      <p>Note there is one FSM for the sender and one FSM for the receiver. Each FSM only has one state because at this point, there's not really much to do besides waiting for data to send or receive. In other words, before we can send data, we have to wait for data. And after we send data, we have to wait for more data. So the only thing to do after taking any action is to wait.</p>
      <h4>Reliable Data Transfer over a Channel with Bit Errors: <code>rdt2.0</code></h4>
      <p>Now let's assume the underlying channel may sometimes corrupt some of the bits. First, we need a way to know if the data is corrupted. For this, we have the checksum technique.</p>
      <p>Next, we need a way to let the sender know whether the data received is good or bad. For this, we introduce acknowledgments. If the data is good, then the receiver will send a <b>positive acknowledgment</b> (ACK) to the sender. If the data is bad, then the receiver will send a <b>negative acknowledgment</b> (NAK) to the sender, telling the sender to send the data again.</p>
      <div class="ln-box">
        <p>Reliable data transfer protocols based on retransmission are called <b>Automatic Repeat reQuest (ARQ) protocols</b>.</p>
      </div>
      <p>On the sender's side:</p>
      <ol>
        <li>take in the data from the upper layer by calling <code>rdt_send(data)</code></li>
        <li>create a packet by calling <code>make_pkt(data)</code></li>
        <li>send the packet into the underlying channel by calling <code>udt_send(sndpkt)</code></li>
        <li>wait for and get an acknowledgment by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the acknowledgment is negative, send the packet again by calling <code>udt_send(sndpkt)</code></li>
        <li>otherwise, if the acknowledgment is positive, get ready to get more data from the upper layer</li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt2_sender.png">
        <p>This is the FSM for the <code>rdt2.0</code> sender.</p>
      </div>
      <p>On the receiver's side:</p>
      <ol>
        <li>take in the packet from the underlying channel by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the packet is corrupted:
          <ol>
            <li>make a negative acknowledgment by calling <code>make_pkt(NAK)</code></li>
            <li>send the negative acknowledgment by calling <code>udt_send(sndpkt)</code></li>
          </ol>
        </li>
        <li>otherwise, if the packet is good:
          <ol>
            <li>extract the data from the packet by calling <code>extract(rcvpkt)</code></li>
            <li>send the data to the receiver by calling <code>deliver_data(data)</code></li>
            <li>make a positive acknowledgment by calling <code>make_pkt(ACK)</code></li>
            <li>send the positive acknowledgment by calling <code>udt_send(sndpkt)</code></li>
          </ol>
        </li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt2_receiver.png">
        <p>This is the FSM for the <code>rdt2.0</code> receiver.</p>
      </div>
      <p>By this design, when the sender is waiting for an ACK or NAK, it cannot get more data from the upper layer. The sender won't send any new data until it knows that the last thing it sent was good. This makes our <code>rdt2.0</code> protocol a <b>stop-and-wait</b> protocol.</p>
      <h4><code>rdt2.1</code>: NACK? What's That?</h4>
      <p>One thing we haven't considered yet is what if the ACK or NAK is corrupted? After all, an acknowledgment is a packet, so it too can get corrupted. Let's say the receiver sends a NAK, but the NAK gets corrupted. If the sender decides to not do anything, then the receiver never gets any data at all. Consider the other situation where the receiver sends an ACK, but the ACK gets corrupted. If the sender decides to resend the data, then the receiver will get duplicate packets. The problem here is that the receiver doesn't know that this is retransmitted data and will treat this as new data.</p>
      <p>The solution is rather simple. If the acknowledgment is corrupted, always resend the data, but add a <b>sequence number</b> to the packet. The receiver can then check the sequence number to see if it's a retransmission. Let's say the sender sends data (with a sequence number) and the data wasn't corrupted. The receiver will send an ACK to the sender, but let's say the ACK gets corrupted. So the sender will resend the data with the same sequence number as before. Seeing that the sequence number of this retransmission is the same as the sequence number of the last packet it received, the receiver will ignore the retransmitted packet.</p>
      <div class="ln-box">
        <p>Since our protocol is a stop-and-wait protocol, alternating between `1`s and `0`s is enough for the sequence number. This is because data gets sent one at a time (because of stop and wait), so as long as the sequence number is different, the receiver will know that it's not a retransmission.</p>
      </div>
      <p>On the sender's side:</p>
      <ol>
        <li>take in the data from the upper layer by calling <code>rdt_send(data)</code></li>
        <li>create a packet by calling <code>make_pkt(0, data)</code>
          <ul>
            <li>`0` is the sequence number</li>
          </ul>
        </li>
        <li>send the `0` packet into the underlying channel by calling <code>udt_send(sndpkt)</code></li>
        <li>wait for and get an acknowledgment by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the acknowledgment is negative â€” or corrupted â€” send the `0` packet again by calling <code>udt_send(sndpkt)</code></li>
        <li>otherwise, if the acknowledgment is positive, get ready to get more data from the upper layer
          <ul>
            <li>but use sequence `1` for the next packet
              <ul>
                <li>after sending a packet with sequence number `1`, switch back to `0`</li>
              </ul>
            </li>
          </ul>
        </li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt2_1_sender.png">
        <p>This is the FSM for the <code>rdt2.1</code> sender.</p>
      </div>
      <p>On the receiver's side:</p>
      <ol>
        <li>take in the packet from the underlying channel by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the sequence number is not `0`, send an ACK, acknowledging the last packet
          <ul>
            <li>check if the packet has sequence number `0` by calling <code>has_seq0(rcvpkt)</code></li>
            <li>check if the packet has sequence number `1` by calling <code>has_seq1(rcvpkt)</code></li>
          </ul>
        </li>
        <li>if the `0` packet is corrupted:
          <ol>
            <li>make a negative acknowledgment by calling <code>make_pkt(NAK)</code></li>
            <li>send the negative acknowledgment by calling <code>udt_send(sndpkt)</code></li>
          </ol>
        </li>
        <li>otherwise, if the `0` packet is good:
          <ol>
            <li>extract the data from the packet by calling <code>extract(rcvpkt)</code></li>
            <li>send the data to the receiver by calling <code>deliver_data(data)</code></li>
            <li>make a positive acknowledgment by calling <code>make_pkt(ACK)</code></li>
            <li>send the positive acknowledgment by calling <code>udt_send(sndpkt)</code></li>
          </ol>
        </li>
        <li>now switch to waiting for a `1` packet
          <ul>
            <li>after getting a `1` packet, switch back to `0`</li>
          </ul>
        </li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt2_1_receiver.png">
        <p>This is the FSM for the <code>rdt2.1</code> receiver.</p>
      </div>
      <h4><code>rdt2.2</code>: ACK! ACK! Who's There? Oh, It's You Again</h4>
      <p>We can get rid of NAKs altogether, so we only have one type of acknowledgment to deal with (I guess having two types is too complicated?). Suppose the sender sends a `0` packet and the receiver gets it just fine, so it sends an ACK back to the sender. Then the sender sends a `1` packet, but that gets corrupted. Instead of sending a NAK for the `1` packet, the receiver can send an ACK for the previous `0` packet. The sender now has received two back-to-back ACK packets (<b>duplicate ACKs</b>) for the `0` packet, so the sender knows that the receiver did not get the `1` packet correctly. In order for this to work, we now have to add the sequence number to the ACK packets as well.</p>
      <div class="ln-box">
        <p>This is like one person asking, "Did you get the message I sent today?" and the other person responding, "The last message I got from you was from yesterday."</p>
      </div>
      <p>On the sender's side:</p>
      <ol>
        <li>take in the data from the upper layer by calling <code>rdt_send(data)</code></li>
        <li>create a packet by calling <code>make_pkt(0, data)</code>
          <ul>
            <li>`0` is the sequence number</li>
          </ul>
        </li>
        <li>send the `0` packet into the underlying channel by calling <code>udt_send(sndpkt)</code></li>
        <li>wait for and get an acknowledgment by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the acknowledgment is corrupted â€” or for a `1` packet â€” send the `0` packet again by calling <code>udt_send(sndpkt)</code></li>
        <li>otherwise, if the acknowledgment is good and for the `0` packet, get ready to get more data from the upper layer
          <ul>
            <li>but use sequence `1` for the next packet
              <ul>
                <li>after sending a packet with sequence number `1`, switch back to `0`</li>
              </ul>
            </li>
          </ul>
        </li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt2_2_sender.png">
        <p>This is the FSM for the <code>rdt2.2</code> sender.</p>
      </div>
      <p>On the receiver's side:</p>
      <ol>
        <li>take in the packet from the underlying channel by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the sequence number is not `0` or the packet is corrupted, send an ACK, acknowledging the last packet
          <ul>
            <li>check if the packet has sequence number `0` by calling <code>has_seq0(rcvpkt)</code></li>
            <li>check if the packet has sequence number `1` by calling <code>has_seq1(rcvpkt)</code></li>
          </ul>
        </li>
        <li>otherwise, if the packet is good and the sequence number is `0`:
          <ol>
            <li>extract the data from the packet by calling <code>extract(rcvpkt)</code></li>
            <li>send the data to the receiver by calling <code>deliver_data(data)</code></li>
            <li>make a positive acknowledgment for the `0` packet by calling <code>make_pkt(0, ACK)</code></li>
            <li>send the positive acknowledgment by calling <code>udt_send(sndpkt)</code></li>
          </ol>
        </li>
        <li>now switch to waiting for a `1` packet
          <ul>
            <li>after getting a `1` packet, switch back to `0`</li>
          </ul>
        </li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt2_2_receiver.png">
        <p>This is the FSM for the <code>rdt2.2</code> receiver.</p>
      </div>
      <h4>Reliable Data Transfer over a Lossy Channel with Bit Errors: <code>rdt3.0</code></h4>
      <p>Now let's assume the underlying channel may also lose some packets. Which brings two new problems: how to detect packet loss and how to deal with it. Well, the whole checksum, sequence numbers, ACK packets, and retransmission process already handles how to deal with packet loss. So we just need a way to detect packet loss.</p>
      <p>One simple approach is for the sender to resend the packet if no response is ever received. Of course, the sender has to wait for at least some time before assuming the packet (either the packet it sent or the ACK from the receiver) is lost. But how long should the sender wait? At a minimum, it should wait at least as long as the round-trip delay between sender and receiver. However, this is hard to quantify because it includes many variables, including queuing delays and processing delays. At a maximum, packet loss should be handled as soon as possible; it's not good to wait for a long time.</p>
      <p>There's not really a perfect solution that can take into account all those variables. The "good enough" solution is for the sender to pick the time it thinks is good enough. One problem (that's not really a problem) is that if the sender doesn't wait long enough, it may end up sending <b>duplicate data packets</b>. But as we saw earlier, the use of sequence numbers allows the receiver to detect duplicates.</p>
      <p>In order for this time-based retransmission to work, there needs to be a <b>countdown timer</b>. When the sender sends a packet, it starts the timer for that packet. If the timer goes off before a response is received, then the sender knows to resend the packet (and restart the timer). If a response is received, then the sender will turn off the timer for that packet.</p>
      <p>On the sender's side:</p>
      <ol>
        <li>take in the data from the upper layer by calling <code>rdt_send(data)</code></li>
        <li>create a packet by calling <code>make_pkt(0, data)</code>
          <ul>
            <li>`0` is the sequence number</li>
          </ul>
        </li>
        <li>send the `0` packet into the underlying channel by calling <code>udt_send(sndpkt)</code>
          <ul>
            <li>start the timer for the `0` packet by calling <code>start_timer</code></li>
          </ul>
        </li>
        <li>wait for and get an acknowledgment by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the acknowledgment is corrupted â€” or for a `1` packet â€” send the `0` packet again by calling <code>udt_send(sndpkt)</code></li>
        <li>or if the timer goes off before receiving a response from the receiver, send the `0` packet again</li>
        <li>otherwise, if the acknowledgment is good and for the `0` packet, stop the timer for the `0` packet and get ready to get more data from the upper layer
          <ul>
            <li>but use sequence `1` for the next packet
              <ul>
                <li>after sending a packet with sequence number `1`, switch back to `0`</li>
              </ul>
            </li>
          </ul>
        </li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt3_sender.png">
        <p>This is the FSM for the <code>rdt3.0</code> sender.</p>
      </div>
      <div class="ln-box">
        <p>Notice there's a new self-loop while the sender waits for a call from above. This self-loop is for premature timeouts. Let's say the sender sends a `0` packet and it takes an abnormally long time to reach the receiver. The timer goes off, making the sender think the `0` packet got lost, so the sender sends the `0` packet again. Right after it sends the second `0` packet, the first `0` packet reaches the receiver, so the receiver sends an ACK back to the sender. The sender receives the ACK and goes to waiting for a call from above. While the sender waits for a call from above, the second `0` packet reaches the receiver, so the receiver sends an ACK back to the sender. This is where the self-loop comes in.</p>
      </div>
      <p>On the receiver's side:</p>
      <ol>
        <li>take in the packet from the underlying channel by calling <code>rdt_rcv(rcvpkt)</code></li>
        <li>if the sequence number is not `0` or the packet is corrupted, send an ACK, acknowledging the last packet
          <ul>
            <li>check if the packet has sequence number `0` by calling <code>has_seq0(rcvpkt)</code></li>
            <li>check if the packet has sequence number `1` by calling <code>has_seq1(rcvpkt)</code></li>
          </ul>
        </li>
        <li>otherwise, if the packet is good and the sequence number is `0`:
          <ol>
            <li>extract the data from the packet by calling <code>extract(rcvpkt)</code></li>
            <li>send the data to the receiver by calling <code>deliver_data(data)</code></li>
            <li>make a positive acknowledgment for the `0` packet by calling <code>make_pkt(0, ACK)</code></li>
            <li>send the positive acknowledgment by calling <code>udt_send(sndpkt)</code></li>
          </ol>
        </li>
        <li>now switch to waiting for a `1` packet
          <ul>
            <li>after getting a `1` packet, switch back to `0`</li>
          </ul>
        </li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/rdt2_2_receiver.png">
        <p>This is the FSM for the <s><code>rdt2.2</code></s> <code>rdt3.0</code> receiver.</p>
      </div>
      <p>Yep, that's right. The receiver for <code>rdt3.0</code> behaves in exactly the same way as the receiver for <code>rdt2.2</code>. This is because adding a timer just adds more retransmissions. In other words, the receiver will do the same thing (send an ACK) regardless of whether the retransmission was from a timeout or from an error.</p>
      <div class="ln-box">
        <p>Here's how the different scenarios are handled.</p>
        <p>No loss:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/rdt3_1.png">
        </div>
        <p>Lost Packet:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/rdt3_2.png">
        </div>
        <p>Lost ACK:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/rdt3_3.png">
        </div>
        <p>Premature timeout:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/rdt3_4.png">
        </div>
      </div>
      <h3>Pipelined Reliable Data Transfer Protocol</h3>
      <p>While <code>rdt3.0</code> is a correct protocol, it's not great because of its stop-and-wait nature.</p>
      <p>Let's look at the math for sending a packet from sender to receiver. Suppose the RTT is `30` milliseconds; the channel between them has a transmission rate of `1` Gbps (`10^9` bits per second); and the packet size is `1000` bytes (`8000` bits).</p>
      <p>The transmission delay is</p>
      <div class="ln-center">
        <p>`d_text(trans)=L/R=(8000text( bits))/(10^9text( bits per second))=0.000008` seconds `=0.008` milliseconds</p>
      </div>
      <p>Once the packet is transmitted, it will take `15` milliseconds to reach the receiver. The receiver now sends the ACK to the sender. For simplicity, we'll assume there is no time to transmit the ACK. It will take `15` milliseconds for the sender to receive the ACK. So this whole process took</p>
      <div class="ln-center">
        <p>`text(RTT)+L/R=30+0.008=30.008` milliseconds</p>
      </div>
      <p>The sender was busy for only `0.008` milliseconds, but the whole process took `30.008` milliseconds. The <b>utilization</b> of the sender (the measure of how busy it was) is</p>
      <div class="ln-center">
        <p>`U_text(sender)=(L/R)/(text(RTT)+L/R)=0.008/30.008~~0.00027`</p>
      </div>
      <p>A low utilization means that it wasn't busy for most of the time, i.e., the sender is being inefficient. In fact, the sender was busy only `0.027%` of the time.</p>
      <p>From another point of view, the sender was only able to send `1000` bytes in `30.008` milliseconds. This means the throughput is</p>
      <div class="ln-center">
        <p>`(8000text( bits))/(30.008text( milliseconds))=(8000text( bits))/(0.030008text( seconds))~~267,000` bits per second `=267` kbps.</p>
      </div>
      <p>This is inefficient, considering the fact that the channel between them has a transmission rate of `1` Gbps. (Imagine paying for `1` Gpbs but only getting `267` kbps.)</p>
      <p>On top of it all, we haven't even considered processing and queuing delays at intermediate routers in the channel.</p>
      <p>It's important to note that all this inefficiency is due to the fact that the sender has to wait for the receiver's response before it can send more data. Naturally, the solution is to let the sender send multiple packets at a time without having to wait for a response. This is called <b>pipelining</b> (because many packets are filling a pipeline). There will be several changes that need to be made for this to work:</p>
      <ul>
        <li>The range of sequence numbers has to be increased. We can't alternate between `0`s and `1`s anymore.</li>
        <li>The sender and receiver need a buffer (queue) to handle multiple packets at a time.</li>
      </ul>
      <h3>Go-Back-N (GBN)</h3>
      <p>Let's take a look at the sender's buffer at a random point in time.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/gbn.png">
      </div>
      <p>The rectangles are spots in the buffer, where each spot is identified by a sequence number. <span class="ln-green">Green means the packet in this spot has already been sent and ACK'ed</span>. <span class="ln-orange">Orange means the packet in this spot has been sent, but no ACK has been received for it yet</span>. <span class="ln-light-blue">Blue means the spot is empty, but once there's a packet in that spot, it will be sent right away</span>. White means the spot is empty and once there's a packet in that spot, it won't be sent right away. (Did you expect me to write that last sentence in white?)</p>
      <p>In the <b>Go-Back-N (GBN) protocol</b>, the sender can send multiple packets as long as the number of unacknowledged packets is less than some number `N`. When there are exactly `N` unacknowledged packets in the sender's buffer, the sender is not allowed to send any more data until some of those `N` unACK'ed packets are acknowledged.</p>
      <p>To formalize things a bit more, we define <code>base</code> to be the sequence number of the oldest unacknowledged packet and <code>nextseqnum</code> to be the sequence number of the next packet to fill that spot.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/gbn_2.png">
      </div>
      <p>Here's what the buffer looks like as packets get sent, acknowledged, and received:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/gbn.gif">
      </div>
      <p>This sliding animation is why `N` is referred to as the <b>window size</b> and why GBN is a <b>sliding-window protocol</b>.</p>
      <div class="ln-box">
        <p>The sequence number of a packet is stored in the packet's headers, which has limited storage (UDP segments have `8`-byte headers for example). This implies that there's a limit to how high a sequence number can be. Generally, if we devote `k` bits to storing the sequence number, the highest sequence number that can be assigned to a packet is `2^k-1`. Once a packet is assigned the sequence number `2^k-1`, the next packet will be assigned a sequence number of `0`.</p>
        <p>For <code>rdt3.0</code>, `k=1`.</p>
      </div>
      <p>When the upper layer has data to send, the sender first checks to see if its window is full, which is the case if there are `N` unACK'ed packets. If it's not full, a packet is created and sent. If it is full, then the packet gets created, but put in the buffer outside the window. (And if the buffer is full, then synchronization methods, like semaphores, are used to control when the upper layer can send data.)</p>
      <p>The timer starts when the <code>base</code> packet gets sent. If an ACK is received for the <code>base</code> packet and there are no unACK'ed packets in the window, then the timer is stopped. If an ACK is received for the <code>base</code> packet and there are still unACK'ed packets in the window, then the timer is restarted. In either case, the next packet to be unACK'ed becomes the new <code>base</code> packet.</p>
      <p>Now is where we find out why this protocol is called Go-Back-N. If the timer goes off (meaning that the sender thinks a packet got lost), then the sender will resend all packets that are currently unACK'ed. Since there can be at most `N` unACK'ed packets, the sender will go back and send at most `N` packets.</p>
      <div class="ln-box">
        <p>Because the sender sends packets in order, the receiver also expects to get packets in order. It's possible for packets to be out of order if packets get lost.</p>
      </div>
      <p>When the receiver gets a packet with sequence number `n`, the receiver checks if `n` is the expected sequence number, i.e., the receiver checks if the last packet delivered to the upper layer was `n-1`. If it is, then the receiver sends an ACK for packet `n`. If the expected sequence number is not `n`, or the packet got corrupted, then the receiver discards packet `n` and sends an ACK for the last packet it received that was in order.</p>
      <div class="ln-box">
        <p>Since data is delivered to the upper layer one at a time, if packet `k` is received and delivered, then sending an ACK for packet `k` means that packet `k` â€” and all the packets before `k` â€” have been received correctly and delivered. This is called <b>cumulative acknowledgment</b>.</p>
      </div>
      <div class="ln-box">
        <p>There is no need for the receiver to have a buffer. Theoretically, if the receiver is waiting for packet `n`, but it gets `n+1` instead, the receiver could store it in a buffer until packet `n` arrives. But if packet `n` never arrives, both packets `n` and `n+1` will be retransmitted by the sender anyway.</p>
        <p>The good thing about not having a buffer to manage is that it reduces complexity. The bad thing about this is that we're throwing away a perfectly good packet. In subsequent retransmissions, that same packet may get lost or corrupted, resulting in more retransmissions.</p>
      </div>
      <div class="ln-box">
        <p>Suppose the sender's window size is `4`. The sender starts by sending packets `0`, `1`, `2`, and `3`, then waiting for their ACKs. The receiver gets packets `0` and `1`, but let's say packet `2` gets lost. When the sender gets the ACKS for `0` and `1`, the sender sends packets `4` and `5`. The receiver gets packets `3`, `4`, and `5`, but discards them since packet `2` hasn't come yet. (The receiver also sends ACKs for packet `1` since that was the last correctly received packet, but the sender will ignore these since the sender already received an ACK for `1`.)</p>
        <p>The timer eventually goes off, so the sender sends packets `2`, `3`, `4`, and `5` again.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/gbn_3.png">
        </div>
      </div>
      <div class="ln-box">
        <p>These FSMs are called extended FSMs because there are now variables, like <code>base</code> and <code>nextseqnum</code>, and other programming language constructs.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/gbn_sender.png">
          <p>This is the FSM for the GBN sender.</p>
        </div>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/gbn_receiver.png">
          <p>This is the FSM for the GBN receiver.</p>
        </div>
        <p>In the receiver, <code>expectedseqnum</code> is only incremented when a valid packet is received. So when a non-valid packet is received, the receiver sends an ACK for the last correctly received packet, which will have sequence number <code>expectedseqnum</code>.</p>
      </div>
      <h3>Selective Repeat (SR)</h3>
      <p>Even though GBN allows the sender to send multiple packets at a time, there are still some situations where GBN is inefficient. If something goes wrong with just one packet, GBN has to go back `N` and retransmit all those packets again, which is problematic especially if `N` is large.</p>
      <p>Based on its name, SR will select only the packets that (it thinks) were lost and retransmit just those packets.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/sr_sender.png">
      </div>
      <p>The sender's buffer in SR is similar to the sender's buffer in GBN. But in SR, packets can be ACK'ed out of order.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/sr.gif">
      </div>
      <p>In order for this to work, the receiver must also have its own buffer.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/sr_receiver.png">
        <p>In this picture, the buffer is not meant to match with the sender's buffer.</p>
      </div>
      <p><span class="ln-green">Green means the packet in this spot has already been delivered to the upper layer</span>. <span class="ln-red">Red means the packet in this spot has not been received yet</span>. <span class="ln-purple">Purple means the packet in this spot has been received and ACK'ed, but not yet delivered to the upper layer</span>. <span class="ln-light-blue">Blue means the spot is empty, but once there's a packet in that spot, it will be ACK'ed</span>. <span class="ln-white">White means the spot is empty and once there's a packet in that spot, it won't be ACK'ed right away. (Hah! I wrote it in white this time!)</span></p>
      <p>In the example above, let's say the sequence numbers of the first two purple packets are `6` and `7`. If packet `5` arrives, then packets `5`, `6`, and `7` will all be delivered to the upper layer at once.</p>
      <p>When the upper layer has data to send, the sender first checks to see if its window is full, which is the case if there are `N` packets and at least `1` of them is unACK'ed. If it's not full, a packet is created and sent. If it is full, then the packet gets created, but put in the buffer outside the window. (And if the buffer is full, then synchronization methods, like semaphores, are used to control when the upper layer can send data.)
      <p>Since packets are selectively repeated, each packet needs its own timer. (Unlike in GBN where one timer was used for `N` packets.)</p>
      <p>If the receiver gets a packet that has already been delivered to the upper layer (sequence number `lt` <code>rcv_base</code>), then the receiver sends an ACK for it and discards it. If the receiver gets a packet outside of its window (sequence number `gt` <code>rcv_base</code>`+N` ), then the receiver puts it in the buffer outside the window. Finally, if the receiver gets a packet with a sequence number inside the window, the receiver sends an ACK for it. And if the sequence number is equal to <code>rcv_base</code>, then it and all consecutive packets that have been ACK'ed will be delivered to the upper layer.</p>
      <div class="ln-box">
        <p>It is necessary for the receiver to send ACKs for packets it has already delivered to the upper layer. Suppose the sender has not received an ACK for the <code>base</code> packet yet, but the receiver has already delivered it to the upper layer. Let's say the ACK from the receiver got lost, so the sender's timer eventually goes off for the <code>base</code> packet, causing the sender to send it again. If the receiver doesn't send an ACK for the <code>base</code> packet it already delivered, then the sender will be stuck forever waiting for the ACK.</p>
      </div>
      <div class="ln-box">
        <p>Suppose the window size is `4`. The underlined numbers are the sequence numbers in the window. The sender starts by sending packets `0`, `1`, `2`, and `3`, then waiting for their ACKs. The receiver gets packets `0` and `1`, but let's say packet `2` gets lost. When the sender gets the ACKS for `0` and `1`, the sender sends packets `4` and `5`. The receiver gets packets `3`, `4`, and `5`, and buffers them since packet `2` hasn't come yet. The timer eventually goes off for packet `2`, so the sender sends packet `2` again. The receiver gets packet `2` and delivers it along with buffered packets `3`, `4`, and `5` to the upper layer.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/sr.png">
        </div>
      </div>
      <div class="ln-box">
        <p>The sender's window does not have to be in sync with the receiver's window. This isn't usually a problem if they sync up eventually. However, it is possible that being out of sync at the wrong time can cause issues, particularly if the size of the window is fairly close to the number of assignable sequence numbers.</p>
        <p>For example, suppose the window size is `3` and the number of assignable sequence numbers is `4` (so the assignable sequence numbers are `0`, `1`, `2`, `3`).</p>
        <p>The sender starts by sending packets `0`, `1`, and `2`. The receiver gets all of them successfully and the sender gets all their ACKs successfully too. So the sender sends packets `3`, `0`, and `1`. The receiver gets packet `0` correctly, but packet `3` gets lost, so packet `0` on the receiver's side gets buffered.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/sr_dilemma.png">
        </div>
        <p>Everything seems normal for now, but let's look at another scenario. Suppose that the ACKs for packets `0`, `1`, and `2` got lost instead. The timer goes off for packet `0`, so the sender resends packet `0`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/sr_dilemma_2.png">
        </div>
        <p>But wait, look at the windows for the sender and receiver! When the sender was sending packet `0` the second time, packet `0` was actually a retransmission. However, the receiver thought it was a new packet `0`.</p>
        <p>Furthermore, the receiver thinks everything is fine; and why wouldn't it? From the receiver's point of view, the sequence of events in this situation is exactly the same as the normal one.</p>
        <p>In order to avoid this problem of confusing one packet for another, the window size and number of assignable sequence numbers has to be different enough. But how much is enough? (Obviously, different by `1` isn't enough!)</p>
        <p>Notice that when the receiver moved by a whole window's length, some of the sequence numbers in the previous window showed up again in the next window.</p>
        <div class="ln-center">
          <img class="img-fluid" src="../pictures/networking/sr_dilemma.gif">
        </div>
        <p>This is what caused the trouble, because it's possible to get these sequence numbers mixed up with the ones from the previous window (where the other side is stuck on the previous window). Naturally, this means that we should be able to move by a whole window and not see any of the previous sequence numbers. Well, that happens when the number of assignable sequence numbers is (at least) twice the window size.</p>
        <div class="ln-center">
          <img class="img-fluid" src="../pictures/networking/sr_dilemma_2.gif">
        </div>
      </div>
      <p>Here's a table summarizing everything we needed to build a reliable data transfer protocol:</p>
      <table class="table">
        <tr>
          <th>Mechanism</th>
          <th>Description</th>
        </tr>
        <tr>
          <td>Checksum</td>
          <td>Used to detect corrupted packets</td>
        </tr>
        <tr>
          <td>Acknowledgment</td>
          <td>Used to tell the sender that the packet was received correctly</td>
        </tr>
        <tr>
          <td>Negative acknowledgment</td>
          <td>Used to tell the sender that the packet was not received correctly</td>
        </tr>
        <tr>
          <td>Sequence number</td>
          <td>Used to give packets an ordering so the receiver can distinguish duplicate packets and detect lost packets</td>
        </tr>
        <tr>
          <td>Timer</td>
          <td>Used to retransmit lost packets</td>
        </tr>
        <tr>
          <td>Window, pipelining</td>
          <td>Used to allow the sender to send multiple packets at a time, but not too many at once</td>
        </tr>
      </table>
      <h2 id="tcp">Connection-Oriented Transport: TCP</h2>
      <p>A lot of the stuff we just saw will show up here because, well, TCP is a reliable data transfer protocol.</p>
      <h3>The TCP Connection</h3>
      <p>As we've seen several times by this point, TCP is <b>connection-oriented</b> because the two processes communicating with each other have to establish a connection with each other first before any data is sent. What's interesting though (at least to me) is that the lower layers are not aware of "connections"; all they see are packets of data. After all, the handshaking part of TCP (i.e., establishing a connection) is nothing more than packets of data.</p>
      <p>TCP provides a <b>full-duplex service</b>, which means that data can go back and forth between Process A and Process B (i.e., data flow is bidirectional)*. A TCP connection is also <b>point-to-point</b>, which means there is only one sender and one receiver at a time. We can't establish a TCP connection between Process A and Process B and have Process C send/receive data to/from them.</p>
      <div class="ln-box">
        <p>*Even though the "Principles of Reliable Data Transfer" section only looked at transferring data from sender to receiver (i.e., <b>unidirectional data transfer</b>), those principles also apply to bidirectional data transfer. In the bidirectional case, there is no longer just one sender and one receiver. There are two hosts that can be sender or receiver depending on who's sending data and who's receiving data at that moment.</p>
      </div>
      <p>Establishing a connection starts with a <b>three-way handshake</b> (animated in "HTTP with Non-Persistent Connections"). After that, the client process sends data through the socket to the transport layer, where TCP puts the data into a <b>send buffer</b>. Then, when it can, TCP gets the data from the send buffer, encapsulates it with headers, and passes it on to the network layer. Eventually, the data reaches the receiver's transport layer, where TCP puts the data into a receive buffer. Finally, the receiver's application process reads from this buffer to get the data.</p>
      <p>There is a <b>maximum segment size (MSS)</b> that limits how big a segment can be. This is determined by the <b>maximum transmission unit (MTU)</b>, which is the length of the largest link-layer frame that the local sending host is capable of sending. Since a TCP/IP header (a TCP segment encapsulated in an IP datagram) is typically `40` bytes, the MSS is calculated as `text(MSS) = text(MTU) - 40`. From this equation, we can see that MSS is the maximum amount of application-layer data that can be in a segment, not the size of the whole segment. A bit of a misnomer.</p>
      <h3>TCP Segment Structure</h3>
      <p>The data portion of the TCP segment contains the application data. If the application data is larger than the MSS, then it is broken up into chunks of size `le` MSS.</p>
      <p>The following are included in the TCP segment:</p>
      <ul>
        <li><b>source</b> (2 bytes) <b>and destination port</b> (2 bytes) <b>numbers</b></li>
        <li><b>checksum</b> (2 bytes)</li>
        <li><b>sequence number</b> (4 bytes)</li>
        <li><b>acknowledgment number</b> (4 bytes)</li>
        <li><b>receive window</b> (2 bytes)
          <ul>
            <li>the number of bytes a receiver is willing to accept</li>
          </ul>
        </li>
        <li><b>header length</b> (0.5 bytes)</li>
        <li><b>flags</b> (1 byte)
          <ul>
            <li><b>ACK</b></li>
            <li><b>RST</b></li>
            <li><b>SYN</b></li>
            <li><b>FIN</b></li>
            <li><b>CWR</b></li>
            <li><b>ECE</b></li>
            <li><b>PSH</b>
              <ul>
                <li>if the flag is on, then the receiver should pass the data to the upper layer right away</li>
              </ul>
            </li>
            <li><b>URG</b>
              <ul>
                <li>if the flag is on, then (parts of) this data has been marked "urgent" by the sending side</li>
              </ul>
            </li>
          </ul>
        </li>
        <li><b>urgent data pointer</b> (2 bytes)
          <ul>
            <li>the location of the last byte of urgent data</li>
          </ul>
        </li>
      </ul>
      <div class="ln-center">
        <p><pre>+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|          Source Port          |       Destination Port        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                        Sequence Number                        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Acknowledgment Number                      |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
| Header |        |C|E|U|A|P|R|S|F|                             |
| Length | Unused |W|C|R|C|S|S|Y|I|       Receive Window        |
|        |        |R|E|G|K|H|T|N|N|                             |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|           Checksum            |      Urgent Data Pointer      |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                            Options                            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                                                               :
:                             Data                              :
:                                                               |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</pre></p>
        <p>(copied from RFC 9293)</p>
      </div>
      <h4>Sequence Numbers and Acknowledgment Numbers</h4>
      <p>So far, we've only seen sequence numbers increasing sequentially by `1`. However, TCP works in bytes, not packets.</p>
      <p>Suppose a file is being transferred over TCP. Let's say the file size is `500,000` bytes and the MSS is `1000` bytes. This means there will be `(500,000)/1000=500` segments, where each segment is divided into `1000` bytes (numbered starting from `0`*).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/tcp_sequence_number.png">
        <p>(each rectangle should be the same size but I ran out of room)</p>
      </div>
      <p>The sequence number of the first segment is `0`. The sequence number of the second segment is `1000`. The sequence number of the `500^(th)` segment is `499,000`. Generally, the <b>sequence number for a segment</b> is the byte-stream number of the first byte in that segment.</p>
      <div class="ln-box">
        <p>*For simplicity, we assumed that the initial sequence number starts at `0`. In reality, the initial sequence number is random. This is to prevent the possibility that a segment that is somehow still in the network from a previous connection gets mistaken for a valid segment in a new connection.</p>
      </div>
      <p>The acknowledgment number is the sequence number that the sender is waiting to receive. If the acknowledgment number is `x`, then that means, "I need you to give me a segment with sequence number `x`."</p>
      <p>Suppose that Host A has only received bytes `0` through `535` from Host B. On every segment that Host A sends to Host B, Host A will put `536` as the acknowledgment number until Host A gets byte `536` from Host B.</p>
      <p>Suppose that Host A has received bytes `0` through `535` and bytes `900` through `1000`. Host A will also put `536` as the acknowledgment number in this case. Notice that Host A received the segments out of order. The TCP RFCs do not specify what to do with out-of-order segments; the decision is left to the people programming the TCP. They can decide to either drop the out-of-order segments or put them in a buffer, which is the preferred method.</p>
      <h4>Telnet: Sequence and Acknowledgment Numbers in Action</h4>
      <p>Telnet is an application-layer protocol that allows us to remotely log in to another computer. (It's the unencrypted version of SSH.)</p>
      <p>Suppose Host A initiates a Telnet session with Host B (so the person on Host A is remotely logging into Host B). Whenever the user on Host A types something, the input will be sent to Host B, which will send back a copy of the input to Host A. This "echo back" is so the user knows that the remote host received the input. This means that each character travels through the network twice (Host A `rarr` Host B and Host B `rarr` Host A). Let's look at what happens if the user on Host A types a single letter 'C'. (Note that sequence numbers will be increasing by `1` in this case because it only takes `1` byte to store a character.)</p>
      <p>Let's say the starting sequence numbers are `42` and `79` for Host A and Host B respectively. When the letter 'C' is typed, Host A sends a segment with sequence number `42` and acknowledgment number `79`. Upon receiving the segment, Host B has to do two things: send an ACK and echo the data back. Both of these can be done in one step. The ACK can be included in the segment by putting `43` as the acknowledgment number. By doing this (a.k.a <b>piggybacking</b> the acknowledgment), Host B is saying that it is now waiting for byte `43` (implying that it has received `42`). And, of course, echoing the data back is done by putting the data in the segment. Once Host A gets the segment, it sends an ACK back to Host B by sending a segment with acknowledgment number `80`. The sequence number is `43` since Host A last sent `42`, and there is no data since this segment is solely for acknowledgment. So the ACK is not being piggybacked here.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/telnet.png">
      </div>
      <h3>Round-Trip Time Estimation and Timeout</h3>
      <p>Somewhere in the <code>rdt3.0</code> section, the question of how long the timeout should be was briefly discussed. In fact, it was mentioned that the timer should be at least as long as the round-trip time between sender and receiver. Now we attempt to quantify that RTT.</p>
      <h4>Estimating the Round-Trip Time</h4>
      <p>The round-trip time is the time from when the segment is sent to when the acknowledgment for that segment is received. Let's denote this time as <code>SampleRTT</code>. So every time a segment is sent and acknowledged, the value of <code>SampleRTT</code> changes to however long that segment took.</p>
      <div class="ln-box">
        <p>A <code>SampleRTT</code> is never calculated for a retransmitted segment. This is because if an acknowledgment for that segment came back, it would be hard to tell if the acknowledgment was for the original segment or the retransmitted segment.</p>
      </div>
      <p>The problem is that the RTT can vary wildly from segment to segment, so it's better to keep track of what the average RTT has been. Let's call this average <code>EstimatedRTT</code>. So every time a segment is sent and acknowledged, the average will be updated by:</p>
      <div class="ln-center">
        <p>`text(EstimatedRTT)=(1-alpha)*text(EstimatedRTT)+alpha*text(SampleRTT)`</p>
      </div>
      <div class="ln-box">
        <p>The recommended value for `alpha` is `0.125`.</p>
      </div>
      <div class="ln-box">
        <p>(Disclaimer: I hope my math is not wrong!) Suppose we only have one <code>SampleRTT</code> so far; let's call it <code>SampleRTT<sub>1</sub></code>. So the <code>EstimatedRTT</code> is just <code>SampleRTT<sub>1</sub></code> at this point*:</p>
        <div class="ln-center">
          <p>`text(EstimatedRTT)=text(SampleRTT)_1`</p>
        </div>
        <p>Now along comes the second <code>SampleRTT</code>.</p>
        <div class="ln-center">
          <p>`text(EstimatedRTT)=(1-alpha)*text(EstimatedRTT)+alpha*text(SampleRTT)_2`</p>
          <p>`=(1-alpha)*text(SampleRTT)_1+alpha*text(SampleRTT)_2`</p>
        </div>
        <p>And the third <code>SampleRTT</code>.</p>
        <div class="ln-center">
          <p>`text(EstimatedRTT)=(1-alpha)*text(EstimatedRTT)+alpha*text(SampleRTT)_3`</p>
          <p>`=(1-alpha)*((1-alpha)*text(SampleRTT)_1+alpha*text(SampleRTT)_2)+alpha*text(SampleRTT)_3`</p>
          <p>`=(1-alpha)^2*text(SampleRTT)_1+(1-alpha)alpha*text(SampleRTT)_2+alpha*text(SampleRTT)_3`</p>
        </div>
        <p>And finally the fourth <code>SampleRTT</code>.</p>
        <div class="ln-center">
          <p>`text(EstimatedRTT)=(1-alpha)*text(EstimatedRTT)+alpha*text(SampleRTT)_4`</p>
          <p>`=(1-alpha)*((1-alpha)^2*text(SampleRTT)_1+(1-alpha)alpha*text(SampleRTT)_2+alpha*text(SampleRTT)_3)+alpha*text(SampleRTT)_4`</p>
          <p>`=(1-alpha)^3*text(SampleRTT)_1+(1-alpha)^2alpha*text(SampleRTT)_2+(1-alpha)alpha*text(SampleRTT)_3)+alpha*text(SampleRTT)_4`</p>
        </div>
        <p>Generally, after `n` <code>SampleRTT</code>s, the equation looks like:</p>
        <div class="ln-center">
          <p>`text(EstimatedRTT)=(1-alpha)^(n-1)*text(SampleRTT)_1+(1-alpha)^(n-2)alpha*text(SampleRTT)_2+...+(1-alpha)^(n-n)alpha*text(SampleRTT)_n`</p>
          <p>`=(1-alpha)^(n-1)*text(SampleRTT)_1+(1-alpha)^(n-2)alpha*text(SampleRTT)_2+...+(1-alpha)^(0)alpha*text(SampleRTT)_n`</p>
          <p>`=(1-alpha)^(n-1)*text(SampleRTT)_1+(1-alpha)^(n-2)alpha*text(SampleRTT)_2+...+alpha*text(SampleRTT)_n`</p>
        </div>
        <p>If we let `n rarr oo`, the <code>EstimatedRTT</code> will look like:</p>
        <div class="ln-center">
          <p>`text(EstimatedRTT)=(1-alpha)^(oo-1)*text(SampleRTT)_1+(1-alpha)^(oo-2)alpha*text(SampleRTT)_2+...+alpha*text(SampleRTT)_(n)`</p>
        </div>
        <p>The <code>EstimatedRTT</code> will just converge to `alpha*text(SampleRTT)_n`, which means as we calculate more RTTs, it all eventually just depends on the most recent RTT. But that's boring. Let's look at another interpretation.</p>
        <p>Notice that `(oo-1)` is "bigger" than `(oo-2)`. With `(1-alpha)lt1`, this means that the weight applied to <code>SampleRTT<sub>1</sub></code> is "smaller" than the weight applied to <code>SampleRTT<sub>2</code>. `(oo-2)` is "bigger" than `(oo-3)`, so the weight applied to <code>SampleRTT<sub>2</sub></code> is "smaller" than the weight applied to <code>SampleRTT<sub>3</code>. What we see here is that the older the RTT value is, the less effect it has in determining the value of <code>EstimatedRTT</code>. Which makes sense because newer RTTs are more reflective of the current state of the network. This is why the equation for calculating the <code>EstimatedRTT</code> is called an <b>exponential weighted moving average (EWMA)</b> (the weights are exponentially decreasing).</p>
        <p>*According to RFC 6298, the value of <code>EstimatedRTT</code> after one <code>SampleRTT</code> should be <code>SampleRTT</code>.</p>
      </div>
      <h4>Setting and Managing the Retransmission Timeout Interval</h4>
      <p>Okay, so the <code>EstimatedRTT</code> is the average time it takes for a segment to be sent and acknowledged. Being the average, there will definitely be segments that take longer than <code>EstimatedRTT</code>. So the timeout should be <code>EstimatedRTT</code> plus some extra length of time. To get a sense of how long some of the segments may take, it is useful to know how varied the RTTs are. Let's call this measure of deviation <code>DevRTT</code>.</p>
      <div class="ln-center">
        <p>`text(DevRTT)=(1-beta)*text(DevRTT)+beta*abs(text(SampleRTT)-text(EstimatedRTT))`</p>
      </div>
      <div class="ln-box">
        <p>If the <code>SampleRTT</code> is close to <code>EstimatedRTT</code>, then <code>DevRTT</code> will not change by much. But if <code>SampleRTT</code> is much larger/smaller than <code>EstimatedRTT</code>, then <code>DevRTT</code> will change by a lot.</p>
        <p>In English, if the RTT is close to the average, then most of the RTTs will be fairly close to each other (i.e., most of them will have roughly the same value). If the RTT is far off from the average, then the range of all possible RTT values will be pretty large.</p>
      </div>
      <div class="ln-box">
        <p>The recommended value for `beta` is `0.25`.</p>
      </div>
      <p>Now that we know how varied the RTTs can be, we can use that to provide a sort of "safety margin" for those segments that take longer than average.</p>
      <div class="ln-center">
        <p>`text(TimeoutInterval)=text(EstimatedRTT)+4*text(DevRTT)`</p>
      </div>
      <div class="ln-box">
        <p>The `4` could be because of statistics. To cover `99%` of normally distributed data, we have to add `3` standard deviations to the average (see the <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule" target="_blank">68â€“95â€“99.7 rule</a>). So adding `4` standard deviations accounts for `gt 99%`, which means `lt 1%` of segments are expected to timeout prematurely.</p>
      </div>
      <h3>Reliable Data Transfer</h3>
      <p>Now we'll look at how TCP behaves. When there is data from the upper layer, if there is room in the sender's window, the sender encapsulates the data in a segment and passes it to the lower layer. The sequence number is updated by adding to it the length of the data that was just sent.</p>
      <p>If the timer hasn't been started yet, then the sender starts the timer as well. This time (pun!), there is only one timer; and that timer is only for the oldest unacknowledged segment. If the timer goes off, then the sender retransmits the segment that timed out and restarts the timer. This differs from GBN, where `N` unacknowledged packets are retransmitted if the timer goes off.</p>
      <p>If an ACK is received, the sender looks at the acknowledgment number (let's call it `y`) and compares it with the sequence number of the oldest unacknowledged segment (let's call it <code>base</code>). If the acknowledgment number is greater than <code>base</code>, then that means the receiver successfully received all the segments from <code>base</code> to `y`. So all unacknowledged segments `le y` get marked as acknowledged, and <code>base</code> gets updated to the next oldest unacknowledged segment (which has to be greater than `y`). Also, something new that wasn't in any of the previous protocols is if an ACK is received for <em>any</em> unacknowledged segment, the timer gets restarted.</p>
      <h4>A Few Examples</h4>
      <p>In the first example, suppose Host A sends a segment with sequence number `92` and `8` bytes of data. Host B gets the segment and sends an ACK for it, but let's say the ACK gets lost. The timer will go off, so Host A will resend the segment with sequence number `92`. Having already received `92`, Host B will send an ACK for it and discard the segment.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/tcp_1.png">
      </div>
      <p>In the second example, suppose Host A sends two consecutive segments. The first segment has sequence number `92` and `8` bytes of data and the second segment has sequence number `100` and `20` bytes of data. Host B gets both of them and sends ACKs for them. The first ACK will have acknowledgment number `100` and the second ACK will have acknowledgment number `120`. But the timer goes off before the ACKs reach Host A. So Host A will retransmit segment `92` and restart the timer.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/tcp_2.png">
      </div>
      <p>In the third example, suppose that the first ACK gets lost but the second ACK makes it to Host A before the timer goes off. Since ACKs are cumulative, Host A knows that Host B received everything up to `120`, so Host A doesn't need to resend `92` or `100`.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/tcp_3.png">
      </div>
      <h4>Doubling the Timeout Interval</h4>
      <p>Whenever there is a timeout, the timeout period is doubled instead of being calculated from <code>EstimatedRTT</code> and <code>DevRTT</code>. So if the timeout interval starts at `0.75` seconds and the timer goes off, the sender retransmits the lost segment and restarts the timer, doubling the timeout interval to `1.5` seconds.</p>
      <p>This doubling is only for when timeouts happen. If the timer is started for a different reason (either there is data received from the upper layer or an ACK is received), then the timeout interval is calculated using <code>EstimatedRTT</code> and <code>DevRTT</code>.</p>
      <p>This doubling is a form of congestion control. If a segment timed out, it's probably because the network is congested, so doubling the timeout interval is kind of allowing the congestion to clear up before sending more data.</p>
      <h4>Fast Retransmit</h4>
      <p>But this doubling introduces a new problem. As more timeouts occur, the timeout interval gets longer and longer as it keeps getting doubled. This is a problem because if a segment gets lost, it will take forever for the segment to be retransmitted. Luckily, there's a way for the sender to know if a segment gets lost before the timer goes off.</p>
      <p>Suppose the sender sends segments with sequence numbers `92`, `100`, `120`, `135`, and `141`. Let's say the receiver gets all the segments except `100`. The receiver will send ACKs to the sender with acknowledgment number `100` for all the ACKs. Seeing a bunch of `100`s, the sender can assume that `100` got lost. So before the timer goes off, the sender retransmits segment `100`. This is called <b>fast retransmit</b>.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/fast_retransmit.png">
      </div>
      <div class="ln-box">
        <p>Fast transmit only occurs when there are `3` <b>duplicate ACKs</b> for a segment. Fast transmitting on only `1` duplicate ACK is not a good idea.</p>
        <p>Suppose the sender sends segments `92`, `100`, and `120`, and the receiver gets `90`, `120`, and `100` in that order. Upon receiving segment `90`, the receiver will send an ACK with acknowledgment number `100`. Upon receiving segment `120`, the receiver will send an ACK with acknowledgment number `100`. There is `1` duplicate ACK for `100`, but `100` wasn't lost; it just took longer than the other two segments.</p>
        <p>However, receiving `3` duplicate ACKs (or any number of duplicate ACKs for that matter) is not a guarantee that there was packet loss. `3` is just an arbitrarily good balance between waiting enough time and not waiting too long.</p>
      </div>
      <h4>Go-Back-N or Selective Repeat?</h4>
      <p>TCP is fairly similar to GBN. There is only one timer for the oldest unacknowledged segment. Out-of-order segments are not individually ACK'ed by the receiver. And there is a <code>base</code> and <code>nextseqnum</code> that the sender keeps track of.</p>
      <p>Where they differ. TCP buffers out-of-order segments (sending cumulative ACKs for them) while GBN ignores them (sending ACKs for the last in-order packet it received). If a timeout occurs, TCP will resend only one segment while GBN will resend `N` packets. Sending only one segment on timeout makes TCP similar to SR. The fact that out-of-order segments are buffered also means that the receiver is managing a buffer, similar to SR.</p>
      <p>Basicallly, TCP is a combination of GBN and SR.</p>
      <h3>Flow Control</h3>
      <p>When the receiver gets and buffers data, the application doesn't necessarily read from this buffer right away. As a result, the receive buffer may get full, which becomes a problem when the sender has more data to send. This can be prevented by TCP's <b>flow-control service</b>, which limits the sender's sending rate to match the application's reading rate.</p>
      <div class="ln-box">
        <p>Flow-control and congestion control are two different things, even though they both limit how much data the sender is sending. Flow-control is to prevent the sender from overwhelming the receiver's buffer while congestion control is to prevent the sender from overwhelming the network.</p>
      </div>
      <p>To simplify things, we'll assume that the TCP receiver discards out-of-order segments instead of buffering them.</p>
      <p>In order for the sender to not overwhelm the receiver, the sender needs to know how much space is available in the receiver's buffer. The sender can get an idea of the receiver's buffer size (denoted by <code>RcvBuffer</code>) by maintaining a variable called the <b>receive window</b> (denoted by <code>rwnd</code>), which tracks how much space is available in the buffer.</p>
      <p>Suppose Host A is sending a file to Host B. The receiver in Host B will keep track of <code>LastByteRead</code> and <code>LastByteRcvd</code>. <code>LastByteRead</code> is the last byte that the application read from the buffer. <code>LastByteRcvd</code> is the last byte that was placed in the buffer. So for example, things could look like this:</p>
      <table class="table text-center">
        <tr>
          <td><code>LastByteRead</code></td>
          <td></td>
          <td></td>
          <td><code>LastByteRcvd</code></td>
          <td></td>
        </tr>
        <tr>
          <td>`9`</td>
          <td>`10`</td>
          <td>`11`</td>
          <td>`12`</td>
          <td>`13`</td>
        </tr>
        <tr>
          <td>empty</td>
          <td>contains segment</td>
          <td>contains segment</td>
          <td>contains segment</td>
          <td>empty</td>
        </tr>
      </table>
      <p>Here, <code>RcvBuffer</code>`=5`, <code>LastByteRead</code>`=9`, <code>LastByteRcvd</code>`=12`. There are `2` empty spots in the buffer, so <code>rwnd</code>`=2`. This can be derived from:</p>
      <div class="ln-center">
        <p>`text(rwnd)=text(RcvBuffer)-(text(LastByteRcvd)-text(LastByteRead))`</p>
      </div>
      <p>There is a "receive window" section in the TCP header; this is where the receiver puts <code>rwnd</code>. So when the receiver sends a segment to the sender, the sender looks at this value to know how much space is available in the receiver's buffer. At the same time, the sender in Host A is keeping track of its own two variables: <code>LastByteSent</code> and <code>LastByteAcked</code>. In order to not overflow the receiver's buffer:</p>
      <div class="ln-center">
        <p>`text(LastByteSent)-text(LastByteAcked)letext(rwnd)`</p>
      </div>
      <p><code>LastByteSent</code> `-` <code>LastByteAcked</code> is the number of unACK'ed segments on the sender's side. From the sender's point of view, unACK'ed segments are traveling to the receiver. So if there are more unACK'ed segments traveling to the receiver than the receiver's buffer can handle, then obviously the receiver will be overwhelmed.</p>
      <p>There is one slight problem though. Suppose the receiver's buffer in Host B gets full. The receiver tells the sender in Host A that its buffer is full so that Host A will stop sending data. Also suppose that Host B has no more data to send to Host A. As the application in Host B is reading data from the receiver's buffer, spots in the buffer start becoming available again. However, since Host B has no more data to send to Host A, the sender in Host A never finds out that the buffer freed up some space.</p>
      <p>To get around this, the sender in Host A can occasionally send segments with `1` byte of data so that the receiver in Host B can acknowledge it and respond with its buffer space.</p>
      <h3>TCP Connection Management</h3>
      <p>As we've seen before, there is a <b>three-way handshake</b> when establishing a TCP connection. These are the steps involved:</p>
      <ol>
        <li>The client sends a TCP segment to the server. This segment has no data, but the SYN bit is set to `1`. Thus, this segment is called a SYN segment. Also, the segment has a randomly chosen sequence number (called <code>client_isn</code>). This is the client's way of telling the server what its initial sequence number will be.</li>
        <li>The server, seeing the SYN flag set to `1`, prepares all the buffers and variables (all the ones mentioned above) needed for TCP communication. Once this is done, the server sends a TCP segment back to the client, acknowledging the connection request. For the server's ACK segment:
          <ul>
            <li>SYN bit is set to `1`</li>
            <li>acknowledgment number is set to <code>client_isn</code>`+1`</li>
            <li>sequence number is set to a randomly chosen <code>server_isn</code></li>
          </ul>
          Thus, this segment is called a <b>SYNACK segment</b>.</li>
        </li>
        <li>When the client gets the SYNACK, the client sets up all the buffers and variables too. Then it sends another segment acknowledging the server's connection-granted segment.
          <ul>
            <li>SYN bit is set to `0` since the connection is established</li>
            <li>acknowledgment number is set to <code>server_isn</code>`+1`</li>
            <li>there may be application data if there is data to send</li>
          </ul>
        </li>
      </ol>
      <div class="ln-box">
        <p>A three-way handshake (as opposed to a two-way handshake) is needed because both sides can send and receive data. Therefore, they need to know each other's sequence numbers and that they are both ready to receive.</p>
        <p>In the first step, Host A will tell Host B its sequence number. In the second step, Host B will acknowledge the first step. So now Host B knows Host A's sequence number and Host A knows that Host B is ready to receive. But this only allows for `ArarrB` communication. Host B doesn't know if Host A is ready to receive.</p>
        <p>This is why the third step is needed. With Host A acknowledging the second step, Host B can now be sure that Host A is ready to receive, allowing for `AlarrB` communication.</p>
      </div>
      <p>Now let's say the client wants to close the connection. The client will send a segment with the FIN bit set to `1`, telling the server that it wants to close the connection. The server will acknowledge this request by sending an ACK for the segment.</p>
      <p>The connection is not closeable at this point though. The client sending the FIN segment only tells the server that it has no more data to send. But the server may still have some data to send to the client. Once the server has no more data to send to the client, the server will send its own FIN segment to the client. When the client receives it and sends an ACK for it, the connection will finally be closed.</p>
      <div class="ln-box">
        <p>The server can also initiate the request to close the connection with the client.</p>
      </div>
      <p>The TCP protocol goes through different <b>TCP states</b> throughout the lifecycle of a connection. For the client, it starts in the CLOSED state and transitions to SYN_SENT when it wants to establish a connection. When it receives a SYNACK from the server, the client moves to the ESTABLISHED state, in which data is transferred back and forth. When the client wants to close the connection, it transitions to FIN_WAIT_1, where it waits for the server to acknowledge the close request. Once the server acknowledges it, the client moves to FIN_WAIT_2, where the client waits for the server to send the FIN segment. After getting the FIN segment from the server, the client sends an ACK for it and moves to TIME_WAIT. The connection is not closed yet; the client waits for a specified time in case it needs to resend the ACK. After the wait, the connection is closed.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/tcp_state_client.png">
      </div>
      <p>For the server, most of the states seem pretty self-explanatory. The CLOSE_WAIT state handles the situation where the client wants to close the connection but the server still has data to send to the client. More generally, the wait is there so that any delayed segments (either still in the network or waiting to be sent by the server) can be handled.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/tcp_state_server.png">
      </div>
      <div class="ln-box">
        <p>If the client attempts to establish a connection to a non-valid port, the server will send a reset segment to the client. The reset segment has the RST flag set to `1`.</p>
      </div>
      <div class="ln-box">
        <h4>The SYN Flood Attack</h4>
        <p>As we saw earlier, when the client sends a SYN segment to the server in the first step of the handshake, the server prepares all the resources needed for the connection. Essentially, the server reserves those resources until the connection is established.</p>
        <p>Attackers can take advantage of this by sending a bunch (emphasis on a bunch) of SYN segments to the server and never completing the third step of the handshake. Not completing the handshake is the important part because this makes the server reserve a bunch of resources for connections that will never be established. Then, when legitimate clients want to establish a connection to the server, the server denies these requests because there are no more resources available â€” they were all reserved for the attackers who never plan on using those connections. This is a form of Denial of Service (DoS) attack. The server is forced to deny its service to everyone because of an attack.</p>
        <p>It may be obvious that when the server gets a SYN segment, it shouldn't immediately start reserving resources. In fact, the resources should only be allocated after the third step of the handshake is complete, i.e., after the client sends the ACK. But if the server is no longer maintaining any resources (like who wanted to connect in the first place), how would the server know that an ACK is from a client who sent the SYN segment earlier? This is where <b>SYN cookies</b> come in.</p>
        <p>When the server gets a SYN segment, it looks at the segment's source IP, source port, destination IP, and destination port. It uses all those numbers, along with a secret number, to create a special number that encodes all that information in a secure way. This value is the SYN cookie. This value is also the server's initial sequence number. So when the server sends the SYNACK in step 2 of the handshake, that SYNACK will have sequence number equal to the SYN cookie.</p>
        <p>If the client completes the third part of the handshake, it will send an ACK to the server. This ACK will have the SYN cookie `+1` as the acknowledgment number. When the server gets the ACK, the server performs the same operations it did the first time by using the ACK segment's source IP, source port, destination IP, destination port, and the server's secret number. If this generated number `+1` matches the acknowledgment number of the client's ACK, then the server knows that this ACK was for the SYN sent earlier.</p>
        <p>If the client doesn't complete the third part of the handshake, no harm is done because no resources were allocated.</p>
        <p>The server's secret number is an important part in generating the SYN cookie. IP addresses and port numbers are obtainable information, so anyone could generate the SYN cookie and send fake ACKs. Only the server knows the secret number, so only the server can tell whether the SYN cookie is real or fake.</p>
      </div>
      <h2 id="congestioncontrol">TCP Congestion Control</h2>
      <p>Network congestion is caused by too many senders sending data at a rate that is too fast for the network to handle. This can lead to packet delays and packet loss. TCP uses something called end-to-end congestion control. This means that congestion is detected by the presence of packet loss and delays.</p>
      <div class="ln-box">
        <p>The other approach for congestion control is network-assisted congestion control. This means that congestion is detected by the network routers telling us (the transport layer) that there is congestion.</p>
      </div>
      <h3>Classic TCP Congestion Control</h3>
      <p>The sender will control its send rate based on how much congestion is in the network. If there is little to no congestion, then the sender will increase its send rate. If there is congestion, then the sender will slow down its send rate. Firstly, how do we control the sender's sending rate?</p>
      <p>Earlier, we saw that the sender keeps track of <code>LastByteSent</code>, <code>LastByteAcked</code>, and <code>rwnd</code>. For congestion control, the sender keeps track of one more variable called the <b>congestion window</b> (denoted by <code>cwnd</code>). This congestion window is an arbitrary dynamic value that is used to limit the sender's rate. So now, the number of unACK'ed segments on the sender's side cannot exceed the receiver's free buffer spaces and the congestion window (whichever one is lower):</p>
      <div class="ln-center">
        <p>`text(LastByteSent)-text(LastByteAcked)le min{text(cwnd),text(rwnd)}`</p>
      </div>
      <p>To only focus on congestion control, we'll ignore <code>rwnd</code> for the rest of the section. If the sender has data to send, then it will be able to send <code>cwnd</code> bytes of data. After one RTT, the sender will receive ACKs for those bytes. Ignoring loss and packet delays, the sender is roughly sending data at a rate of `(text(cwnd))/(text(RTT))` bytes per second. So by adjusting the value of <code>cwnd</code>, we can control the sender's sending rate.</p>
      <div class="ln-box">
        <p>For example, if <code>cwnd</code>`=500` bytes and one RTT is `200` milliseconds, then the sender's sending rate is</p>
        <div class="ln-center">
          <p>`(text(cwnd))/(text(RTT))=(500text( bytes))/(200text( ms))=(500text( bytes))/(0.2text( seconds))=(2500text( bytes))/(1text( second))=2.5text( kilobytes/sec)`</p>
        </div>
      </div>
      <div class="ln-box">
        <p>One subtle note about the interpretation of RTT: instead of being the round-trip time for a single segment, RTT will be referring to the round-trip time for sending <code>cwnd</code> bytes.</p>
      </div>
      <p>Secondly, how can we (the sender) tell if there is congestion? Well, if there is a lot of congestion, then it's likely that packets will be dropped. We know packets are dropped (or considered dropped), when the timer goes off or three duplicate ACKs are received (see "Fast Retransmit"). So if there is a loss event, then the sender's sending rate should be lowered.</p>
      <p>If there is not a lot of congestion, then the sender will receive ACKs, in which case the sender's sending rate can be increased. However, there could be a middle-ground situation where there is not too much congestion, but there is still a fair amount of it. In this case, the ACKs will come back, just at a slower rate. This is an indication that the sender's sending rate can be increased, just not by too much. Since TCP uses acknowledgments to trigger (or clock) an increase in congestion window size, TCP is <b>self-clocking</b>.</p>
      <p>All of these points lead to this strategy: if ACKs keep coming, keep on increasing the sending rate until no more ACKs come. If no more ACKs come, then decrease the sending rate to a point where ACKs will come and start increasing the rate again.</p>
      <p>So we know when the sending rate should be changed, but now the question is by how much? There is a <b>TCP congestion-control algorithm</b> that decides.</p>
      <h4>Slow Start</h4>
      <p>At the beginning of the TCP connection, <code>cwnd</code> is initialized to `1` MSS, and every time an ACK is received, <code>cwnd</code> is increased by `1`. So the sender starts by sending `1` segment, then `2` segments, then `4` segments, then `8` segments, and so on. As we can see, the sending rate is doubled, causing it to grow exponentially. This process is called <b>slow start</b>.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/slow_start.gif">
      </div>
      <p>If there is a timeout, then <code>cwnd</code> will be reset to `1` and start doubling again. However, we know that we will eventually get a timeout again by repeating the same process. To prevent this, we keep track of what <code>cwnd</code> was when the timeout occurred and stop slow start before it reaches that point. The "slow start threshold" (denoted as <code>ssthresh</code>) is set to `text(cwnd)/2`, where the value of <code>cwnd</code> is its value when the timeout occurred. So slow start will keep doubling <code>cwnd</code> until its value is equal to <code>ssthresh</code>.</p>
      <h4>Congestion Avoidance</h4>
      <p>Now the value of <code>cwnd</code> is <code>ssthresh</code>. Since we're at a point where congestion could occur, the sending rate should be increased slowly. So rather than doubling <code>cwnd</code>, it will be increased by just `1` MSS instead. For example, if the MSS is `1000` bytes and <code>cwnd</code> is `10,000` bytes, then one RTT consists of `10` segments. After ACKs have been received for those `10` segments, the <code>cwnd</code> will be `10,000+1000=11,000` bytes.</p>
      <div class="ln-box">
        <p>This is equivalent to increasing <code>cwnd</code> by `text(MSS)/(text(cwnd)/text(MSS))=text(MSS)*(text(MSS)/text(cwnd))` on every ACK. `text(cwnd)/text(MSS)` is the number of segments that can be sent, which in turn, is the number of ACKs that will be received. So increasing <code>cwnd</code> by `text(MSS)/(text(cwnd)/text(MSS))` on every ACK will result in an increase of `1` MSS on all ACKs.</p>
        <p>For the above example, <code>cwnd</code> will be increased by `1000/((10,000)/1000)=100` bytes on every ACK. Since there are `10` ACKs in one RTT, <code>cwnd</code> will be increased by `100*10=1000` bytes `=1` MSS on all ACKs.</p>
      </div>
      <p>Even though we're increasing the sending rate cautiously, we will still eventually raise it to the point where there's a timeout. When that happens, <code>ssthresh</code> gets updated to the current value of <code>cwnd</code> and <code>cwnd</code> gets reset to `1`. Yeah, the same procedure as in slow start.</p>
      <h4>Fast Recovery</h4>
      <p>So far, we've only seen what happens when congestion is detected by a timeout. As we established earlier, packet loss, and thus congestion, can also be detected by triple duplicate ACKs. This type of congestion is not as bad though, because at least the ACKs were able to make it through. So the sending rate doesn't need to be decreased as much. <code>cwnd</code> is essentially* halved, which is also the updated value for <code>ssthresh</code>.</p>
      <div class="ln-box">
        <p>*Technically, <code>cwnd</code> is halved `+3`. <code>ssthresh</code> is still half of <code>cwnd</code> though (before the `+3`).</p>
      </div>
      <p>Now <code>cwnd</code> is increased by `1` MSS for every duplicate ACK that caused TCP to enter the fast recovery stage. If the ACK for the missing segment comes, then TCP goes back to congestion avoidance. But if a timeout occurs, then TCP goes back to slow start. Note that <code>cwnd</code> is not changing as new segments are sent and ACK'ed.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/fsm_congestion_control.png">
      </div>
      <p>Slow start and congestion avoidance are required for TCP, but fast start is only recommended. <b>TCP Tahoe</b> is an early version of TCP that only has slow start and congestion avoidance, so every time packet loss is detected, TCP switches to slow start regardless of whether it was a timeout or triple duplicate ACK. <b>TCP Reno</b>, a newer version of TCP, includes fast recovery.</p>
      <div class="ln-box">
        <p>The graph shows the changes to <code>cwnd</code> under TCP Reno in a hypothetical transmission scenario.</p>
        <div id="tcp_reno"></div>
        <p>What are the intervals of time when TCP slow start is operating?</p>
        <div id="tcp_reno_q1" class="collapse ln-box">
          <p>[1, 6] and [23, 26] because the congestion window size is doubling in these intervals</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q1" aria-expanded="false" aria-controls="tcp_reno_q1">Answer</button>
        <p>What are the intervals of time when TCP congestion avoidance is operating?</p>
        <div id="tcp_reno_q2" class="collapse ln-box">
          <p>[6, 16] and [17, 22] because the congestion window size is increasing by `1` in these intervals</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q2" aria-expanded="false" aria-controls="tcp_reno_q2">Answer</button>
        <p>After the 16th transmission round, is segment loss detected by a triple duplicate ACK or by a timeout?</p>
        <div id="tcp_reno_q3" class="collapse ln-box">
          <p>triple duplicate ACK because the congestion window size is halved`+3`</p>
          <p>(the easier way to tell is to see that the congestion window size is not dropped to `1`)</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q3" aria-expanded="false" aria-controls="tcp_reno_q3">Answer</button>
        <p>After the 22nd transmission round, is segment loss detected by a triple duplicate ACK or by a timeout?</p>
        <div id="tcp_reno_q4" class="collapse ln-box">
          <p>timeout because the congestion window size is dropped to `1`</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q4" aria-expanded="false" aria-controls="tcp_reno_q4">Answer</button>
        <p>What is the initial value of <code>ssthresh</code> at the first transmission round?</p>
        <div id="tcp_reno_q5" class="collapse ln-box">
          <p>32 because that's when slow start stops and congestion avoidance begins</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q5" aria-expanded="false" aria-controls="tcp_reno_q5">Answer</button>
        <p>What is the value of <code>ssthresh</code> at the 18th transmission round?</p>
        <div id="tcp_reno_q6" class="collapse ln-box">
          <p>21 because <code>ssthresh</code> gets set to half of whatever <code>cwnd</code> was when packet loss was detected, which was 42</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q6" aria-expanded="false" aria-controls="tcp_reno_q6">Answer</button>
        <p>What is the value of <code>ssthresh</code> at the 24th transmission round?</p>
        <div id="tcp_reno_q7" class="collapse ln-box">
          <p>14 because <code>ssthresh</code> gets set to half of whatever <code>cwnd</code> was when packet loss was detected, which was 29</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q7" aria-expanded="false" aria-controls="tcp_reno_q7">Answer</button>
        <p>During what transmission round is the 70th segment sent?</p>
        <div id="tcp_reno_q8" class="collapse ln-box">
          <p>7</p>
          <p>packet 1 is sent in round 1 (<code>cwnd</code> = 1)</p>
          <p>packets 2-3 are sent in round 2 (<code>cwnd</code> = 2)</p>
          <p>packets 4-7 are sent in round 3 (<code>cwnd</code> = 4)</p>
          <p>packets 8-15 are sent in round 4 (<code>cwnd</code> = 8)</p>
          <p>packets 16-31 are sent in round 5 (<code>cwnd</code> = 16)</p>
          <p>packets 32-63 are sent in round 6 (<code>cwnd</code> = 32)</p>
          <p>packets 64-96 are sent in round 7 (<code>cwnd</code> = 33)</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q8" aria-expanded="false" aria-controls="tcp_reno_q8">Answer</button>
        <p>Assuming a packet loss is detected after the 26th round by the receipt of a triple duplicate ACK, what will be the values of the <code>cwnd</code> and <code>ssthresh</code>?</p>
        <div id="tcp_reno_q9" class="collapse ln-box">
          <p><code>cwnd</code> = 7 and <code>ssthresh</code> = 4</p>
          <p>when packet loss is detected, <code>cwnd</code> = 8, so by the rules of fast recovery, <code>ssthresh</code> gets set to half of <code>cwnd</code> and <code>cwnd</code> gets set to half + 3</p>
        </div>
        <button class="btn collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#tcp_reno_q9" aria-expanded="false" aria-controls="tcp_reno_q9">Answer</button>
      </div>
      <h4>TCP Congestion Control: Retrospective</h4>
      <p>Since the congestion window grows exponentially during slow start, TCP doesn't remain in slow start for very long. So most of the changes to the congestion window size are adding `1` and dividing by `2`. Because of this, TCP congestion control is referred to as <b>additive-increase, multiplicative decrease</b>.
      <div class="ln-box">
        <p>This is assuming that most/all packet losses are detected by triple duplicate ACKs rather than timeouts. Although one could argue that setting the congestion window size to `1` is a multiplicative decrease (a division by <code>cwnd</code> ðŸ˜).</p>
      </div>
      <div class="ln-box">
        <h4>Macroscopic Description of TCP Reno Throughput</h4>
        <p>Let `w` be the current window size at any given time. Let `W` be the window size when a loss event occurs.</p>
        <p>If it takes one RTT to get ACKs for `w` bytes, then the sending rate is about `w/text(RTT)`. This rate keeps increasing by `1` MSS until it reaches `W/text(RTT)` when a loss event occurs. Then the rate drops to `W/text(RTT)*1/2=W/(2text(RTT))` in response to the loss event and increases by `1` again. And this repeats over and over again.</p>
        <p>Assuming the values of `W` and RTT stay mostly consistent the whole time, the sending rate increases between `W/text(RTT)` and `W/(2text(RTT))`. So the average throughput is</p>
        <div class="ln-center">
          <p>`(W/text(RTT)+W/(2text(RTT)))/2=((2W)/(2text(RTT))+W/(2text(RTT)))/2=((3W)/(2text(RTT)))/2=(3W)/(4text(RTT))=(0.75W)/text(RTT)`</p>
        </div>
      </div>
      <h3>Other TCP Variations</h3>
      <h4>Explicit Congestion Notification (ECN)</h4>
      <p>Even though TCP uses end-to-end congestion control, there have been some proposals to include network-assisted congestion control, in which the routers in the network layer explicitly say there is congestion. This involves carrying the information in two bits of the datagram's header field.</p>
      <p>If a router is experiencing congestion, then it will set one of the ECN bits to `1`. At the transport layer, if the TCP receiver sees that this bit is on, it will set the ECE bit to `1` in the ACK segment's header field, so that the TCP sender knows that there is congestion. ECE stands for "Explicit Congestion Notification Echo", so the receiver is "echoing" the news from the network layer that there's congestion.</p>
      <p>Seeing the ECE bit on, the TCP sender halves the congestion window (just like it would if there was actual packet loss) and sets the CWR (Congestion Window Reduced) bit in the header of the next transmitted segment to let the receiver know that it has reduced its window size. If the sender doesn't tell the receiver, then the receiver will think that the sender hasn't reduced its window size yet, so it will keep including the ECE bit, which in turn will force the sender to keep halving its window size.</p>
      <div class="ln-box">
        <p>The definition of congestion at the router level is not explicitly defined; it's up to the router vendor and network operator to work out what congestion means.</p>
        <p>One idea is that turning the bit on can mean that congestion is close to happening. Maybe the router's buffers are close to being full. This proactive detection is the opposite of end-to-end network congestion's reactive detection where congestion is detected after packet loss has already occurred.</p>
      </div>
      <div class="ln-box">
        <p>So one of the ECN bits is turned on by the router if it experiences congestion. The other ECN bit is used to let the routers know that the TCP sender and receiver are ECN-capable, that is, they know what to do if there is ECN-indicated network congestion.</p>
      </div>
      <h4>Delay-based Congestion Control</h4>
      <p>Another proactive approach to detecting congestion is to measure every RTT. Let's say the sender knows what the RTT is when there's no congestion at all (of course, this RTT would be relatively low). If most of the RTTs are close to that value, then the sender knows there isn't congestion. But if the RTTs start to become a lot larger than the uncongested RTT, then the sender knows that congestion is coming.</p>
      <div class="ln-box">
        <p>The RTT when there's no congestion at all can be denoted as `text(RTT)_text(min)`. Then the uncongested throughput is `text(cwnd)/text(RTT)_text(min)`. If the current actual throughput is close to this value, then the sending rate can be increased since there's no congestion. But if the current actual throughput is much lower than this, then the sending rate should be decreased since there is congestion.</p>
      </div>
      <p>TCP Vegas uses this approach.</p>
      <h3>Fairness</h3>
      <p>For a congestion-control mechanism to be fair, all connections should be able to achieve approximately equal transmission rates. If there are `K` TCP connections all passing through a bottleneck link with transmission rate `R`, then each connection's average transmission rate should be `R/K`, i.e., each connection should get an equal share of the link bandwidth.</p>
      <p>Suppose there are only `2` TCP connections sharing a single link with transmission rate `R`. To see if TCP's congestion control is fair, we could plot each connection's (hypothetical) throughputs throughout the duration of the connection. The full bandwidth utilization line marks the points where the link is at full capacity, which means the combined throughput is `R`. Note that if the combined throughput exceeds `R`, then there will be packet loss (since there will be more packets than the link can handle). The equal bandwidth share line marks the points where both of the connection's throughputs are equal to each other.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/fair_1.png">
      </div>
      <p>Their throughput should be on the full bandwidth utilization line so that the link is maximally utilized. However, to be fair, one connection shouldn't have much more throughput than the other. Ideally, their throughput should be in the middle of everything.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/fair_2.png">
      </div>
      <ol>
        <li>Let's say both of their throughputs start at point A. Since their throughputs are `lt R`, both connections will continuously increase their window sizes by `+1`.</li>
        <li>Eventually, their throughputs will be `gt R`, so there will be packet loss soon. Suppose that happens at point B.</li>
        <li>In response to congestion, both connections will halve their windows (to point C).</li>
        <li>Since their throughputs are `lt R`, both connections will continuously increase their window sizes by `+1`.</li>
        <li>Eventually, their throughputs will be `gt R`, so there will be packet loss again. Suppose that happens at point D.</li>
        <li>In response to congestion, both connections will halve their windows (to point E).</li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/fair_3.png">
      </div>
      <p>Notice that both throughputs eventually end up along the middle, where the equal bandwidth share line should be. Since both connections are achieving an equal amount of throughput, things are fair.</p>
      <div class="ln-box">
        <p>Once their throughputs are along the equal bandwidth share line, they will continue going back and forth along that line. By definition, the equal bandwidth share line is on the line `y=x`. Once their throughputs are on that line, their throughputs are equal to each other. So performing the same operations (`+1`, `-:2`) on the same numbers still keeps the numbers equal to each other. And since they're equal, they belong on the line `y=x`, which is the equal bandwidth share line.</p>
        <p>Also, their throughputs will converge to the middle regardless of where their initial throughputs are. It's sorta like dividing two even numbers by `2`, where one number may be larger than the other. Eventually, they both end up at `1`.</p>
      </div>
      <div class="ln-box">
        <p>Several assumptions were made for the above hypothetical scenario:</p>
        <ul>
          <li>each connection has a large amount of data to send</li>
          <li>there is no UDP traffic passing through the link</li>
          <li>the two connections have the same MSS and RTT</li>
          <li>they are in congestion avoidance mode at all times</li>
        </ul>
        <p>While these conditions are almost never true, the above idea about why TCP is fair is still true.</p>
      </div>
      <h4>Fairness and UDP</h4>
      <p>UDP doesn't have a congestion-control mechanism; it will just keep pushing data into the network. As a result, UDP traffic can theoretically force out TCP traffic. This isn't fair! But there are some mechanisms in place to prevent this from happening.</p>
      <h4>Fairness and Parallel TCP Connections</h4>
      <p>As we've seen before, applications can multiple parallel TCP connections. This can cause an application to unfairly get more of the available bandwidth.</p>
      <p>Suppose there are `9` applications that are each using only `1` TCP connection over a shared link with transmission rate `R`. So each application has a transmission rate of `R/9`. Now along comes another application, but this one opens `11` parallel TCP connections. Each connection will now have a transmission rate of `R/20`, but one of the applications owns `11` of those connections, so that application is using more than half of the available bandwidth.</p>
      <hr>
      <p>Now to the network layer!</p>
      <hr>
      <h2 id="networklayer">Overview of Network Layer</h2>
      <p>The network layer is responsible for transferring the transport layer's segment from router to router in the network core. So the network layer is implemented in end systems <em>and</em> routers (whereas the application and transport layer only existed in the end systems).</p>
      <p>The network layer can be divided into two parts: the <b>data plane</b> and the <b>control plane</b>. The data plane is responsible for forwarding a datagram from a router's input link to its output link. The control plane is responsible for routing a datagram from source to destination, i.e., deciding the complete path that a datagram will take from source to destination.</p>
      <h3>Forwarding and Routing: The Data and Control Planes</h3>
      <p>A router has multiple input and output links. When a packet moves to a router, the router takes it in through its input link. Then, for the packet to get to the correct destination, the router must forward the packet to the correct output link. Each router has a <b>forwarding table</b>, which tells the router which output link the packet should go to.</p>
      <p>But the router is not responsible for choosing which router the packet should go to. There are <b>routing algorithms</b> that determine the best router-to-router path that a packet should follow.</p>
      <h4>Control Plane: The Traditional Approach</h4>
      <p>The routing algorithm determines the contents of the routers' forwarding tables. Traditionally, each router ran a routing algorithm, and the routing algorithms communicated with each other to update the routers' forwarding tables.</p>
      <h4>Control Plane: The SDN Approach</h4>
      <p>Instead of running the routing algorithm in each router, there could be one remote controller that runs the routing algorithm, creates the forwarding tables, and distributes the forwarding tables to each router. The remote controller could be in a remote data center somewhere and be managed by the ISP or some third party. This approach is referred to as <b>software-defined networking (SDN)</b>, because the network is defined by a controller implemented in software.</p>
      <h3>Network Service Model</h3>
      <p>The network layer could theoretically provide:</p>
      <ul>
        <li>guaranteed delivery: a packet will arrive at its destination</li>
        <li>guaranteed delivery with bounded delay: a packet will arrive at its destination within a certain time</li>
        <li>in-order packet delivery: packets will arrive in the order they're sent</li>
        <li>guaranteed minimal bandwidth: packets will arrive as long as the sender transmits bits at a rate below the specified bit rate</li>
        <li>security: data will be encrypted</li>
      </ul>
      <p>But the network layer only actually provides one service: <b>best-effort service</b>. This means that the network layer will try its best to deliver the data, but there are no guarantees, including the things above. Surprisingly, this has been good enough.</p>
      <h2 id="routingalgorithms">Routing Algorithms</h2>
      <p>In the math world, a <b>graph</b> is a set of nodes and a collection of edges, where each edge is a connection from one node to another. `G=(N,E)`. A graph can be used to represent a network, where the nodes are the routers and the edges are the links. For two nodes `x` and `y`, an edge between them is denoted by `(x,y)`. If there is an edge between them, then they are <b>neighbors</b>.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/graph.png">
      </div>
      <p>An edge has a numerical value representing its cost. In the context of networks, this value can be the physical length of the link, the speed of the link, or the monetary cost of using the link. The cost of the link is denoted by `c(x,y)`. If there is no edge that exists for nodes `x` and `y`, then `c(x,y)=oo`.</p>
      <p>A <b>path</b> in a graph is a sequence of nodes and edges that can be traversed. The cost of a path is the sum of the cost of all the edges on that path.</p>
      <div class="ln-box">
        <p>A path is a sequence of `p` nodes `(x_1,x_2,...,x_p)` such that each of the pairs `(x_1,x_2),(x_2,x_3), ..., (x_(p-1),x_p)` are edges in the graph. The cost of a path is `c(x_1,x_2)+c(x_2,x_3)+...+c(x_(p-1),x_p)`.</p>
        <p>`(j,m,l,o)` is a path where the edges are `(j,m),(m,l),(l,o)`. The cost of this path is `c(j,m)+c(m,l)+c(l,o)=1+3+5=9`.</p>
      </div>
      <p>For any two nodes, there is typically more than one path between them. But at least one of them is the least costly. This path is the <b>least-cost</b> path. So the job of a routing algorithm is to find this least-cost path.</p>
      <p>A routing algorithm can be centralized or decentralized. In a <b>centralized routing algorithm</b>, each node has complete knowledge about the whole network, so every node knows the cost of every edge. These algorithms are referred to as <b>link-state (LS) algorithms</b>, since they know the state of every link. In a <b>decentralized algorithm</b>, each node only has information about the cost of its neighbors, and they communicate with each other to gradually calculate the best path.</p>
      <p>A routing algorithm can also be static or dynamic. In a <b>static routing algorithm</b>, routes change very slowly over time, often as a result of human intervention. In a <b>dynamic routing algorithm</b>, routes are dynamically updated in response to network topology or link cost changes.</p>
      <p>A routing algorithm can also be load-sensitive or load-insensitive. In a <b>load-sensitive algorithm</b>, the link costs change to reflect the current level of congestion in the link. <b>Load insensitive algorithms</b> are the opposite.</p>
      <h3>The Link-State (LS) Routing Algorithm</h3>
      <p>In order for each node to have complete information about the network, they send link-state packets to all other nodes informing each other of its neighbors' costs. This is referred to as a link-state advertisement or a link-state broadcast. After this is done, each node will have an identical view of the network and can then run the LS algorithm.</p>
      <p>One version of the LS algorithm is Dijkstra's algorithm. Dijkstra's algorithm is iterative. It keeps track of what the current least-cost path is so far, and, on every iteration, updates it as needed.</p>
      <div class="ln-box">
        <p>Yes, that Dijkstra.</p>
      </div>
      <p>Now, some notation. `D(v)` is the cost of the least-cost path from the source node to `v` as of the current iteration. `p(v)` is the previous node of `v` along the current least-cost path from the source to `v`. `N'` is the set of nodes where each node has a least-cost path calculated and determined.</p>
      <div class="ln-box">
        <p>This is the pseudocode for Dijkstra's algorithm. Let `u` be the source node.</p>
        <p>Initialization:</p>
        <p>`N' = {u}`<br>for all nodes `v`<br>&nbsp;&nbsp;if `v` is a neighbor of `u` then `D(v) = c(u,v)`<br>&nbsp;&nbsp;else `D(v)=oo`</p>
        <p>Loop:</p>
        <p>find `w` not in `N'` such that `D(w)` is a minimum<br>add `w` to `N'`<br>update `D(v)` for each neighbor `v` of `w` and not in `N'`:<br>&nbsp;&nbsp;`D(v)=min(D(v),D(w)+c(w,v))`<br>until `N'=N`</p>
      </div>
      <p>We'll use Dijkstra's algorithm on the graph above (copied and pasted below).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/graph.png">
      </div>
      <p>In the initialization step, we start at node `j` and calculate the cost to get to each node directly from `j`. Since `n` and `o` are directly unreachable from `j`, their cost on this iteration is `oo`.</p>
      <table class="table">
        <tr>
          <th>step</th>
          <th>`N'`</th>
          <th>`D(k),p(k)`</th>
          <th>`D(l),p(l)`</th>
          <th>`D(m),p(m)`</th>
          <th>`D(n),p(n)`</th>
          <th>`D(o),p(o)`</th>
        </tr>
        <tr>
          <td>`0`</td>
          <td>`j`</td>
          <td>`2,j`</td>
          <td>`5,j`</td>
          <td>`1,j`</td>
          <td>`oo`</td>
          <td>`oo`</td>
        </tr>
      </table>
      <p>In the first iteration, we look at the node with the current least cost (in this case, `m`) and go there. Then we calculate the cost to get to each neighbor directly from `m`. The cost to go to `k` from `m` is `3` (`1` for `jrarrm` and `2` for `mrarrk`), but it costs less to go to `k` from `j`, so we don't update `D(k),p(k)`. The cost to go to `l` from `m` is `4` (`1` for `jrarrm` and `3` for `mrarrl`), and it costs less than the current path to `l`, so we update `D(l),p(l)`. Node `n` is now reachable from `j`, so we update `D(n),p(n)`.</p>
      <table class="table">
        <tr>
          <th>step</th>
          <th>`N'`</th>
          <th>`D(k),p(k)`</th>
          <th>`D(l),p(l)`</th>
          <th>`D(m),p(m)`</th>
          <th>`D(n),p(n)`</th>
          <th>`D(o),p(o)`</th>
        </tr>
        <tr>
          <td>`0`</td>
          <td>`j`</td>
          <td>`2,j`</td>
          <td>`5,j`</td>
          <td>`1,j`</td>
          <td>`oo`</td>
          <td>`oo`</td>
        </tr>
        <tr>
          <td>`1`</td>
          <td>`jm`</td>
          <td>`2,j`</td>
          <td>`4,m`</td>
          <td></td>
          <td>`2,m`</td>
          <td>`oo`</td>
        </tr>
      </table>
      <p>In the second iteration, we look at the node with the current least cost and go there. (There's a tie between `k` and `n`, so we'll just choose `n`.) Then we calculate the cost to get to each neighbor directly from `n`. The cost to go to `l` from `n` is `3` (`2` for `jrarrmrarrn` and `1` for `nrarrl`), and it costs less than the current path to `l`, so we update `D(l),p(l)`. Node `o` is now reachable from `j`, so we update `D(o),p(o)`.</p>
      <table class="table">
        <tr>
          <th>step</th>
          <th>`N'`</th>
          <th>`D(k),p(k)`</th>
          <th>`D(l),p(l)`</th>
          <th>`D(m),p(m)`</th>
          <th>`D(n),p(n)`</th>
          <th>`D(o),p(o)`</th>
        </tr>
        <tr>
          <td>`0`</td>
          <td>`j`</td>
          <td>`2,j`</td>
          <td>`5,j`</td>
          <td>`1,j`</td>
          <td>`oo`</td>
          <td>`oo`</td>
        </tr>
        <tr>
          <td>`1`</td>
          <td>`jm`</td>
          <td>`2,j`</td>
          <td>`4,m`</td>
          <td></td>
          <td>`2,m`</td>
          <td>`oo`</td>
        </tr>
        <tr>
          <td>`2`</td>
          <td>`jmn`</td>
          <td>`2,j`</td>
          <td>`3,n`</td>
          <td></td>
          <td></td>
          <td>`4,n`</td>
        </tr>
      </table>
      <p>In the third iteration, we look at the node with the current least cost (in this case, `k`) and go there. It's important to note that for every iteration we do this, we have to pick the node that isn't in `N'` yet. The point of the algorithm is to visit every node so that we know what the least-cost path is from all nodes. (`N'` isn't the least-cost path; it's just the set of nodes that have been visited.) When looking at the neighbors, we also only look at the neighbors not in `N'` yet. So on this iteration, we only look at `l` (node `o` isn't a neighbor of `k`).</p>
      <table class="table">
        <tr>
          <th>step</th>
          <th>`N'`</th>
          <th>`D(k),p(k)`</th>
          <th>`D(l),p(l)`</th>
          <th>`D(m),p(m)`</th>
          <th>`D(n),p(n)`</th>
          <th>`D(o),p(o)`</th>
        </tr>
        <tr>
          <td>`0`</td>
          <td>`j`</td>
          <td>`2,j`</td>
          <td>`5,j`</td>
          <td>`1,j`</td>
          <td>`oo`</td>
          <td>`oo`</td>
        </tr>
        <tr>
          <td>`1`</td>
          <td>`jm`</td>
          <td>`2,j`</td>
          <td>`4,m`</td>
          <td></td>
          <td>`2,m`</td>
          <td>`oo`</td>
        </tr>
        <tr>
          <td>`2`</td>
          <td>`jmn`</td>
          <td>`2,j`</td>
          <td>`3,n`</td>
          <td></td>
          <td></td>
          <td>`4,n`</td>
        </tr>
        <tr>
          <td>`3`</td>
          <td>`jmnk`</td>
          <td></td>
          <td>`3,n`</td>
          <td></td>
          <td></td>
          <td>`4,n`</td>
        </tr>
      </table>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/graph.png">
      </div>
      <p>The table after finishing the algorithm looks like this:</p>
      <table class="table">
        <tr>
          <th>step</th>
          <th>`N'`</th>
          <th>`D(k),p(k)`</th>
          <th>`D(l),p(l)`</th>
          <th>`D(m),p(m)`</th>
          <th>`D(n),p(n)`</th>
          <th>`D(o),p(o)`</th>
        </tr>
        <tr>
          <td>`0`</td>
          <td>`j`</td>
          <td>`2,j`</td>
          <td>`5,j`</td>
          <td>`1,j`</td>
          <td>`oo`</td>
          <td>`oo`</td>
        </tr>
        <tr>
          <td>`1`</td>
          <td>`jm`</td>
          <td>`2,j`</td>
          <td>`4,m`</td>
          <td></td>
          <td>`2,m`</td>
          <td>`oo`</td>
        </tr>
        <tr>
          <td>`2`</td>
          <td>`jmn`</td>
          <td>`2,j`</td>
          <td>`3,n`</td>
          <td></td>
          <td></td>
          <td>`4,n`</td>
        </tr>
        <tr>
          <td>`3`</td>
          <td>`jmnk`</td>
          <td></td>
          <td>`3,n`</td>
          <td></td>
          <td></td>
          <td>`4,n`</td>
        </tr>
        <tr>
          <td>`4`</td>
          <td>`jmnkl`</td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td>`4,n`</td>
        </tr>
        <tr>
          <td>`5`</td>
          <td>`jmnklo`</td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
        </tr>
      </table>
      <p>So now, for every node, we have the cost to get to that node, and that cost is the lowest cost possible. For example, it costs `3` to get to node `l`, and that path involves going there from `n`. It costs `2` to get to node `n`, and that path involves going there from `m`. It costs `1` to get to node `m`, and that path involves going there from `j`. Using this information, we can trim the graph so that only the least-cost paths are kept.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/dijkstra.png">
      </div>
      <p>From this, we can create a forwarding table for all the routers to use:</p>
      <table class="table">
        <tr>
          <th>Destination</th>
          <th>Link</th>
        </tr>
        <tr>
          <td>`k`</td>
          <td>`(j,k)`</td>
        </tr>
        <tr>
          <td>`l`</td>
          <td>`(j,m)`</td>
        </tr>
        <tr>
          <td>`m`</td>
          <td>`(j,m)`</td>
        </tr>
        <tr>
          <td>`n`</td>
          <td>`(j,m)`</td>
        </tr>
        <tr>
          <td>`o`</td>
          <td>`(j,m)`</td>
        </tr>
      </table>
      <div class="ln-box">
        <p>This algorithm is `O(n^2)`. In the first iteration, we process `n` nodes when calculating `D(v),p(v)`. In the second iteration, we process `n-1` nodes. In the (second-to-) last iteration we process `1` node. So the number of operations is `n+n-1+...+1=(n(n+1))/2`.</p>
        <p>Using a heap to "find `w` not in `N'` such that `D(w)` is a minimum" can bring it down to `O(nlogn)`.</p>
      </div>
      <div class="ln-box">
        <p>Suppose we have a network of four routers where the cost of each link is the measure of how much traffic is on that link. Let's say `x` has data of size `1` to send to `w`, `y` has data of size `elt1` to send to `w`, and `z` has data of size `1` to send to `w`. At the beginning, no data has been sent yet, so all the costs will be `0`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/oscillation_1.png">
        </div>
        <p>After sending the data, the costs will now look like this:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/oscillation_2.png">
        </div>
        <p>After running the LS algorithm, each router will take the clockwise path since that direction is the least-cost path:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/oscillation_3.gif">
        </div>
        <p>After sending the data, the costs will now look like this:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/oscillation_4.png">
        </div>
        <p>After running the LS algorithm, each router will take the counterclockwise path since that direction is the least-cost path:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/oscillation_5.gif">
        </div>
        <p>After sending the data, the costs will now look like this:</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/oscillation_6.png">
        </div>
        <p>Notice that the routers will go back to sending the data clockwise again after running the LS algorithm. Earlier, we saw that if all the routers send the data clockwise, then they will send the data counterclockwise afterwards. After sending the data counterclockwise, they will send the data clockwise again. So the least-cost path will oscillate back and forth between clockwise and counterclockwise.</p>
        <p>This is the result of using congestion as the measure of a link's cost. But we can't prevent congestion from being used as a metric since the whole point of routing is to avoid congested links. To prevent oscillation, the solution is to include randomness. So have the routers perform link-state advertisement at random times and have the routers run the LS algorithm at random times if the routers are using the traditional approach.</p>
      </div>
      <h3>The Distance-Vector (DV) Routing Algorithm</h3>
      <p>One problem with the LS algorithm is that all the routers have to store the global information of the whole network. If there are a lot of routers in the network, then that's a lot of information for each router to store. So instead of having each router keep track of the whole network, each router can just keep track of its neighbors. Enter the <b>distance-vector (DV) algorithm</b>.</p>
      <p>Let's say we wanted to deliver some package (as fast as possible) to some location that we can't get to ourselves. However, there are several delivery centers between us and the destination. Naturally, we would start by going to the nearest delivery center. There, we could tell them, "Deliver this to the destination, but use the best path possible." The delivery center will look at the next delivery center that's best for reaching the destination and send it there. Each delivery center will tell its next delivery center, "Deliver this to the destination, but use the best path possible" until the pacakge arrives at the destination. As long as the best path is used each time, the overall path will be the best.</p>
      <p>Notice that we didn't have to know about all the delivery centers in the world; we only had to know the best delivery center for us. And similarly for each delivery center, each one only had to know about the delivery center that was best for it.</p>
      <div class="ln-box">
        <p>The Bellman-Ford equation encompasses the idea above. Let `d_x(y)` be the cost of the least-cost path from node `x` to node `y`.</p>
        <div class="ln-center">
          <p>`d_x(y)=min_v{c(x,v)+d_v(y)}`</p>
        </div>
        <p>The Bellman-Ford equation says that for all neighbors `v` of `x`, look at their least-cost paths to `y` and the cost of going to that neighbor `v`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/graph.png">
        </div>
        <p>For example, let's consider the least-cost path from `j` to `o`. Node `j` has three neighbors: `k`, `l`, and `m`.</p>
        <ul>
          <li>For `k`:
            <ul>
              <li>the least-cost path to `o` has a cost of `5` (`krarrmrarrnrarro`)</li>
              <li>the cost to get to `k` is `2`</li>
            </ul>
          </li>
          <li>For `l`:
            <ul>
              <li>the least-cost path to `o` has a cost of `3` (`lrarrnrarro`)</li>
              <li>the cost to get to `l` is `5`</li>
            </ul>
          </li>
          <li>For `m`:
            <ul>
              <li>the least-cost path to `o` has a cost of `3` (`mrarrnrarro`)</li>
              <li>the cost to get to `m` is `1`</li>
            </ul>
          </li>
        </ul>
        <p>So `d_j(o)=min{2+5,5+3,3+1}=min{7,8,4}=4`.</p>
      </div>
      <p>Now for the actual DV algorithm. Each node begins with a list of costs from itself to every other node. Each node also keeps track of its neighbors' lists. From time to time, each node sends a copy of its own list to its neighbors. When a node receives a list from a neighbor, the node saves the neighbor's list and uses it to update its own list. If there were any changes, then that node will send its list to its neighbors. Initially, each node will start with a list of estimated costs, but eventually, the list will contain the actual costs.</p>
      <div class="ln-box">
        <p>Let `D_x(y)` be the estimated cost of the least-cost path from `x` to `y` for all `yinN`. Each node `x` begins with a distance vector `bbD_x(y)=[D_x(y):yinN]`. Each node also keeps track of its neighbors' distance vectors `bbD_v(y)=[D_v(y):yinN]` for neighbors `v` of `x`. Whenever a node `x` receives a distance vector from one of its neighbors, node `x` will update its own distance vector using the Bellman-Ford equation:</p>
        <div class="ln-center">
          <p>`D_x(y)=min_v{c(x,v)+D_v(y)}`</p>
          <p>where `c(x,v)` is the cost of going to `v` from `x`</p>
        </div>
        <p>Initially, `D_x(y)` will be an estimated cost, but eventually, `D_x(y)` will converge to `d_x(y)`, which we denoted as the actual cost of the least-cost path from `x` to `y`.</p>
      </div>
      <div class="ln-box">
        <p>This is the pseudocode for the Distance-Vector algorithm.</p>
        <p>Initialization for each node `x`:</p>
        <p>for all destinations `y` in `N`<br>&nbsp;&nbsp;`D_x(y)=c(x,y)`<br>for each neighbor `w`<br>&nbsp;&nbsp;`D_w(y)=?` for all destinations `y` in `N`<br>for each neighbor `w`<br>&nbsp;&nbsp;send distance vector `bbD_x(y)=[D_x(y):yinN]` to `w`</p>
        <p>Loop over each node `x`:</p>
        <p>wait until there is a link cost change to some neighbor `w` or until a distance vector is received from neighbor `w`<br>for each `y` in `N`<br>&nbsp;&nbsp;`D_x(y)=min_v{c(x,v)+D_v(y)}`<br>if `D_x(y)` changed for any destination `y`<br>&nbsp;&nbsp;send distance vector `bbD_x(y)=[D_x(y):yinN]` to all neighbors</p>
      </div>
      <p>We'll use the DV algorithm on a network of three nodes.</p>
      <div class="ln-center">
        <img class="img-fluid" src="../pictures/networking/dv.png">
      </div>
      <p>At the start, all three nodes will have their own initial <b>routing tables</b> (a row is a distance vector). Each node hasn't received the distance vector from their neighbors yet, so the values for each node's neighbors are `oo`.</p>
      <div class="row">
        <div class="col-sm">
          <img class="img-fluid" src="../pictures/networking/dv_1.png">
        </div>
        <div class="col-sm">
          <img class="img-fluid" src="../pictures/networking/dv_2.png">
        </div>
        <div class="col-sm">
          <img class="img-fluid" src="../pictures/networking/dv_3.png">
        </div>
      </div>
      <p>Now, each node will send their distance vector to their neighbors.</p>
      <div class="row">
        <div class="col-sm">
          <img class="img-fluid" src="../pictures/networking/dv_4.png">
        </div>
        <div class="col-sm">
          <img class="img-fluid" src="../pictures/networking/dv_5.png">
        </div>
        <div class="col-sm">
          <img class="img-fluid" src="../pictures/networking/dv_6.png">
        </div>
      </div>
      <p>Upon receiving a distance vector from their neighbors, each node will update their own distance vectors. Let's start with `x`:</p>
      <div class="ln-center">
        <p>`D_x(y)=min{c(x,y)+D_y(y),c(x,z)+D_z(y)}=min{2+0,7+1}=min{2,8}=2`</p>
        <p>`D_x(z)=min{c(x,y)+D_y(z),c(x,z)+D_z(z)}=min{2+1,7+0}=min{3,7}=3`</p>
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/dv_7.png">
      </div>
      <p>Since `x`'s distance vector changed, it will now send its updated distance vector to its neighbors. For `y`:</p>
      <div class="ln-center">
        <p>`D_y(x)=min{c(y,x)+D_x(x),c(y,z)+D_z(x)}=min{2+0,1+7}=min{2,8}=2`</p>
        <p>`D_y(z)=min{c(y,x)+D_x(z),c(y,z)+D_z(z)}=min{2+7,1+0}=min{9,1}=1`</p>
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/dv_8.png">
      </div>
      <p>Notice that `y`'s distance vector did not change. So `y` will not send its distance vector to its neighbors.</p>
      <p>We still have to calculate `z`'s distance vector as a result of `x` sending its distance vector to its neighbors:</p>
      <div class="ln-center">
        <p>`D_z(x)=min{c(z,x)+D_x(x),c(z,y)+D_y(x)}=min{7+0,1+2}=min{7,3}=3`</p>
        <p>`D_z(y)=min{c(z,x)+D_x(y),c(z,y)+D_y(y)}=min{7+2,1+0}=min{9,1}=1`</p>
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/dv_9.png">
      </div>
      <p>`z`'s distance vector changed, so `z` will send its updated distance vector to its neighbors. If we decide to continue with the math, we'll find that `x` and `y`'s distance vectors don't change from `z`'s updated distance vector. So the final routing table for all three nodes will look like the one above. At this point, since no nodes are sending distance vectors, all the nodes will just stay in a waiting state until there is a link cost change.</p>
      <div class="ln-box">
        <p>Even though we went through the example synchronously (starting with `x`, then updating `y` and `z`), the DV algorithm is asynchronous. So computing distance vectors and sending distance vectors can happen in any order at any time, depending on what happens to occur first. For example, instead of starting with `x`, we could've started with `y`.</p>
        <p>The DV algorithm is also self-terminating. There is no explicit signal to stop; things just naturally stop when there is no updated information.</p>
      </div>
      <h4>Link-Cost Changes and Link Failure</h4>
      <p>Let's look at the same three-node network but with different link costs this time.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/link_cost_change.png">
      </div>
      <p>Suppose the `x-y` link changes from `4` to `1` and that `y` detects this change first. `y`'s distance vector will be updated so that `D_y(x)=1` instead of `4`. Then, `y` will send its distance vector to its neighbors. When `z` gets `y`'s updated distance vector, `z` will update its own distance vector so that `D_z(x)=2`. (`x` will also update its own distance vector, but we'll only focus on `y` and `z` for this section.) Since `z`'s distance vector changed, `z` will send its distance vector to `y`, but `y`'s distance vector won't be changed, so everything will stop.</p>
      <p>So things go normally if a link cost decreases. But what happens if the link cost increases? Let's say the it increases from `4` to `60` instead.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/link_cost_change_2.png">
      </div>
      <p>This is `y`'s table before it updates its distance vector.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/dv_link_cost_increase_1.png">
      </div>
      <p>`y` will detect this change and update its distance vector:</p>
      <div class="ln-center">
        <p>`D_y(x)=min{c(y,x)+D_x(x),c(y,z)+D_z(x)}=min{60+0,1+5}=6`</p>
      </div>
      <p>But this is clearly wrong since it's impossible to get from `y` to `x` with a cost of `6`. Of course, we can tell it's impossible since we have a global view of the network, but the nodes don't. This happened because `y` was using outdated information, namely `D_z(x)=5`; and this was outdated because `z` hadn't updated its distance vector yet.</p>
      <p>At this point, there is also a <b>routing loop</b>. If `y` wants to send a packet to `x`, it will send it to `z` first since `y` thinks `z` can get to `x` with a cost of only `5`. But `z` can only get to `x` with a cost of `5` by going through `y`! (since `z` still believes `y` can get to `x` with a cost of `4`) So if there happens to be a packet to send at this time, then it will get bounced back and forth between `y` and `z`.</p>
      <p>Since `y`'s distance vector changed, `y` will send its distance vector to `z`, which will update its distance vector by performing:</p>
      <div class="ln-center">
        <p>`D_z(x)=min{c(z,x)+D_x(x),c(z,y)+D_y(x)}=min{50+0,1+6}=min{50+7}=7`</p>
      </div>
      <p>These are the tables of `y` and `z` after `z` updates its distance vector:</p>
      <div class="row">
        <div class="col-sm">
          <div class="ln-center">
            <img class="img-fluid ln-image-small" src="../pictures/networking/dv_link_cost_increase_2.png">
          </div>
        </div>
        <div class="col-sm">
          <div class="ln-center">
            <img class="img-fluid ln-image-small" src="../pictures/networking/dv_link_cost_increase_3.png">
          </div>
        </div>
      </div>
      <p>Since `z`'s distance vector updated, `z` will send its distance vector to `y`, which will update its distance vector so that `D_y(x)=8`. Then `z` will update its distance vector so that `D_z(x)=9`. Then `y` will update its distance vector so that `D_y(x)=10`. And so on.</p>
      <p>Eventually, `z` will perform the calculation:</p>
      <div class="ln-center">
        <p>`D_z(x)=min{c(z,x)+D_x(x),c(z,y)+D_y(x)}=min{50+0,1+50}=min{50+51}=50`</p>
      </div>
      <p>Finally, this will reflect the current state of the network: it is cheaper to go from `z` to `x` than it is to go from `z` to `y` to `x`. However, it took a long time to get there. So if a link cost increases, it can potentially take a long time for the update to propagate among the nodes.</p>
      <div class="ln-box">
        <p>It would take even longer if the link cost had changed from `4` to `10,000` and the cost to go to `x` from `z` had been `9999`. Because of situations like this, this problem is referred to as the count-to-infinity problem.</p>
      </div>
      <h4>Adding Poisoned Reverse</h4>
      <p>This can be avoided with a little trick called poisoned reverse. If the least-cost path from `z` to `x` is through `y`, then `z` will lie and tell `y` that `z` has no path to `x`, i.e., `D_z(x)=oo`. This way, `y` will not go through `z` to get to `x`.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/link_cost_change_2.png">
      </div>
      <p>Let's look at the example again but with poisoned reverse. The link cost changes from `4` to `60`, so `y` will update its distance vector. This time, `y` thinks that `D_z(x)=oo`, so `y` will calculate `D_y(x)` to be</p>
      <div class="ln-center">
        <p>`D_y(x)=min{c(y,x)+D_x(x),c(y,z)+D_z(x)}=min{60+0,1+oo}=60`</p>
      </div>
      <p>And `z` will update its  `D_z(x)` to be</p>
      <div class="ln-center">
        <p>`D_z(x)=min{c(z,x)+D_x(x),c(z,y)+D_y(x)}=min{50+0,1+60}=50`</p>
      </div>
      <p>And in response, `y` will update its `D_y(x)` to be</p>
      <div class="ln-center">
        <p>`D_y(x)=min{c(y,x)+D_x(x),c(y,z)+D_z(x)}=min{60+0,1+50}=51`</p>
      </div>
      <p>which means that `y` is now going to `x` through `z`. And, since `y` is going to `x` through `z`, `y` will tell `z` that `y` has no path to `x`, i.e., `D_y(x)=oo`.</p>
      <p>And just like that, all the nodes have the correct information about the current state of the network.</p>
      <div class="ln-box">
        <p>It's called poisoned reverse because the node is poisoning the reverse path so that the other node can't use it.</p>
      </div>
      <div class="ln-box">
        <p>This poisoned reverse trick won't work for loops involving three or more nodes. The above example only had a two-node loop between `y` and `z`.</p>
      </div>
      <div class="ln-box">
        <h4>LS vs DV</h4>
        <p>Let `N` be the set of nodes (routers) and `E` be the set of edges (links).</p>
        <table class="table table-bordered">
          <tr>
            <th></th>
            <th>LS</th>
            <th>DV</th>
          </tr>
          <tr>
            <th>Message Complexity</th>
            <td>Since LS is global, each node needs to know about every other node and link. This means `O(|N||E|)` messages need to be sent on every run.</td>
            <td>Since DV is decentralized, each node only needs to know about and exchange messages with its neighbors.</td>
          </tr>
          <tr>
            <th>Speed of Convergence</th>
            <td>LS takes `O(|N|^2)` time to complete. (For every node, we need to calculate the least-cost path to every other node.)</td>
            <td>DV can converge slowly (from the count-to-infinity problem) and run into routing loops while converging.</td>
          </tr>
          <tr>
            <th>Robustness (What if a router malfunctions?)</th>
            <td>Since every node is computing their own forwarding table, route calculations are indpendent of each other across nodes. So if one node does a wrong calculation, only that node will have an incorrect forwarding table.</td>
            <td>If one node spreads wrong information, that info will be propagated across the whole network.</td>
          </tr>
        </table>
        <p>Neither algorithm is really that much better than the other. In the Internet, both of them are used.</p>
      </div>
      <h2 id="ospf">Intra-AS Routing in the Internet: OSPF</h2>
      <p>So far, we've been assuming that all the routers are running the same routing algorithm. This isn't really practical because in the real world, there are billions of routers. All of them can't have enough memory to store this much information for LS. As for DV, running DV over this many routers would never converge.</p>
      <p>Also recall that the Internet is a network of ISPs, which is a network of routers. An ISP can choose which algorithm it wants its routers to run. So each network of routers could potentially be running different algorithms anyway.</p>
      <p>In fact, routers are organized into <b>autonomous systems (ASs)</b>, where each AS is typically controlled by an ISP. All the routers in the same AS run the same routing algorithm and communicate within the AS. Regardless of whichever algorithm it is, the routing algorithm that is running in an AS is called an <b>intra-autonomous system routing protocol</b>.</p>
      <h4>Open Shortest Path First (OSPF)</h4>
      <p>OSPF is an intra-AS routing protocol that uses Dijkstra's algorithm, so each router will have a complete topological map of the network in the AS. Individual link costs are configured by network admins.</p>
      <div class="ln-box">
        <p>When talking about the LS algorithm, we looked at it as "given a set of costs, find the least-cost path". In practice, costs are assigned by network admins to influence where they want traffic to go. That is, the optimal paths are already known, and the network admin must find the right set of costs so that things are optimal.</p>
        <p>For example, a link's cost can be inversely proportional to its capacity so that low-bandwidth links are not prioritized.</p>
      </div>
      <p>Link-state advertisements are sent to all the routers whenever there is a link change (e.g., cost change or status change) and every 30 minutes.</p>
      <p>All messages exchanged between OSPF routers can be authenticated, which means that only trusted routers can talk to each other. This prevents attackers from forging OSPF messages and sending incorrect routing information.</p>
      <div class="ln-box">
        <p>OSPF messages are exchanged over the IP protocol, not TCP or UDP.</p>
      </div>
      <p>If there are multiple paths that have the same least cost, then they any/all of them can be used.</p>
      <p>An OSPF system can be divided into areas. Each area runs its own LS algorithm and the routers broadcast to other routers in their area. Then there is a backbone area that is responsible for routing traffic to each of the different areas. And at the edge of each area, there is an area border router that transfers messages from the area to the backbone area.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/ospf.png">
      </div>
      <h2 id="bgp">Routing Among the ISPs: BGP</h2>
      <p>An intra-AS routing protocol, like OSPF, routes packets from a source to a destination inside the AS. But to route a packet from one AS to another AS, an <b>inter-autonomous system routing protocol</b> is needed. And in order for the communication between ASs to work, they all need to be using the same inter-AS routing protocol. In fact, the inter-AS routing protocol that all ASs run is the Border Gateway Protocol, a.k.a, <b>BGP</b>.</p>
      <h3>The Role of BGP</h3>
      <p>A subnet is a smaller part of a larger network (in this case, an AS) where all the IP addresses of the routers in the subnet start with the same prefix. BGP allows subnets to advertise their existence to all the routers in the Internet. And as mentioned earlier, BGP also determines the best route from one AS to another AS.</p>
      <div class="ln-box">
        <p>Having subnets reduces the amount of memory needed since the forwarding tables only need to store the prefix instead of the IP addresses of all the routers in the subnet.</p>
      </div>
      <h3>Advertising BGP Route Information</h3>
      <p>A router in an AS is either a <b>gateway router</b> or an <b>internal router</b>. A gateway router sits at the edge of an AS and connects to other routers in other ASs. An internal router only connects to hosts and routers in the AS.</p>
      <p>Suppose we have a network with three autonomous systems: AS1, AS2, and AS3. Suppose AS3 includes a subnet with prefix x. We'll look at the details involved to make the routers in AS1 and AS2 aware of x.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/bgp.png">
      </div>
      <p>When routers send messages to each other, they use semi-permanent TCP connections. A connection between routers is called a <b>BGP connection</b>. If the two routers communicating are in the same AS, then their connection is an <b>internal BGP (iBGP)</b> connection. If the two routers are in different ASs, then their connection is an <b>external BGP (eBGP)</b> connection.</p>
      <p>So the steps for advertising the reachability information for x are:</p>
      <ol>
        <li>Gateway router 3a sends an eBGP message "AS3 x" to gateway router 2c.</li>
        <li>Gateway router 2c sends an iBGP message "AS3 x" to all the routers in AS2, including gateway router 2a.</li>
        <li>Gateway router 2a sends an eBGP message "AS2 AS3 x" to gateway router 1c.</li>
        <li>Gateway router 1c sends an iBGP message "AS2 AS3 x" to all the routers in AS1.</li>
      </ol>
      <h3>Determining the Best Routes</h3>
      <p>There can be more than one way to reach a prefix. Suppose, there's a connection from 1d to 3d.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/bgp_2.png">
      </div>
      <p>When a router sends a message advertising a prefix, it includes some <b>BGP attributes</b>. The prefix with its attributes is called a <b>route</b>.</p>
      <p>The AS-PATH attribute is a list of ASs that the advertisement has passed through so far. When the message arrives at an AS, the AS adds itself to the AS-PATH list. But only if it isn't in the list yet to prevent loops.</p>
      <p>There are two paths from AS1 to x:</p>
      <ul>
        <li>AS2 AS3 x</li>
        <li>AS3 x</li>
      </ul>
      <p>The NEXT-HOP attribute is the IP address of the router at the beginning of the AS-PATH. For the route "AS2 AS3 x", the NEXT-HOP attribute is the IP address of router 2a. The NEXT-HOP attribute of the route "AS3 x" is the IP address of router 3d.</p>
      <h4>Hot Potato Routing ðŸ¥”</h4>
      <p>Hot potato routing tries to forward packets to other ASs as fast as possible. In this case, "fast" means "least cost".</p>
      <p>Let's look at router 1b. This router has two routes to choose from: "AS2 AS3 x" and "AS3 x". Router 1b will use its intra-AS routing information to find the least-cost intra-AS path to 2a and and the least-cost intra-AS path to 3d and select the path with the smaller cost. We can define cost to be the number of links traversed for example. Then the cost of route "AS2 AS3 x" is `2` and the cost of route "AS3 x" is `3`, so 1b would choose route "AS2 AS3 x" and add router 2a to its forwarding table.</p>
      <h4>Route-Selection Algorithm</h4>
      <p>The actual process of choosing the best route is a little bit more complicated, but it does include hot potato routing.</p>
      <ol>
        <li>In addition to the AS-PATH and NEXT-HOP attributes, there is another attribute for a route's <b>local preference</b>. The local preference is decided by the AS's network admin. Routes with the highest local preference values are prioritized.</li>
        <li>If there are multiple routes with the same local preference values, then the route with the shortest AS-PATH is selected.</li>
        <li>If there are multiple routes with the same local preference values and the same AS-PATH length, then hot potato routing is used; so the route with the closest NEXT-HOP router is picked.</li>
        <li>If there are still multiple routes to choose from at this point, then BGP identifiers are used.</li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/bgp_2.png">
      </div>
      <p>Let's look at router 1b again. We saw that it had two routes to choose from: one that went to AS2 and one that went to AS3. We also saw that under hot potato routing, the AS2 path is chosen. But under this new route-selection algorithm, rule 2 is applied before hot potato (rule 3). And rule 2 says to choose the route with the shortest AS PATH, which in this case is the route to AS3.</p>
      <h3>Routing Policy</h3>
      <p>For rule 1 of the route-selection algorithm, the local preference attribute is set by an AS routing policy. For example, a policy can prevent ASs from advertising a path to other ASs. ISPs may want to do this so that they don't have to forward traffic from customers that aren't subscribed to them.</p>
      <p>For example, suppose we have six ASs where `W,X,Y` are access ISPs and `A,B,C` are provider networks. Suppose `A` advertises to `B` that there is a path to `W` through `A`. `B` can choose to advertise this path to `C`. Now `C` knows of a route to `W`, so if `Y` wants to communicate with `W`, then `C` will route through `B`.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/route_policy.png">
      </div>
      <p>But now `B` has to carry traffic between `Y` and `W`, both of which aren't customers of `B`. So it's in `B`'s interest to not advertise the path "BAW" to `C`.</p>
      <div class="ln-box">
        <h3>IP-Anycast</h3>
        <p>IP-anycast is a service that allows multiple servers to share the same IP address. So when a request is sent to that IP address, any of the servers can pick up the request. This is helpful for CDNs and DNS since they both replicate their contents across multiple servers. BGP implements this IP-anycast service using the route-selection algorithm to select the "nearest" server.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/ip_anycast.png">
        </div>
        <p>Suppose a CDN company assigns the same IP address (212.21.21.21) to all of its servers. When routers receive advertisements for this IP address, they will think that there are multiple routes to the same location, when in fact they are in different locations. This way, the closest server can be used whenever content is requested from the CDN.</p>
        <div class="ln-box">
          <p>IP-anycast is actually not really used by CDNs because changes in routes can cause different packets of the same TCP connection to end up at different servers. IP-anycast is used more by DNS.</p>
        </div>
      </div>
    </div>
    <script src="https://cdn.plot.ly/plotly-2.26.0.min.js" charset="utf-8"></script>
    <script>
      Plotly.newPlot('tcp_reno', [{
        x: [1, 2, 3, 4, 5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26],
        y: [1, 2, 4, 8, 16, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 24, 25, 26, 27, 28, 29, 1,  2,  4,  8],
        mode: 'lines+markers',
        type: 'scatter'
      }], {
        xaxis: {
          dtick: 1,
          range: [0, 27],
          title: {
            text: 'Transmission Round'
          }
        },
        yaxis: {
          dtick: 5,
          range: [0,45],
          title: {
            text: 'Congestion Window Size (segments)'
          }
        }
      }, {
        responsive: true,
        staticPlot: true
      });
    </script>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
