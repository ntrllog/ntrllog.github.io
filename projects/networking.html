<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
                          _   _
     _  _    _     _  _  | | | |   __     ____
    | |/ \  | |_  | |/_| | | | |  /  \   /    \
    | |   | |  _| |  /   | | | | | || | |  ||  |
    | |   | | |_  | |    | | | | | || | |  ||  |
    |_|   | |___| |_|    |_| |_|  \__/   \____/|
                                               |
                                          ____/
    -->
    <title>ntrllog | Computer Networking</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
    <link rel="shortcut icon" type="image/png" href="../pictures/favicon.ico"/>
    <link href="../css/content.css" rel="stylesheet">
  </head>
  <body>
    <div class="dropdown ln-fixed-right">
      <button class="btn btn-secondary dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">ToC</button>
      <ul class="dropdown-menu">
        <li><a class="dropdown-item" href="#whatisit">What is the Internet?</a></li>
        <li><a class="dropdown-item" href="#networkedge">Livin' on the Edge</a></li>
        <li><a class="dropdown-item" href="#networkcore">The Network Core</a></li>
        <li><a class="dropdown-item" href="#delay">Delay, Loss, and Throughput</a></li>
        <li><a class="dropdown-item" href="#protocollayers">Protocol Layers</a></li>
        <li><a class="dropdown-item" href="#networkapplications">Principles of Network Applications</a></li>
        <li><a class="dropdown-item" href="#http">The Web and HTTP</a></li>
        <li><a class="dropdown-item" href="#email">Email</a></li>
        <li><a class="dropdown-item" href="#dns">DNS</a></li>
        <li><a class="dropdown-item" href="#p2p">Peer-to-Peer File Distribution</a></li>
        <li><a class="dropdown-item" href="#cdn">Video Streaming and Content Distribution Networks</a></li>
      </ul>
    </div>
    <div class="container ln-line-height">
      <a href="projects.html"><i class="fas fa-long-arrow-alt-left fa-2x"></i></a>
      <h1>Computer Networking</h1>
      <hr>
      <p>Shortcut to this page: <a href="networking.html">ntrllog.netlify.app/networking</a></p>
      <p>Info provided by <a href="https://gaia.cs.umass.edu/kurose_ross/index.php" target="_blank">Computer Networking: A Top-Down Approach</a> and Professor Senhua Yu.</p>
      <h2 id="whatisit">What is the Internet?</h2>
      <p>Computers. Laptops. Smartphones. TVs. Watches. And everything else that has the word "smart" in front of it. All of these things that can connect to the Internet are called <b>hosts</b> or <b>end systems</b>. End systems send and receive data through a system of communcation links and packet switches. The data travels through this system as little pieces called packets and they are reassembled when they reach their destination. Packet switches are machines that help transfer these packets. The whole path that a packet takes is called a <b>route</b> or <b>path</b>.</p>
      <div class="ln-box">
        <p>A <b>router</b> is a type of packet switch. Yes, the router that connects to the modem.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/nuts_and_bolts.png">
      </div>
      <div class="ln-box">
        <p>I believe the shape I'll be using for all packet switches is a circle/oval. And links will be just a line connecting the circles.</p>
      </div>
      <p>Access to the Internet is provided by <b>Internet Service Providers</b> (<b>ISPs</b>). Think ATT and Spectrum for example. ISPs themselves are a network of packet switches and communication links.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/isp.png">
      </div>
      <h3>What is a Protocol?</h3>
      <p>All the devices involved with sending and receiving packets run <b>protocols</b>, which are a set of rules defining:</p>
      <ul>
        <li>how messages should be formatted</li>
        <li>the order of the messages exchanged</li>
        <li>the actions that can be performed when the messages are received or transmitted</li>
      </ul>
      <p>The protocols are defined by <b>Internet standards</b>, which are developed by the Internet Engineering Task Force (IETF). The actual documents are called <b>requests for comments</b> (<b>RFCs</b>).</p>
      <div class="ln-box">
        <p>When we go to a URL on our browser, the actions that occur are a result of following a protocol (the TCP protocol).</p>
        <ol>
          <li>Computer sends a request to connect to the Web server and waits for a reply</li>
          <li>Web server receives the connection request and returns a "connection okay" message</li>
          <li>Computer sends the name of the Web page we want</li>
          <li>Web server sends it to our computer</li>
        </ol>
      </div>
      <h2 id="networkedge">Livin' on the Edge</h2>
      <p>The word "end system" is used to describe devices that connect to the Internet because they exist at the "edge" of the Internet.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/network_edge.png">
      </div>
      <h3>Access Networks</h3>
      <p>The access network is the network where end systems connect to the first router in the path to the ISP. (There may be many routers in between the ISP and a home. The first router is the one that's actually in the home.) Basically, an access network is a network on the edge of the Internet.</p>
      <h4>Home Access: What's the WiFi Password?</h4>
      <p>The home network will probably be the easiest for everyone to relate to. All our phones, laptops, tablets, computers, and everything else that connect to the WiFi in our homes form a home network.</p>
      <p>Two of the most popular ways to get Internet access are by a <b>digital subscriber line</b> (<b>DSL</b>) (think telephone lines and telephone companies) and cable.</p>
      <h4>Wide-Area Wireless Access: Unlimited Talk, Text, and Web For Only $ Per Month!</h4>
      <p>When our phones aren't connected to WiFi, it uses mobile data, which is Internet provided by cellular network providers through base stations (cell towers?). This is where the terms 3G, 4G, 5G, and LTE come into play. (6G doesn't exist yet at the time of this writing.)</p>
      <h2 id="networkcore">The Network Core</h2>
      <p>The network core is, well, the part of the Internet that is not at the edge. This is the part of the Internet that our router connects to â€” out to where our Internet access comes from.</p>
      <h3>Packet Switching</h3>
      <p>The data that the end systems send and receive are called <b>messages</b>. Messages can be any type of data, such as emails, pictures, or audio files. When going from its source to its destination, messages are broken up into packets, which are transferred from packet switch to packet switch. From packet switch to packet switch, each packet is broken up into bits and sent across the communication link connecting the two packet switches.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/packet_switching.png">
      </div>
      <h4>No Packet Left Behind</h4>
      <p>Most packet switches use <b>store-and-forward transmission</b>. This means that the packet switch waits until it receives the entire packet before it starts sending the first bit of the packet to the next location.</p>
      <div class="ln-box">
        <p>Consider a simple network of two end systems and one packet switch. The source has three packets to send to the destination.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/packet_switching_2.png">
        </div>
        <p>Let's say each packet has `L` bits and that each communication link can transfer `R` bits per second. Then the total time it takes to transfer one packet across one communication link is `L/R` seconds.</p>
        <p>So the time it takes to transfer the packet from source to destination is `2L/R`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/packet_switching_3.png">
        </div>
        <p>How long does it take to transfer all three packets from source to destination? At `L/R` seconds, the packet switch receives all of the first packet. At this time, while the packet switch starts sending bits of the first packet to the destination, it also starts receiving bits of the second packet from the source.</p>
        <p>At `2L/R` seconds, the first packet arrives at the destination and the second packet arrives at the packet switch.</p>
        <p>At `3L/R` seconds, the second packet arrives at the destination and the third packet arrives at the packet switch.</p>
        <p>At `4L/R` seconds, the third packet arrives at the destination.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/packet_switching.gif">
        </div>
        <p>Let's generalize this for `N` communication links and `P` packets.</p>
        <p>Since the time it takes for one packet to travel across one communication link is `L/R`, it takes one packet `NL/R` time to travel across `N` communication links.</p>
        <p>At time `NL/R`, the first packet will reach the destination. If there are `P` packets in total, then there will be `P-1` more times where there is a packet sitting in the last packet switch (i.e., there will be `P-1` more packets passing through the last packet switch). Since it takes `L/R` time to go from the last packet switch to the destination, it will take `(P-1)L/R` time for all the packets to reach the destination. So the total time it takes `P` packets to travel across `N` communication links is `NL/R+(P-1)L/R=(N+P-1)L/R`.</p>
      </div>
      <div class="ln-box">
        <p>Besides store-and-forward, there's also cut-through switching, in which the switch starts sending bits before the whole packet is received.</p>
      </div>
      <h4>Queuing Delays</h4>
      <p>Realistically, a packet switch has more than two links. For each link in the packet switch, there is an <b>output buffer</b> (a.k.a. <b>output queue</b>) where the packets wait if the link is busy. This can happen if the transmission rate of the link is slower than the arrival rate of the packets, i.e., the packets are coming in faster than the packet switch can send them out.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/output_buffer.png">
      </div>
      <h4>Okay, Some Packets Left Behind</h4>
      <p>Since the buffer can only be so big, if there isn't enough room for more incoming packets, some packets will be dropped, resulting in <b>packet loss</b>. Depending on the implementation, either the arriving packet or one of the packets in the buffer is dropped.</p>
      <h4>Forwarding Tables</h4>
      <p>Again, packet switches usually have multiple links. So how does the packet switch know which link to use to send the packet? The answer is that each packet switch has a <b>forwarding table</b> that lists the destination of each link. When a packet arrives, the packet switch examines the packet to see its destination and searches it up in its forwarding table. The table tells the packet switch which link to use to get to the destination.</p>
      <h3>Circuit Switching</h3>
      <p>With packet switching, the packets are sent and if there is a lot of traffic, then oh well. Maybe the data will take a long time to send, or worse, get dropped. To avoid this, we can reserve the resources ahead of time.</p>
      <p>With circuit switching, each link has a number of circuits. One circuit per link is required for communication between two end systems. So a link with, for example, four circuits can support four different connections at the same time. However, each connection only gets a fraction of the link's transmission rate.</p>
      <div class="ln-box">
        <p>Suppose we have a link with four circuits and a transmission rate of `1` Mbps. Then each circuit can only transfer data at a rate of `1/4` Mbps, which is `250` kbps.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/circuit_switching.png">
      </div>
      <h4>Frequency-Division Multiplexing</h4>
      <p>Multiplexing is sending multiple signals as one big signal.</p>
      <p>Multiple end systems can all use one link to transfer data if there are enough circuits. Each end system will be transferring different data, but they all get transferred over as "one big data" through the link. The way to differentiate which data came from which end system is to divide the link into different frequencies, like the example above.</p>
      <div class="ln-box">
        <p>Radio uses FDM. We can tune in to a specific frequency to listen to the station we want.</p>
      </div>
      <h4>Time-Division Multiplexing</h4>
      <p>With time-division multiplexing, the end systems get to use all of the frequency to transfer data, but only for a limited amount of time.</p>
      <div class="ln-box">
        <p>Suppose there are two end systems (`A` and `B`) communicating with two other end systems using the same link. Let's say they're only allowed `1`-second intervals to send their data. So from `0` to `1` seconds, part of the data from `A` and part of the data from `B` gets sent. From `1` to `2` seconds, part of the data from `A` and part of the data from `B` gets sent. And this repeats for every second.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/tdm.png">
        </div>
      </div>
      <div class="ln-box">
        <p>More formally, time is divided into frames, and each frame is divided into slots. Once a connection between two end systems is established, one slot in every frame is reserved just for that connection.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/tdm_2.png">
        </div>
      </div>
      <div class="ln-box">
        <p>Suppose that all links in a network use TDM with `24` slots and can transfer data at a rate of `1.536` Mbps. Then each circuit has a transmission rate of `1.536/24=64` kbps.</p>
      </div>
      <h4>Switching Teams</h4>
      <p>Packet switching is not good for things that require a continuous connection, like video calls. However, packet switching is, in general, more efficient than circuit switching.</p>
      <div class="ln-box">
        <p>Suppose several users share a `1` Mbps link. But each user is not using the connection all of the time, i.e., there are periods of activity and inactivity. Let's say that a user is actively using the connection 10% of the time, and generates data at `100` kbps when they are active. With circuit switching, this means the link can support `(1 text( Mbps))/(100 text( kbps))=10` users at once. The circuit must be reserved for all `10` users as long as they are connected, even if they're not using it all the time.</p>
        <p>With packet switching, resources are not reserved, so any number of users can use the link. If there are `35` users, the probability that `ge 11` users are actively using the connection at the same time is ~`0.0004`, which means there is a ~`0.9996` chance that there are `le 10` simultaneous users at any time. As long as there are not more than `10` users at a time, there will be no queuing delay (if there are more than `10` users, there will be more data than the `1` Mbps link can handle). So packet switching performs just as well as circuit switching while allowing for more users at the same time.</p>
      </div>
      <div class="ln-box">
        <p>Sticking with the same `1` Mbps link, suppose there are `10` users. One of the users suddenly generates `1,000,000` bits of data, but the other users are inactive. Suppose the link has `10` slots per frame. If all `10` slots are utilized, then the link can send `1,000,000` bits per second (`= 1` Mbps). However, under the rules of circuit switching, only one of the slots per frame will be used for that user. So the link will only be able to transfer `(1,000,000)/10=100,000` bits per second. So it will take `(1,000,000)/(100,000)=10` seconds to transfer all the data.</p>
        <p>With packet switching, since no other users are active, the one user gets to send all `1,000,000` bits of data in `1` second since there will be no queuing delays.</p>
      </div>
      <h3>A Network of Networks</h3>
      <p>We get our Internet access from an ISP. Of course, all of us don't get Internet access from the same ISP. There are different ISPs out there providing Internet access to different networks. But then how do our end systems communicate with end systems? How do we communicate with servers and access websites from different parts of the world for example? The answer is that the ISPs themselves are connected with each other. But what exactly does that look like?</p>
      <p>One idea is that we connect every ISP with each other directly.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_1.png">
      </div>
      <p>The problem with this is that it is too costly, because there would be way too many links. In computer science terms, there would be `n^2` connections.</p>
      <p>To minimize the number of connections, another idea is that there is a global ISP and all the other ISPs connect to that global one.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_2.png">
      </div>
      <p>This, too, is costly. But to offset the cost, the global ISP would charge the other ISPs (we'll call them access ISPs from this point on) money to connect to the global ISP.</p>
      <p>But if one global ISP becomes profitable, naturally there will be other global ISPs wishing to be profitable too.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_3.png">
      </div>
      <p>This structure is good for the access ISPs because they can choose which global ISP they want to connect to by comparing prices and service quality. So what we have so far is a two-tier hierarchy where global ISPs are at the top and access ISPs are at the bottom.</p>
      <p>The reality is that these global ISPs can't exist in every city in the world. What happens instead is that the access ISPs connect to a <b>regional ISP</b>, which then connects to a global ISP. (We'll now be calling global ISPs by the more correct term <b>tier-1 ISP</b>.) Some examples of tier-1 ISPs are AT&amp;T, Verizon, and T-Mobile.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_4.png">
      </div>
      <div class="ln-box">
        <p>Sometimes, an access ISP can connect directly to a tier-1 ISP. In that case, the access ISP would pay the tier-1 ISP.</p>
      </div>
      <div class="ln-box">
        <p>In some regions, there can be a larger regional ISP consisting of smaller regional ISPs.</p>
      </div>
      <div class="ln-box">
        <p>Access ISPs and regional ISPs can choose to <b>multi-home</b>. That is, they can connect to more than one provider ISP at the same time, getting Internet access from multiple ISPs. While they have to pay each ISP they're connected to, the multi-homed ISPs can achieve better reliability in case one of the provider ISPs has a failure.</p>
      </div>
      <p>This multi-tier hierarchy is closer to the structure of today's Internet, but there are still a few pieces missing.</p>
      <p>Lower-tier ISPs pay higher-tier ISPs based on how much traffic goes through their connection. To avoid sending traffic to the higher-tier ISPs, the lower-tier ISPs can <b>peer</b>. This means that they connect with each other so that the traffic goes between them instead of up to the higher-tier ISP.</p>
      <p>For example, suppose that a computer connected to the green ISP wants to communicate with a computer connected to the blue ISP. Without peering, the traffic would have to go all the way up to the tier-1 ISP and back down from there.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_5.png">
      </div>
      <p>But with peering, they can avoid the cost of going through the regional and tier-1 ISPs by connecting with each other directly. Usually when ISPs peer with each other, they agree to not pay each other for the traffic that comes from peering.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_6.png">
      </div>
      <p>To facilitate this peering, third-party companies can create an <b>Internet Exchange Point</b> (<b>IXP</b>) which ISPs connect to to peer with other ISPs.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_7.png">
      </div>
      <p>There's one more piece: <b>content-provider networks</b>. Content providers are companies like Google, Amazon, and Microsoft.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/isp_structure_8.png">
      </div>
      <p>They run their own private networks so that they can bring their services and content closer to users. They do this by having data centers distributed everywhere across the world. Each data center has tons of servers that are all connected to each other.</p>
      <p>And this is why the Internet is a network of networks. (Recall that ISPs themselves are networks of packet switches and links. There's also the edge network from the previous section, and each network in the edge network is a network of routers and links.)</p>
      <h2 id="delay">Delay, Loss, and Throughput</h2>
      <p>In a (developer's) perfect world, data would travel through the Internet instantaneously without any limitations. Well, we know things aren't perfect.</p>
      <h3>What's the Holdup?</h3>
      <p>As packets travel along their route, they experience <b>nodal processing delay</b>, <b>queuing delay</b>, <b>transmission delay</b>, and <b>propagation delay</b>. All these delays combined is called <b>total nodal delay</b>.</p>
      <h4>Processing Delay</h4>
      <p>When a packet arrives at a router, the router looks at the packet's header to see which link it should go on. It also checks for any bit-level errors in the packet. The time it takes to do this (microseconds), is the <b>processing delay</b>.</p>
      <h4>Queuing Delay</h4>
      <p>Once the outbound link has been determined, the packet goes to the queue for that link. The time it waits in the queue (microseconds to milliseconds), is the <b>queuing delay</b>.</p>
      <h4>Transmission Delay</h4>
      <p>Once the packet reaches the front of the queue, it is ready to be transferred across the link. Recall that packets travel through links as bits. The time it takes to push all of the bits into the link (microseconds to milliseconds) is the <b>transmission delay</b>. If a packet has `L` bits and the link can transfer `R` bits per second, then the transmission delay is `L/R`.</p>
      <h4>Propagation Delay</h4>
      <p>Now that the bits are on the link, they must get to the other end. The time it takes to get from the beginning of the link to the end of the link (milliseconds) is the <b>propagation delay</b>. This is determined by two things: how fast the bits are able to move and how far they have to go. How fast the bits can move depends on the physical characteristics of the link (e.g., whether it's fiber optics or twisted-pair copper wire) and is somewhere near the speed of light.</p>
      <p>If `d` is the distance the bit has to travel, i.e., the length of the link, and `s` is the propagation speed of the link, then `d/s` is the propagation delay.</p>
      <div class="ln-box">
        <p>Transmission delay and propagation delay sound very similar to each other. So here are some pictures illustrating the difference.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/transmission_delay.png">
        </div>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/propagation_delay.png">
        </div>
        <p>Yeah, somehow the bits go from being on a conveyer belt to being pushed in needlessly large carts. There's no delay from belt to cart though. That part happens magically.</p>
      </div>
      <div class="ln-box">
        <p>Consider a packet of length `L` which begins at end system `A` and travels over three links to a destination end system. These three links are connected by two packet switches. For `i=1,2,3`, let `d_i`, be the length of each link, `s_i` be the propagation speed of each link, and `R_i` be the transmission rate of each link. The packet switch delays each packet by `d_text(proc)`. Assume there are no queuing delays.</p>
        <p>In terms of `d_i`, `s_i`, `R_i`, and `L`, what is the total end-to-end delay for the packet?</p>
        <div class="collapse ln-box" id="delay_q1">
          <p>The total end-to-end delay is the sum of all the delays: processing delay, queuing delay, transmission delay, and propagation delay.</p>
          <p>The processing delay is given to us as `d_text(proc)`. There are two packet switches, so there will be two processing delays.</p>
          <p>We are also given that there are no queuing delays.</p>
          <p>The transmission delay is `L/R_1+L/R_2+L/R_3`.</p>
          <p>The propagation delay is `d_1/s_1+d_2/s_2+d_3/s_3`.</p>
          <p>So the total end-to-end delay is `d_text(proc)+d_text(proc)+L/R_1+L/R_2+L/R_3+d_1/s_1+d_2/s_2+d_3/s_3`.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#delay_q1" aria-expanded="false" aria-controls="delay_q1">Answer</button>
        <p>Suppose now the packet is `1500` bytes, the propagation speed on all three links is `2.5*10^8` m/s, the transmission rates of all three links are `2` Mbps, the packet switch processing delay is `3` ms, the length of the first link is `5000` km, the length of the second link is `4000` km, and the length of the third link is `1000` km. What is the end-to-end delay?</p>
        <div class="collapse ln-box" id="delay_q2">
          <p>From the first question, we know that the end-to-end delay is `d_text(proc)+d_text(proc)+L/R_1+L/R_2+L/R_3+d_1/s_1+d_2/s_2+d_3/s_3`, so now we'll just calculate and plug in those values.</p>
          <p>We're given that `d_text(proc)=3` ms.</p>
          <p>To calculate transmission delay, we'll first convert everything to bits. There are `8` bits in a byte, so the packet length is `1500*8=12,000` bits. For the transmission rate, `2` Mbps = `2,000,000` bits per second. Since all three links have the same transmission rate, `R_1=R_2=R_3`. So `L/R_1=L/R_2=L/R_3=(12,000)/(2,000,000)=0.006` seconds `=6` ms.</p>
          <p>For the propagation delay, we'll first convert everything to meters. The lengths of the links are `5,000,000` m, `4,000,000` m, and `1,000,000` m. Since all three links have the same propagation speed, `s_1=s_2=s_3`. So `d_1/s_1=(5,000,000)/(250,000,000)=0.02` seconds `=20` ms, `d_2/s_2=(4,000,000)/(250,000,000)=0.016` seconds `=16` ms, and `d_3/s_3=(1,000,000)/(250,000,000)=0.004` seconds `=4` ms.</p>
          <p>So the total-end-to-end delay is `3+3+6+6+6+20+16+4=64` ms.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#delay_q2" aria-expanded="false" aria-controls="delay_q2">Answer</button>
      </div>
      <div class="ln-box">
        <p>Consider a highway that has a tollbooth every `100` km. It takes `12` seconds for a tollbooth to service a car. Suppose a caravan of `10` cars are on the highway with each car traveling `100` km/hour. When going from tollbooth to tollbooth, the first car waits at the tollbooth until the the other nine cars arrive. To analogize, the tollbooth is a router, the highway is a link, the cars are bits, and the caravan is a packet.</p>
        <p>How long does it take the caravan to travel from one tollbooth to the next?</p>
        <div class="collapse ln-box" id="caravan_q1">
          <p>Since the tollbooth takes `12` seconds to service a car and there are `10` cars, it takes a total of `120` seconds (or `2` minutes) to service all the cars.</p>
          <p>Since the distance between every tollbooth is `100` km and the cars are moving at `100` km/hour, it takes `1` hour (or `60` minutes) to reach the next tollbooth.</p>
          <p>Since the cars in the caravan travel together, everything depends on the last car. So we'll look at this from the last car's point of view.</p>
          <p>The last car has to wait `2` minutes before it can be serviced by the tollbooth. Once this is done, it takes `60` minutes for it to reach the next tollbooth. So it takes the last car `62` minutes to reach the next tollbooth.</p>
          <p>Here's some animation to visualize:</p>
          <div class="ln-center">
            <img class="img-fluid ln-image-small" src="../pictures/networking/caravan.gif">
          </div>
          <p>At `120` seconds all `10` cars have exited the first tollbooth and are on the highway to the next tollbooth.</p>
          <p>The first car will take `60` minutes to enter the next tollbooth (get off the animation line), but since it waited `12` seconds for the previous tollbooth servicing, it will enter the next tollbooth at `60:12`.</p>
          <div class="ln-center">
            <img class="img-fluid ln-image-small" src="../pictures/networking/caravan_2.gif">
          </div>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#caravan_q1" aria-expanded="false" aria-controls="caravan_q1">Answer</button>
        <p>To analogize some more, the time it takes the tollbooth to service a car is the transmission delay. The time it takes the car to move to the next tollbooth is the propagation delay.</p>
        <p>Now suppose that the cars travel at `1000` km/hour and the tollbooth takes `1` minute to service a car. Will cars arrive at the second tollbooth before all the cars are serviced at the first tollbooth? That is, will there still be cars at the first tollbooth when the first car arrives at the second tollbooth?</p>
        <div class="collapse ln-box" id="caravan_q2">
          <p>Now it takes `10` minutes to service all `10` cars.</p>
          <p>The first car will wait `1` minute for the tollbooth to service it. Then it will take `6` minutes (`(100text(km))/((1000text(km))/(60text(min)))=6`) to reach the next tollbooth. So it will take a total of `7` minutes for the first car to go from one tollbooth to the next.</p>
          <p>But in `7` minutes, the first tollbooth will have only serviced `7` cars. So there will be `3` cars still at the first tollbooth when the first car arrives at the second tollbooth.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#caravan_q2" aria-expanded="false" aria-controls="caravan_q2">Answer</button>
      </div>
      <h3>Queuing Delay and Packet Loss</h3>
      <p>Queuing delay is the most complicated delay since it varies from packet to packet (as implied, processing delay, transmission delay, and propagation delay are the same for every packet). For example, if 10 packets arrive at an empty queue, the first packet will have no queuing delay, but the last packet will have to wait for the first 9 packets to leave the queue.</p>
      <p>The factors that determine queuing delay are the rate at which traffic arrives, the transmission rate of the link, and the type of traffic that arrives (e.g., periodically or in bursts).</p>
      <div class="ln-box">
        <p>Let `a` be the average rate (in packets per second) at which packets arrive. If a packet has `L` bits, then `La` is the average rate (in bits per second) at which bits arrive. `R` is the transmission rate of the link. So `(La)/R` is the ratio of bits coming in vs bits going out; this ratio is called <b>traffic intensity</b>.</p>
        <p>`(La)/R gt 1` means that bits are arriving at the queue faster than they are leaving. In this case, the queuing delay will approach infinity.</p>
        <p>If `(La)/R le 1`, then the type of traffic that arrives has an effect on the queuing delay. For example, if one packet arrives every `L/R` seconds, then the packet will always arrive at an empty queue, so there will be no queuing delay.</p>
        <p>However, if `N` packets arrive in bursts every `NL/R` seconds, then the first packet will experience no queuing delay. But the second packet will experience a queuing delay of `L/R` seconds. The third packet will experience a queuing delay of `2L/R` seconds. Generally, each packet will have a queuing delay of `(n-1)L/R` seconds.</p>
      </div>
      <div class="ln-box">
        <p>Average queuing delay grows exponentially with traffic intensity, i.e., a small increase in traffic intensity results in a large increase in average queuing delay.</p>
      </div>
      <h4>Packet Loss</h4>
      <p>If packets are coming in faster than they are leaving the queue, then the queue will eventually be full and not be able to store any more packets. In this case, some packets will be <b>dropped</b> or <b>lost</b>. As traffic intensity increases, so does the size of the queue, which means the number of packets lost also increases.</p>
      <h3>End-to-End Delay</h3>
      <p>End-to-end delay is the total delay that a packet experiences from source to destination.</p>
      <div class="ln-box">
        <p>Suppose there are `N-1` routers between the source and destination. Then there are `N` links. Also suppose that the network is uncongested so there are practically no queuing delays.</p>
        <p>Then the end-to-end delay is `d_text(end-end)=Nd_text(proc)+Nd_text(trans)+Nd_text(prop)=N(d_text(proc)+d_text(trans)+d_text(prop))`.</p>
      </div>
      <h4>Traceroute</h4>
      <p>Traceroute is a program that allows us to see how packets move and how long they take. We specify the destination we want to send some packets to and the program will print out a list of all the routers the packets went through and how long it took to reach that router.</p>
      <div class="ln-box">
        <p>Suppose there are `N-1` routers between the source and destination. Traceroute will send `N` special packets into the network, one for each router and one for the destination. When the nth router receives the nth packet, it sends a message back to the source. When it receives a message, the source records the time it took to receive the message and where the message came from. Traceroute performs these steps 3 times.</p>
      </div>
      <h3>Throughput</h3>
      <p>Throughput is the rate at which bits are transferred between the source and destination. The <b>instantaneous throughput</b> is the rate at any point during the transfer. The <b>average throughput</b> is the rate over a period of time.</p>
      <div class="ln-box">
        <p>If a file has `F` bits and the transfer takes `T` seconds, then the average throughput of the file transfer is `F/T` bits per second.</p>
      </div>
      <p>Suppose we have two end systems connected by two links and one router. Let `R_s`, `R_c` be the rates of the two links.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/throughput.png">
      </div>
      <p>Suppose `F=32,000,000` bits, `R_s=2` Mbps, and `R_c=1` Mbps.</p>
      <ul>
        <li>After `1` second, `2,000,000` bits will move from the server to the router.</li>
        <li>After `2` seconds, `2,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `3` seconds, `2,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `4` seconds, `2,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
      </ul>
      <p>The file transfer is complete when the client receives `32,000,000` bits, which means `1,000,000` bits will have to move from the router to the client `32` times. Since it takes `1` second to do so, it will take `32` seconds for the file transfer to complete.</p>
      <p>Notice how it didn't matter how fast the bits moved from the server to the router. The time the file took to reach the client only depended on the rate from the router to the client. This doesn't mean the time it takes to reach the client is always equal to the rate from router to client though. Let's flip the numbers around to prove this point.</p>
      <p>Suppose `F=32,000,000` bits, `R_s=1` Mbps, and `R_c=2` Mbps.</p>
      <ul>
        <li>After `1` second, `1,000,000` bits will move from the server to the router.</li>
        <li>After `2` seconds, `1,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `3` seconds, `1,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
        <li>After `4` seconds, `1,000,000` bits will move from the server to the router and `1,000,000` bits will move from the router to the client.</li>
      </ul>
      <p>Wait, but isn't the rate from router to client `2,000,000` bits per second? Why aren't `2,000,000` bits moving from router to client every second? That's because there are only `1,000,000` bits entering the router every second.</p>
      <p>From this, we can see that the throughput of a file transfer depends on the link that is transferring bits at the lowest rate. This link is called the <b>bottleneck link</b>.</p>
      <div class="ln-box">
        <p>Mathematically, the throughput is `min(R_s,R_c)`.</p>
        <p>The time it takes to transfer the file is `F/(min(R_s,R_c))`.</p>
      </div>
      <div class="ln-box">
        <p>Generally, the throughput for a network with `N` links is `min(R_1,R_2,...,R_n)`.</p>
      </div>
      <p>The bottleneck link is not always the link with the lowest transmission rate. We'll look at another example to show this.</p>
      <p>Suppose there are `10` clients downloading data from `10` servers (each client is connected to a unique server). Suppose they all share a common link with transmission rate `R` somewhere in the network core. We'll denote the transmission rates of the server links as `R_s` and the transmission rates of the client links as `R_c`.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/throughput_2.png">
        <p>(I think sharing a common link means all the server links eventually connect to one router and all the client links eventually connect to a different router. And the link between these two routers are the same.)</p>
      </div>
      <p>Let's assign actual values to these transmission rates. So let `R_s=2` Mbps, `R_c=1` Mbps, and `R=5` Mbps. Since the common link is being used for `10` simultaneous downloads, the link divides its transmission rate equally among the `10` connections. So now the common link is transferring bits at a rate of `(5text(Mbps))/10=500` kbps. Which means the throughput for each download is `500` kbps. Even though the common link had the highest transmission rate, it ended up being the bottleneck link.</p>
      <div class="ln-box">
        <p>Links in the network core are almost never the bottleneck link because they are usually over-provisioned with high speed links. Typically, the bottleneck links are in the access network.</p>
      </div>
      <h2 id="protocollayers">Protocol Layers</h2>
      <p>There are many pieces to the Internet, like routers, links, applications ("apps"), protocols, and end systems. These pieces are organized into layers.</p>
      <h3>Layered Architecture: The Internet Is Like an Onion</h3>
      <p>The airline system can be seen as an organization of layers.</p>
      <table class="table">
        <tr>
          <th>Layer</th>
          <th>Action (Leaving)</th>
          <th>Action (Arriving)</th>
        </tr>
        <tr>
          <td>Ticketing</td>
          <td>Purchase</td>
          <td>Complain</td>
        </tr>
        <tr>
          <td>Baggage</td>
          <td>Check</td>
          <td>Claim</td>
        </tr>
        <tr>
          <td>Gates</td>
          <td>Enter</td>
          <td>Exit</td>
        </tr>
        <tr>
          <td>Runway</td>
          <td>Takeoff</td>
          <td>Land</td>
        </tr>
        <tr>
          <td>Airplane</td>
          <td>Routing</td>
          <td>Routing</td>
        </tr>
      </table>
      <p>Every time we board and leave an airplane, we perform the same steps in a certain order. And the action we perform depends on the layer we're at since each layer is responsible for one thing and one thing only. (I.e., Each layer provides a different service.) Also, each layer depends on the layer before it. For example, we can only check in our baggage if we have a ticket, or we can only enter the gate once we have checked in our baggage.</p>
      <p>This layering makes it easier to distinguish the different components of an airline system. It also makes it easier to change the implementation of a layer's service without affecting the rest of the layers. For example, the current implementation of the gate layer is to board people by ticket priority and check-in time. But the implementation can be changed to board people by height. This change doesn't affect any of the other layers, and the function of this layer remains the same: getting people on the airplane.</p>
      <h4>Protocol Layering</h4>
      <p>The Internet is made up of 5 layers.</p>
      <table class="table">
        <tr>
          <th>Layer</th>
        </tr>
        <tr>
          <td>5. Application</td>
        </tr>
        <tr>
          <td>4. Transport</td>
        </tr>
        <tr>
          <td>3. Network</td>
        </tr>
        <tr>
          <td>2. Link</td>
        </tr>
        <tr>
          <td>1. Physical</td>
        </tr>
      </table>
      <p>Recall that a protocol is a set of rules that define how things should work. Each protocol belongs to one of the layers.</p>
      <h4>Application Layer</h4>
      <p>The application layer includes all the network applications (basically apps or programs that connect to the Internet).</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
          <th>Details</th>
        </tr>
        <tr>
          <td>HTTP</td>
          <td>sending and receiving Web documents</td>
        </tr>
        <tr>
          <td>SMTP</td>
          <td>sending and receiving emails</td>
        </tr>
        <tr>
          <td>FTP</td>
          <td>sending and receiving files</td>
        </tr>
      </table>
      <p>Applications communicate with each other by sending <b>messages</b> to each other.</p>
      <h4>Transport Layer</h4>
      <p>The transport layer is responsible for delivering the application's message to the network core.</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
          <th>Details</th>
        </tr>
        <tr>
          <td>TCP</td>
          <td>
            <p>connection oriented</p>
            <p>guaranteed delivery</p>
          </td>
        </tr>
        <tr>
          <td>UDP</td>
          <td>
            <p>connectionless</p>
            <p>no reliability, no flow control, no congestion control</p>
          </td>
        </tr>
      </table>
      <p>In the transport layer, the packets of information are called <b>segments</b>.</p>
      <h4>Network Layer</h4>
      <p>The network layer is responsible for moving the transport layer's segments from router to router.</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
          <th>Details</th>
        </tr>
        <tr>
          <td>IP</td>
          <td>
            <p>defines how packets must be formatted</p>
            <p>defines the actions that must be taken when sending or receiving packets</p>
          </td>
        </tr>
      </table>
      <p>In the network layer, the packets are called <b>datagrams</b>.</p>
      <h4>Link Layer</h4>
      <p>The link layer is responsible for moving the network layer's datagrams across links.</p>
      <table class="table">
        <tr>
          <th>Protocol</th>
        </tr>
        <tr>
          <td>Ethernet</td>
        </tr>
        <tr>
          <td>WiFi</td>
        </tr>
        <tr>
          <td>DOCSIS</td>
        </tr>
      </table>
      <p>In the link layer, the packets are called <b>frames</b>.</p>
      <h4>Physical Layer</h4>
      <p>The physical layer is responsible for moving the individual bits of the link layer's frames across links.</p>
      <div class="ln-box">
        <h4>OSI Model</h4>
        <p>In the late 1970s (before the Internet became public), there was another model that was proposed by the International Organization for Standardization (ISO) on how to organize computer networks. This model was called the Open Systems Interconnection (OSI) model and it had 7 layers:</p>
        <table class="table">
        <tr>
          <th>Layer</th>
        </tr>
        <tr>
          <td>7. Application</td>
        </tr>
        <tr>
          <td>6. Presentation</td>
        </tr>
        <tr>
          <td>5. Session</td>
        </tr>
        <tr>
          <td>4. Transport</td>
        </tr>
        <tr>
          <td>3. Network</td>
        </tr>
        <tr>
          <td>2. Link</td>
        </tr>
        <tr>
          <td>1. Physical</td>
        </tr>
      </table>
      <p>The layers that also appear in the Internet model work roughly the same way. So we'll just go over the ones that are new.</p>
      <p>The presentation layer makes sure the data is readable and usable. It provides data compression, data encryption, and data description. I guess it's sorta like a translator.</p>
      <p>The session layer makes sure the data goes where it needs to go. It provides data synchronization, checkpointing, and data recovery.</p>
      </div>
      <h3>Encapsulation</h3>
      <p>Whenever we moved across the different layers, we called the packets of information by a different name. That is because the packets change as they move from layer to layer.</p>
      <ol>
        <li>Application layer sends message</li>
        <li>Transport layer adds transport-layer header information to the message, forming a segment</li>
        <li>Network layer adds network-layer header information to the segment, forming a datagram</li>
        <li>Link layer adds link-layer header information to the datagram, forming a frame</li>
      </ol>
      <p>The header information is needed so that the thing (e.g., router, link, end system) receiving the packet knows what it is and what to do with it. Here are some animations to illustrate:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/encapsulation.gif">
      </div>
      <p>End systems implement all 5 layers while routers usually implement only the bottom 3 layers (network, link, physical).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/encapsulation_2.gif">
      </div>
      <p>Notice that when the packet left the end system, the last header added was the link-layer header, which is the first thing that gets processed when entering the router. When the packet reaches the link layer, the link-layer headers are extracted away. So now the top-most header is the network-layer header, which is what the network layer then processes.</p>
      <p>After going through the network layer, the router determines that the packet has to go on another link again. So now the packet has to travel down the layers. When it goes from the network layer down to the link layer, a new link-layer header is added, so that the next thing (router or end system) can process it.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/encapsulation_3.gif">
      </div>
      <p>Now that the packet has reached the end system, it goes up the layers one last time. Again, at each layer the headers are removed until only the message remains, which the application understands.</p>
      <hr>
      <p>From this point on, we'll go into more detail for each layer, starting with the application layer.</p>
      <hr>
      <h2 id="networkapplications">Principles of Network Applications</h2>
      <p>A network application is a set of apps or programs that connect to and communicate with the Internet. These apps are made by our local software developers using languages like C, Java, or Python.</p>
      <p>Typically, network applications involve multiple programs running on different end systems. For example, we can download Spotify on our phone, tablet, computer, or watch. And Spotify will have a different program running on their servers to provide functionality to the app.</p>
      <h3>Network Application Architectures</h3>
      <p>There are 2 types of network application architectures: client-server and peer-to-peer (P2P).</p>
      <p>In the client-server architecture, there is a server that is always on and running. Whenever clients want data, they make a request to the server, which then provides the clients with what they requested. In addition to always being on, the server has a fixed IP address, so clients can make requests to the server at any time.</p>
      <p>Big companies, like Google, need more than one server to handle all the requests, otherwise that one server would get overwhelmed. So they set up <b>data centers</b>, which house multiple servers. And then they go and set up multiple data centers.</p>
      <p>In peer-to-peer architecture, there are no servers (and thus no clients). Instead, devices communicate directly with other devices â€” called peers â€” to transfer data.</p>
      <p>Let's say you wanted to download a large file (perhaps an online textbook ðŸ¤«). You could go online and download it (this would be client-server), but let's say the file is so large, that it takes too long. Suppose that I happen to have the complete file already downloaded. Using the right application, I could transfer my file directly to your computer. Suppose that someone else also has the file. Then you can receive the file from both of us. And the more people that have the file, the more people you can connect to to receive the file, making the download faster.</p>
      <p>The effectiveness of peer-to-peer relies on the number of peers. The more peers there are, the more computing power there is. This is referred to as <b>self-scalability</b>.</p>
      <h3>Processes Communicating</h3>
      <p>We call them programs. Operating systems call them <b>processes</b>.</p>
      <h4>Client and Server Processes</h4>
      <p>A network application consists of pairs of processes that send messages to each other. A Web broswer (a process) communicates with a Web server (another process). Spotify, the app (a process), communicates with Spotify's servers (another set of processes). The process that is requesting data is the <b>client</b> process and the process that is providing the data is the <b>server</b> process. Despite the naming, the terms "client" and "server" processes apply for both client-server architecture and peer-to-peer architecture.</p>
      <div class="ln-box">
        <p>More formally (and perhaps more accurately), client processes are the ones that initiate communication and server processes are the ones that wait to be contacted.</p>
      </div>
      <h4>The Interface Between the Process and the Computer Network</h4>
      <p>As we saw before, a process in the application layer sends a message to the transport layer. In order to do so, it opens a <b>socket</b>, which is the application layer's door to the transport layer. Sockets are also used to receive messages from the transport layer.</p>
      <div class="ln-box">
        <p>For us developers, sockets are also referred to as the <b>Application Programming Interface</b> (<b>API</b>) between the application and the network. We don't have control over how the socket is implemented; that's handled by the OS.</p>
      </div>
      <h4>Addressing Processes</h4>
      <p>In order for the message to go to the correct place, the sending process needs to specify where the message needs to be delivered to. First, it needs to specify which end system is receiving the message. Each end system has a unique identifier called an <b>IP address</b>. Knowing just the end system is not enough though because an end system is running multiple applications while it's on, and we need to know which application needs the message. Whenever processes open a socket, they must choose a number to "label" the socket. This number is called a <b>port number</b>.</p>
      <h3>Transport Services Available to Applications</h3>
      <p>On the other side of the socket is a transport-layer protocol that delivers the message to the socket of the receiving process. There are several transport-layer protocols an application can choose from.</p>
      <h4>Reliable Data Transfer</h4>
      <p>As we saw earlier, packets can get lost. For some situations, like transferring financial documents, packet loss is undesirable. But even though packets can get lost, a protocol can still guarantee that the file will be delivered correctly. Then that protocol provides <b>reliable data transfer</b>.</p>
      <p>In other situations, like video calls, some packet loss is acceptable. ("I'm sorry, can you repeat that? You cut out for a few seconds.") Those applications are referred to as <b>loss-tolerant applications</b>.</p>
      <h4>Throughput</h4>
      <p>Throughput is the rate at which bits are transferred between the source and destination (I feel like I've said these exact words before). A transport-layer protocol can guarantee a specified throughput of some amount of bits per second. This would be useful for voice calls, which need a certain throughput in order for the voices to be heard clearly. Applications that have throughput requirements are <b>bandwidth-sensitive applications</b>.</p>
      <p>Applications that don't are <b>elastic applications</b>. File transfers don't need a certain throughput. If more bits can be transferred, then the download will be faster. If fewer bits can be transferred, then the download will be slower.</p>
      <h4>Timing</h4>
      <p>Whereas throughput is the number of bits going through, timing is the delay between each bit. One timing guarantee might be that bits go to the receiving process every 100 milliseconds. For real-time applications, like video calls and online games, it is ideal to have low delay.</p>
      <h4>Security</h4>
      <p>Before transferring the data, a transport protocol can encrypt it.</p>
      <h3>Transport Services Provided by the Internet</h3>
      <p>There are 2 transport protocols an application can use: TCP and UDP.</p>
      <h4>TCP</h4>
      <p>TCP is a connection-oriented protocol. The client and server have to confirm a connection (<b>TCP connection</b>) with each other first before messages can be transferred. And once the application is done sending messages, it must close the connection.</p>
      <p>TCP also provides reliable data transfer. This means data is sent without error and in the proper order â€” there are no missing or duplicate bytes. Part of this is achieved by implementing congestion control. If the network is congested, the sending process will be throttled.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/tcp_comic.png">
      </div>
      <h4>UDP</h4>
      <p>UDP is like the opposite of TCP. It's connectionless. The data transfer is unreliable â€” there's no guarantee all the messages will arrive and if it does arrive, there's no guarantee that the messages will arrive in order. And there's no congestion control. All of these things sound bad, so why does UDP even exist in the first place?</p>
      <p>Well no congestion control means that UDP can send bits at any rate with no limits*. This is good for voice call applications because they want data to be sent as much as possible and as fast as possible.</p>
      <p>*However, there will be actual limitations to this rate, including link transmission rate and network congestion.</p>
      <div class="ln-box">
        <p>For some reason, most firewalls are configured to block most types of UDP traffic. So applications that use UDP have to be prepared to use TCP as a backup.</p>
      </div>
      <div class="ln-box">
        <table class="table">
          <tr>
            <th>Application</th>
            <th>Application-Layer Protocol</th>
            <th>Transport-Layer Protocol</th>
          </tr>
          <tr>
            <td>Email</td>
            <td>SMTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Remote terminal access</td>
            <td>SSH</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Web</td>
            <td>HTTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>File transfer</td>
            <td>FTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Streaming</td>
            <td>HTTP</td>
            <td>TCP</td>
          </tr>
          <tr>
            <td>Internet telephony</td>
            <td>SIP, RTP, etc.</td>
            <td>TCP or UDP</td>
          </tr>
        </table>
      </div>
      <div class="ln-box">
        <p>TCP and UDP both <em>don't</em> guarantee:</p>
        <ul>
          <li>throughput</li>
          <li>timing</li>
          <li>security*</li>
        </ul>
        <p>Despite this, time-sensitive applications still work. This is because they are designed with these lack of guarantees in mind.</p>
        <p>*TCP can be enchanced with <b>Transport Layer Security</b> (<b>TLS</b>) to provide encryption. TLS is not a protocol; it's an add-on to TCP.</p>
      </div>
      <h3>Application-Layer Protocols</h3>
      <p>Application-layer protocols define:</p>
      <ul>
        <li>the types of messages exchanged (e.g., request and response messages)</li>
        <li>the format of the messages
          <ul>
            <li>what fields can go in a message</li>
          </ul>
        </li>
        <li>the meaning of the information in the fields</li>
        <li>rules for determining when and how processes send and respond to messages</li>
      </ul>
      <p>Some protocols are defined in RFCs and are therefore public. While other protocols, like Skype's, are proprietary.</p>
      <h2 id="http">The Web and HTTP</h2>
      <p>The Internet became big in the 1990s with the arrival of the World Wide Web.</p>
      <h3>Some Dry Facts About HTTP</h3>
      <p>A <b>Web page</b> is a document with a bunch of <b>objects</b>, which can be images, videos, JavaScript files, CSS style sheets, etc. (More formally, an object is a file that is addressable by a single URL.) A Web page is usually an HTML file that includes references to these objects. <b>Web browsers</b> request and display Web objects while <b>Web servers</b> store Web objects.</p>
      <p>HTTP stands for <b>HyperText Transfer Protocol</b>, and it is the Web's application-layer protocol. When browsers request a Web page, they send HTTP request messages to the server and the server responds with HTTP response messages that contain the objects.</p>
      <p>HTTP uses TCP as its transport protocol. Port 80 is reserved for HTTP, so when browsers and servers initialize their sockets, they use port 80.</p>
      <p>HTTP is a <b>stateless protocol</b>. This means that the server does not store any information about the exchanges the server has with clients. For example, the server will not know if a particular client has asked for a Web page before or if it's the client's first time asking for the Web page.</p>
      <div class="ln-box">
        <p>The original version of HTTP is called HTTP/1.0. Then there was HTTP/1.1 (1997), HTTP/2 (2015), and HTTP/3 (2022).</p>
      </div>
      <h3>Non-Persistent and Persistent Connections</h3>
      <p>Each time a browser makes a request for a Web page, the browser actually has to make a request for the base HTML file and then each object in the file. In <b>non-persistent connections</b>, the browser opens a TCP connection for each object. In <b>persistent connections</b>, the browser opens one TCP connection and requests/receives all the objects using that one connection. HTTP uses persistent connections by default.</p>
      <h4>HTTP with Non-Persistent Connections</h4>
      <ol>
        <li>Browser starts a TCP connection to the server on port 80</li>
        <li>Browser sends an HTTP request message to the server</li>
        <li>Server receives the request, gets the base file, puts it in an HTTP response message, and sends the message</li>
        <li>Server closes the TCP connection</li>
        <li>Browser receives the response.</li>
      </ol>
      <p>And these steps repeat for each object in the Web page. So if a Web page has `10` objects, there will be `11` connections made (`1` for the Web page and `10` for each object). While this may seem tedious, browsers can be configured to open multiple parallel connections at the same time instead of waiting for one connection to be done before opening another.</p>
      <p>Let's look at one request in action:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/http.gif">
      </div>
      <p>As we can see, it takes some time for the client to receive the file after making a request.</p>
      <p>The <b>round-trip time</b> (<b>RTT</b>) is the time it takes for a packet to travel from client to server. The delays we saw in the "Delay, Loss, and Throughput" section also factor into the RTT.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/rtt.png">
      </div>
      <p>So the total response time is `2` RTTs plus the transmission time of the file. `1` RTT is for establishing the TCP connection and `1` RTT is for sending the file.</p>
      <p>Besides the delay for sending each object, there is also the burden of resource management. For each TCP connection, buffers and variables must be allocated; this can take up a lot of resources on the server if there are a bunch of clients.</p>
      <h4>HTTP with Persistent Connections</h4>
      <p>Instead of closing the TCP connection after sending the requested object, the server keeps the connection open for a certain period of time.</p>
      <div class="ln-box">
        <p>For both non-persistent and persistent connections, they can also be non-parallel or parallel. In non-parallel connections, the client waits until it receives an object before requesting another one. So each object requires `1` RTT. In parallel connections, the client requests an object as soon as it needs it, even if a previous request for an object has not been completed yet. This means if there are `n` objects, it could request for all `n` objects at once. So it could take as little as `1` RTT to get all the objects. (Note that the `1` RTT is just for the objects in the base file. The browser needs `1` RTT to get the base file itself first.)</p>
      </div>
      <div class="ln-box">
        <p>Suppose there is a `100`-kilobit page that contains `5` `100`-kilobit images, all of which are stored on a server. Suppose the round-trip time from the server to a client's browser is `200` milliseconds. For simplicity, we'll assume there is only one `100` Mbps link between the client and server; the time it takes to transmit a GET request onto the link is zero; and it takes some time to transmit things onto the link (well this last one technically doesn't need to be assumed). Note that based on the assumptions, the link has a `100` millisecond one-way propagation delay as well as a transmission delay.</p>
        <p>For non-persistent, non-parallel connections, how long is the response time? (The time from when the user requests the URL to the time when the page and its images are displayed?)</p>
        <div class="collapse ln-box" id="http_q1">
          <p>Let's start with the base file. It takes `1` RTT to establish the TCP connection and another `1` RTT to get the base file. So it takes a total of `2` RTTs for the base file. We are given that one RTT is `200` milliseconds, so it takes `200*2=400` milliseconds for the base file to propagate through the link.</p>
          <p>The transmission delay for the base file is `L/R=(100text( kb))/(100text( Mbps))=(100,000text( bits))/(100,000,000text( bits/s))=0.001` seconds `=1` millisecond.</p>
          <p>So the total time it takes to get the base file is `400+1=401` milliseconds.</p>
          <p>Now for the objects. Since it is non-persistent, the TCP connection for the base file will be closed and new ones (`1` each) will be needed for the objects. Since it is non-parallel, the previous TCP connection has to close before a new one opens.</p>
          <p>For one object, it takes `1` RTT to establish the TCP connection, and `1` RTT to get the object. So `2` RTTs for one object. For `5` objects, this is a total of `5*2=10` RTTs. In terms of milliseconds, this is `200*10=2000` milliseconds.</p>
          <p>The transmission delay for one object is the same for the base file. Then for `5` objects, the transmission delay is `1*5=5` milliseconds.</p>
          <p>So the total time it takes to get all of the objects is `2000+5=2005` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `2005+401=2406` milliseconds `=2.406` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q1" aria-expanded="false" aria-controls="http_q1">Answer</button>
        <p>What about for non-persistent, parallel connections?</p>
        <div class="collapse ln-box" id="http_q2">
          <p>Again, we'll start with the base file. It takes `1` RTT to establish the TCP connection and another `1` RTT to get the base file. So `2` RTTs. Same as before, `2*200=400` milliseconds for the base file to propagate.</p>
          <p>The transmission delay is also the same at `1` millisecond.</p>
          <p>So the total time it takes to get the base file is `400+1=401` milliseconds.</p>
          <p>For the objects, since the connections are parallel, all `5` objects will be retrieved at the same time in `1` RTT. Also, all `5` TCP connections will be opened parallelly in `1` RTT. So `2` RTTs for all `5` objects. This is also `2*200=400` milliseconds for them to propagate.</p>
          <p>Even though the connections are parallel, the objects are not transmitted onto the link in parallel. (There is only one link, so only one object can be transmitted at a time.) As we saw before, it takes `1` millisecond to transmit `1` object. So for `5` objects, `5` milliseconds.</p>
          <p>So the total time it takes to get all of the objects is `400+5=405` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `401+405=806` milliseconds `=0.806` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q2" aria-expanded="false" aria-controls="http_q2">Answer</button>
        <p>What about for persistent, non-parallel connections?</p>
        <div class="collapse ln-box" id="http_q3">
          <p>We open the TCP connection and wait for the base file to transmit and propagate, which takes `401` milliseconds.</p>
          <p>For the objects, since the connection is persistent, we don't need to open any more TCP connections.</p>
          <p>But since it is non-parallel, we need to get the objects one at a time. `1` RTT for each object means `5` RTTs for all objects, which means `5*200=1000` milliseconds for all objects to propagate.</p>
          <p>And `5` milliseconds to transmit.</p>
          <p>So the total time it takes to get all of the objects is `1000+5=1005` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `401+1005=1406` milliseconds `=1.406` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q3" aria-expanded="false" aria-controls="http_q3">Answer</button>
        <p>What about for persistent, parallel connections?</p>
        <div class="collapse ln-box" id="http_q4">
          <p>The base file takes `401` milliseconds to transmit and propagate.</p>
          <p>We don't need to open any more TCP connections, and all `5` objects will be retrieved at once in `1` RTT. So `200` milliseconds to propagate.</p>
          <p>`5` milliseconds to transmit.</p>
          <p>So the total time it takes to get all of the objects is `200+5=205` milliseconds.</p>
          <p>Bringing the total time for the whole Web page to `401+205=606` milliseconds `=0.606` seconds.</p>
        </div>
        <button class="btn" type="button" data-bs-toggle="collapse" data-bs-target="#http_q4" aria-expanded="false" aria-controls="http_q4">Answer</button>
      </div>
      <h3>HTTP/2</h3>
      <p>Opening multiple connections results in the server having to open and maintain multiple sockets. However, using only one TCP connection introduces a problem called <b>Head of Line</b> (<b>HOL</b>) <b>blocking</b>. Let's say the base file has a large video clip near the top and many small objects below the video. Being near the top, the video will be requested first. Especially if the bottleneck link is bad, the video will take some time to transfer, causing the objects to have to wait for the video. The video at the head of the line blocks the objects behind it. Parallel connections is one way to solve this problem.</p>
      <p>Standardized in 2015, HTTP/2 introduced a way to allow for using one TCP connection without experiencing HOL blocking.</p>
      <h4>HTTP/2 Framing</h4>
      <p>The solution is to break each object into small frames and interleave them. That way, each response will contain parts of each object.</p>
      <div class="ln-box">
        <p>Let's say the base file has `1` large video clip and `8` small objects. Suppose the video clip is broken up into `1000` frames and each of the small objects is broken up into `2` frames. The first response can contain `10` frames: `1` frame from the video and the first frame of each of the small objects. The second reponse can also contain `10` frames: `1` frame from the video and the last frame of each of the small objects. In just `18` frames, all of the small objects have been sent. If the frames weren't interleaved, the small objects would be sent after `1016` frames.</p>
        <p>(For some reason, interloven sounds right.)</p>
      </div>
      <h4>Server Pushing</h4>
      <p>The server can analyze the base file, see what objects there are, and send them to the client without needing to wait for the client to ask for them.</p>
      <h3>HTTP Message Format</h3>
      <p>There are two types of HTTP messages: request messages and response messages.</p>
      <h4>Request Messages: Give Me What I Want!</h4>
      <p>Here's an example of what a request message might look like:</p>
      <p><code>GET /projects/page.html HTTP/1.1<br>Host: www.myschool.edu<br>Connection: close<br>User-agent: Mozilla/5.0<br>Accept-language: fr</code></p>
      <p>The first line is the <b>request line</b>. There are three fields: the method field, the URL field, and the HTTP version field. As we can infer, the method is GET, which is the method for requesting objects (because, you know, we want to "get" it).</p>
      <p>The lines below the request line are the <b>header lines</b>. <code>Connection: close</code> means "close the connection after giving me what I requested", i.e., use a non-persistent connection. <code>Connection: keep-alive</code> is for a persistent connection. The user agent specifies what type of browser is making the request. And <code>Accept-language: fr</code> means get the French version if it exists.</p>
      <div class="ln-box">
        <p>It may look like the header line specifying the host is unnecessary, because the browser needed to know the host in order to establish the TCP connection in the first place. It turns out that it is needed by Web proxy caches, which we'll see later.</p>
      </div>
      <p>There is also an entity body that follows the header lines. It wasn't included in the example above because the entity body is usually empty for GET requests. The entity body is usually used for POST requests, which are used to submit information, like user inputs from a form. The information that needs to be submitted are included in the entity body.</p>
      <div class="ln-box">
        <p>This is a little bit unconventional, but GET requests can also be used to submit information. Instead of going in the entity body, the inputted data is included in the URL with a question mark, like google.com/search?q=what+is+http, where q is the name of the input and the stuff after the '=' is the value of the input.</p>
      </div>
      <h4>Response Messages: Here You Go, Now Leave Me Alone!</h4>
      <p>Here's an example of what a response message might look like:</p>
      <p><code>HTTP/1.1 200 OK<br>Connection: close<br>Date: Sat, 16 Sep 2023 19:23:09 GMT<br>Server: Apache/2.2.3 (CentOS)<br>Last-Modified: Sat, 16 Sep 2023 18:34:09 GMT<br>Content-Length: 6821<br>Content-Type: text/html<br><br>(the requested data...)</code></p>
      <p>The first line is the <b>status line</b>. There are three fields: the protocol version, the status code, and the status message.</p>
      <p>The lines below are the <b>header lines</b>. The <code>Date:</code> header line specifies when the HTTP response was created and sent by the server. The <code>Server:</code> header line specifies the type of the Web server. <code>Last-Modified:</code> is when the object was created or last modified. <code>Content-Length:</code> specifies the object's size in bytes.</p>
      <p>The <b>entity body</b> contains the actual object.</p>
      <h3>Cookies! ðŸª</h3>
      <p>HTTP is stateless, so the server doesn't store information about the requests that the clients are making. However, servers sometimes want a way to identify users so that they can restrict access or provide user-specific information when responding to the client. This is where cookies come in. Cookies allow sites to keep track of users.</p>
      <p>Let's say we go to our favorite shopping site on our favorite browser. The site creates a unique ID (let's say 9876) for this request, stores the ID in its database, and sends the ID back in the response. The response includes a <code>Set-cookie:</code> header; this tells the browser to create and store a cookie file. For example, the header will look like <code>Set-cookie: 9876</code>, so the browser will store 9876 in a file. Now, as we navigate through our shopping site, the site can ask the browser to include this cookie in every request. Then the site will query its database for this ID to identify the user. If the ID exists in the database, the site will know the user has visited before, and if the ID isn't in the database, the site will consider this a new user visiting for the first time.</p>
      <p>When we say that the site can identify the user, the site doesn't actually know who we are, like our name and stuff. Until we enter that information ourselves. When we enter our information to buy something or to fill out a form, the site can store that information and associate that with the ID in the cookie. So the next time we visit that site, it can look at the ID and the information linked to that ID.</p>
      <h3>Web Caching</h3>
      <p>A <b>Web cache</b> is a server that stores Web pages and objects that have been recently requested.</p>
      <p>When a browser requests an object, it sends the request to a Web cache. If the Web cache has a copy of the requested object, the Web cache sends it to the browser. If it doesn't, the Web cache requests it from the origin server, which sends the object to the Web cache. The Web cache stores a copy of it in its storage and sends that copy to the browser.</p>
      <p>Having a Web cache can significantly speed things up. If the server is physically located far away from the client or if the bottleneck link between the server and client is really bad, then using the Web cache reduces response times, assuming the Web cache is physically closer to the client or the bottleneck link between the Web cache and client is faster than the bottleneck link between the server and client.</p>
      <div class="ln-box">
        <p>Let's look at a hypothetical access network connected to the Internet. The router in the access network is connected to the router in the Internet by a `15` Mbps link. The links in the access network have a transmission rate of `100` Mbps. Suppose the average object size is `1` Mbits and that the average request rate is `15` requests per second. Also suppose that it takes `2` seconds on average for data to travel between the router in the Internet and the origin servers (we'll informally call this "Internet delay"). And that it takes `0.01` seconds on average for data to travel in the access network.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/networking/web_cache.png">
        </div>
        <p>Recall that traffic intensity is `(La)/R` where `La` is the average rate at which bits are travelling and `R` is the transmission rate of the link.</p>
        <p>On the access network, `15` requests per second and `1` Mbits per request means there are `15` Mbits travelling per second. So the traffic intensity is</p>
        <div class="ln-center">
          <p>`(La)/R=15/100=0.15`</p>
        </div>
        <p>which isn't too bad. In fact, the delay on the access link is negligible.</p>
        <p>On the access link, the traffic intensity is</p>
        <div class="ln-center">
          <p>`(La)/R=15/15=1`</p>
        </div>
        <p>which is really bad. The average response time could be in minutes.</p>
        <p>One solution is to upgrade the access link, but that's costly.</p>
        <p>Instead, let's consider placing a Web cache in the access network. Suppose that there is a `40%` chance that the cache can satisfy a request (i.e., the hit rate is `0.4`). This means only `60%` of the requests are going through the access link, so the traffic intensity is `1*0.6=0.6`.</p>
        <p>The average delay is the sum of the access network delay, access link delay, and Internet delay. The access network delay is `0.01` seconds; the access link delay is negligible with a traffic intensity of `lt 0.8`; and the Internet delay is `2` seconds. Since `40%` of the requests stay in the access network and `60%` of the request go to the Internet, the average delay is</p>
        <div class="ln-center">
          <p>`0.4*(0.01+0+0)+0.6*(0.01+0+2)=0.004+1.206=1.21` seconds</p>
        </div>
        <p>If we had upgraded the access link instead of using a Web cache, the average delay would be at least `2` seconds from the Internet delay alone. So we can see that using a Web cache provides faster response times.</p>
      </div>
      <h4>Conditional GET</h4>
      <p>The problem with using a Web cache is that objects in the Web cache can get outdated since the Web cache only stores a copy of the object while the actual object sits at the server and can be modified. The simple fix is for the Web cache to send a request to the server to check the last time the object was modified.</p>
      <p>Let's say the browser sends a request to the Web cache. The Web cache doesn't have it, so it sends a request to the server. The server sends the object to the Web cache:</p>
      <p><code>HTTP/1.1 200 OK<br>Date: Sun, 17 Sep 2023 19:10:09<br>Server: Apache/1.3.0 (Unix)<br>Last-Modified: Sun, 17 Sep 2023 18:30:09<br>Content-Type: image/gif<br><br>(the requested data...)</code></p>
      <p>The Web cache sends the object to the browser and makes a copy of the object. Let's say the browser requests the same object one week later. The Web cache has a copy of the object this time, but it sends a conditional GET to the server first to check if there have been any changes:</p>
      <p><code>GET /fruit/kiwi.gif HTTP/1.1<br>Host: www.exotiquecuisine.com<br>If-modified-since: Sun, 17 Sep 2023 18:30:09</code></p>
      <p>Let's say the object hasn't been modified since then. The server sends this to the Web cache:</p>
      <p><code>HTTP/1.1 304 Not Modified<br>Date: Sun, 24 Sep 2023 15:39:09<br>Server: Apache/1.3.0 (Unix)</code></p>
      <p>The entity body will be empty since it is pointless to send the object. However, if the object had been modified, the object would have been sent in the entity body.</p>
      <h2 id="email">Email</h2>
      <p>The Internet's mail system has three components: user agents, mail servers, and the Simple Mail Transfer Protocol (SMTP). <b>User agents</b> are applications that allow us to read and write emails, like Gmail and Outlook. <b>Mail servers</b> store emails in <b>mailboxes</b> and send emails to other peoples' mailboxes. Before being sent, emails wait in the sender's mail server's <b>message queue</b>.</p>
      <div class="ln-box">
        <p>We, as individuals, have our own mail servers (in a sense). This may sound a little weird (what? I'm running a server?). When we create an email account on, say, Google, Google manages a mail server for us. This mail server is shared with other Google users. (Note, the mail server is shared, not the mailbox.)</p>
      </div>
      <h3>SMTP</h3>
      <p><b>SMTP</b> is the application-layer protocol for email applications. Specifically, it is the protocol for <em>sending</em> emails (not receiving). It uses TCP (which is probably not surprising). When a mail server sends mail, it is an SMTP client. When a mail server receives mail, it is an SMTP server.</p>
      <div class="ln-box">
        <p>SMTP requires emails to be encoded using 7-bit ASCII. This restriction is a result of SMTP being around since the 1980s, when people weren't sending a lot of emails, much less emails with large attachments like images and videos.</p>
      </div>
      <ol>
        <li>Person 1 uses their user agent to write and send an email</li>
        <li>Person 1's user agent sends the email to Person 1's mail server, which places the email in its message queue</li>
        <li>Person 1's mail server opens a TCP connection to Person 2's mail server</li>
        <li>After TCP's whole handshaking thing, Person 1's mail server sends the email</li>
        <li>Person 2's mail server receives the email and puts it in Person 2's mailbox</li>
        <li>Person 2 uses their user agent to read the email</li>
      </ol>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/email.png">
      </div>
      <div class="ln-box">
        <p>Person 1's user agent sends emails to Person 1's mail server, which then sends emails to Person 2's mail server. It may seem more efficient for Person 1's user agent to send emails directly to Person 2's mail server. This won't work though because user agents don't have a way to deal with Person 2's mail server being unreachable for whatever reason. Mail servers, which are always on and always connected to the Internet, keep retrying every `n` minutes until it works.</p>
      </div>
      <p>SMTP uses five commands, all of which are fairly obvious: <code>HELO</code>, <code>MAIL FROM</code>, <code>RCPT TO</code>, <code>DATA</code>, and <code>QUIT</code>.</p>
      <h3>Mail Message Formats</h3>
      <p>An email consists of a header and a body. The header is kinda like the metadata of the email. An example:</p>
      <p><code>From: me@gmail.com<br>To: you@gmail.com<br>Subject: Thanks for reading this</code></p>
      <p>And, of course, the body contains the message.</p>
      <h3>Mail Access Protocols</h3>
      <p>As mentioned eariler, SMTP is used only for sending emails. For receiving emails, there are two main protocols. Web-based applications, like Google's Gmail website and app, use HTTP. Some other clients, like Outlook, use <b>Internet Mail Access Protocol</b> (<b>IMAP</b>).</p>
      <p>Both of these protocols also allow us to manage our emails. For example, we can move emails into folders, delete emails, and mark emails as important.</p>
      <div class="ln-box">
        <p>Mail servers only use SMTP to send emails. User agents on the other hand, can use SMTP or HTTP to send emails to the mail server.</p>
      </div>
      <h2 id="dns">DNS</h2>
      <p>As we probably know, computers like to work with numbers. So naturally, servers are uniquely identified by a set of numbers (IP address). However, we don't type a bunch of numbers if we want to go to Google â€” we type "google.com" (does anyone actually type this to Google something?). "google.com" is what's known as a <b>hostname</b>, which is a human-readable name for a server.</p>
      <h3>Services Provided by DNS</h3>
      <p>The Internet's <b>domain name system</b> (<b>DNS</b>) is what allows us to type human-readable names to go to websites. It converts human-readable hostnames to IP addresses.</p>
      <div class="ln-box">
        <p>DNS is a distributed database deployed on a bunch of <b>DNS servers</b>.</p>
        <p>It is also an application-layer protocol that allows hosts to query the database. It uses UDP.</p>
      </div>
      <p>Here's how it works:</p>
      <ol>
        <li>There is a DNS application running on our phone/computer</li>
        <li>The browser sends the hostname to the DNS application</li>
        <li>The DNS application uses the hostname to query a DNS server for the IP address</li>
        <li>The DNS application receives the IP address and sends it to the browser</li>
        <li>The browser uses the IP address to open a TCP connection</li>
      </ol>
      <p>From this, we can see yet another source of delay: translating the hostname to an IP address.</p>
      <p>DNS also provides additional related services. Sometimes, a hostname can be complicated, like "relay1.west-coast.enterprise.com". An alias can be set up so that we can type "enterprise.com" instead of that long hostname. The long (real) hostname is called the <b>canonical hostname</b>. This is called <b>host aliasing</b>. Something similar is available for mail servers too. "gmail.com" is more likely an alias than an actual hostname for a mail server (which might be something like "relay1.west-coast.gmail.com"). This is <b>mail server aliasing</b>.</p>
      <p>Companies often have several servers running their website. Each server has its own IP address, but those IP addresses should all point to the same website. The DNS database stores this set of IP addresses and rotates between the IP addresses so that traffic is distributed evenly across all servers. So DNS also provides <b>load distribution</b>.</p>
      <h3>How DNS Works</h3>
      <p>DNS is a distributed database organized in a hierarchy. At the top are <b>root DNS servers</b>. This is where the translation starts. The DNS application queries the root DNS server, which then looks at the top-level domain (e.g., com, org, edu, gov, net). It does this because there are dedicated DNS servers for each top-level domain (i.e., there are DNS servers for all .com websites, DNS servers for all .edu websites, etc.). Appropriately, they're called <b>top-level domain</b> (<b>TLD</b>) <b>servers</b>. So the root server will tell the DNS application which TLD server to contact. The TLD server will send back to the DNS application the IP address of the <b>authoritative DNS server</b>, which is where the actual IP address of the website sits. So finally, the DNS application will contact the authoritative DNS server for the IP address of the website.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/dns.png">
      </div>
      <p>There's also a <b>local DNS server</b>, which aren't part of the hierarchy for some reason. The local DNS server is the entry point into the DNS hierarchy; the DNS application first contacts the local DNS server, which then contacts the root DNS server. Local DNS servers are managed by ISPs.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-smaller" src="../pictures/networking/dns_2.png">
        <p>1. Local DNS! What is google.com's IP address?</p>
        <p>2. Root DNS! What is google.com's IP address?</p>
        <p>3. Go ask a com TLD server. Here's the IP address of one.</p>
        <p>4. TLD DNS! What is google.com's IP address?</p>
        <p>5. Go ask one of Google's authoritative server. Here's the IP address of one.</p>
        <p>6. Authoritative DNS! What is google.com's IP address?</p>
        <p>7. The IP address is 127.0.0.1.</p>
        <p>8. I finally know google.com's IP address. It's 127.0.0.1.</p>
      </div>
      <div class="ln-box">
        <p>In the example above, the queries from the local DNS server (2, 4, and 6) are <b>iterative queries</b>. This means that the responsibility of asking the next query is placed on the server that asked the query.</p>
        <p>There are also <b>recursive queries</b>, in which the responsibility of asking the next query is placed on the server who got asked. In the example above, 1 is a recursive query.</p>
        <p>Here's an example where all the queries are recursive.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-smaller" src="../pictures/networking/dns_3.png">
          <p>1. Local DNS! What is google.com's IP address?</p>
          <p>2. Root DNS! What is google.com's IP address?</p>
          <p>3. TLD DNS! What is google.com's IP address?</p>
          <p>4. Authoritative DNS! What is google.com's IP address?</p>
          <p>5. TLD DNS! The IP address is 127.0.0.1.</p>
          <p>6. Root DNS! The IP address is 127.0.0.1.</p>
          <p>7. Local DNS! The IP address is 127.0.0.1.</p>
          <p>8. I finally know google.com's IP address. It's 127.0.0.1.</p>
        </div>
        <p>In practice though, most queries are iterative.</p>
      </div>
      <h4>DNS Caching</h4>
      <p>As we saw earlier, there are a lot of queries that get made when going through the hierarchy. This traversal can be skipped if the DNS server stores the IP address of whatever was queried. For example, a local DNS server can store the IP address of a website, so that if another host wants to visit the same website, the local DNS server doesn't have to ask all those other DNS servers; it just returns the IP address it stored. This cache isn't good for very long though (typically 2 days).</p>
      <div class="ln-box">
        <p>Especially for .com websites, root DNS servers are often bypassed since the local DNS server should typically have the IP address of the appropriate TLD server in its cache. How many .com websites do we (as a whole) go to in a day?</p>
      </div>
      <h3>DNS Records and Messages</h3>
      <p>The entries in the database are called <b>resource records</b>. A resource record is a four-tuple that looks like this: <code>(Name, Value, Type, TTL)</code>. <code>TTL</code> stands for "time to live", which indicates when the record should be removed from a cache. The <code>name</code> and <code>value</code> depend on what the <code>type</code> is. (We'll ignore the TTL in the examples below.)</p>
      <p>Type A records are hostname-to-IP mappings. <code>(relay1.abc.def.com, 145.37.93.126, A)</code>. The <code>name</code> is the hostname and the <code>value</code> is the IP address for that hostname.</p>
      <p>Type NS records are domain-to-DNS-server mappings. <code>(abc.com, dns.abc.com, NS)</code>. The <code>name</code> is a domain and the <code>value</code> is the hostname of a DNS server.</p>
      <p>CNAME records are alias-to-canonical-hostname mappings. <code>(abc.com, relay1.abc.com, CNAME)</code>. The <code>name</code> is an alias and the <code>value</code> is the canonical hostname.</p>
      <p>MX records are the equivalent of CNAME records but for mail servers. <code>(abc.com, mail.abc.com, MX)</code>.</p>
      <h4>Inserting Records into the DNS Database</h4>
      <p>If we want to register a domain name, say "ourwebsite.com", we do so through a <b>registrar</b>. A registrar makes sure the domain is unique and inserts the appropriate records into the DNS database.</p>
      <div class="ln-box">
        <p>The registrar also needs the names and IP addresses of the primary and secondary authoritative DNS servers. Once that is provided, the registrar inserts a Type NS record and a Type A record into the appropriate TLD server. For example, <code>(ourwebsite.com, dns1.ourwebsite.com, NS)</code> and <code>(dns1.ourwebsite.com, 199.199.199.199, A)</code> are inserted into the TLD servers for .com websites. So when a (local) DNS server contacts a TLD server, the TLD server will return both the Type NS record and the Type A record.</p>
      </div>
      <h2 id="p2p">Peer-to-Peer File Distribution</h2>
      <p>We saw this earlier, but in peer-to-peer applications, there are no servers sending data to clients. Instead, devices connect to each other and are called peers. Specifically for file distribution, peers send/receive portions of files to/from other peers.</p>
      <div class="ln-box">
        <p>It may be weird to think that only portions of files get distributed. While it's possible for a peer to have the whole file in storage, the peer may choose/have to only share a portion of the file with other peers for whatever reason. The peers who only got a portion of the file will share whatever they got with other peers who don't have it yet.</p>
      </div>
      <div class="ln-box">
        <h3>Scalability of P2P Architectures</h3>
        <p>Suppose there is one server that contains a file with `F` bits. There are `N` peers that want to download the file. Let `u_s` be the upload rate of the server and `d_i` be the download rate of the `i^(th)` peer.</p>
        <p>First we'll look at the <b>distribution time</b> (the time it takes for all `N` peers to get the file) in the client-server architecture, which we'll denote `D_(cs)`.</p>
        <p>The server has to transfer `F` bits to `N` peers, so the server has to transfer a total of `NF` bits. The server's upload rate is `u_s`, so the distribution time is at least `(NF)/u_s`.</p>
        <p>Peers will download the file at different rates, so let `d_(min)` be the download rate of the slowest peer, i.e., `d_(min)=min{d_1, d_2, ..., d_n}`. This peer will take `F/d_(min)` seconds to receive the file, so the distribution time is also at least `F/d_(min)`.</p>
        <p>So the distribution time is</p>
        <div class="ln-center">
          <p>`D_(cs)gemax{(NF)/u_s, F/d_(min)}`</p>
        </div>
        <p>For large `N`, `(NF)/u_s` will be larger than `F/d_(min)`, so the distribution time can be determined by `(NF)/u_s`. This means the distribution time will be higher the more peers there are. Mathematically, the distribution time increases linearly with the number of peers.</p>
        <p>Now we'll look at the distribution time for peer-to-peer architecture, which we'll denote as `D_(P2P)`. Since peers can now upload, let `u_i` be the upload rate of the `i^(th)` peer.</p>
        <p>The server only has to transfer `F` bits once instead of to each peer. This is because the peers can transfer the bits among themselves. With an upload rate of `u_s`, the distribution time is at least `F/u_s`.</p>
        <p>Again, the peer with the lowest download rate will affect distribution time, so the distribution time will also be at least `F/d_(min)`.</p>
        <p>Looking at the system as a whole, there are `NF` bits that need to be transferred. With everyone uploading, the total upload rate is `u_text(total)=u_s+u_1+...+u_N=u_s+sum_(i=1)^Nu_i`. So the distribution time is also at least `(NF)/(u_s+sum_(i=1)^Nu_i)`.</p>
        <p>So the distribution time is</p>
        <div class="ln-center">
          <p>`D_(P2P)gemax{F/u_s,F/d_(min),(NF)/(u_s+sum_(i=1)^Nu_i)}`</p>
        </div>
        <p>This time, for large `N`, the distribution time doesn't necessarily increase as `N` increases. While having more peers does mean there are more bits that need to be transferred, it also means that there are more peers to help with uploading.</p>
      </div>
      <h4>BitTorrent</h4>
      <p>BitTorrent is a P2P protocol for file distribution. A group of peers uploading and downloading a particular file is called a torrent. A file is broken up into equal-size chunks, so peers in a torrent upload and download files in chunks.</p>
      <div class="ln-box">
        <p>Peers who have not received the whole file yet are called leechers. After a peer has received the whole file, they can choose to leave or stay in the torrent. Those who stay are called seeders. These are the good guys since they help other peers get the file. Those who leave are selfish.</p>
      </div>
      <p>Each torrent has a tracker, which is responsible for keeping track of peers in the torrent. When a new peer joins a torrent, the tracker randomly selects a subset of existing peers and sends their IP addresses to the new peer. The new peer then tries to establish TCP connections to these peers. We'll refer to succcessfully connected peers as "neighboring peers". The tracker continuously sends IP addresses to new and existing peers as peers leave and join the torrent.</p>
      <p>Let's say we join a torrent. While downloading the file, we will periodically ask our neighboring peers for a list of chunks they have. Let's say this is what the list looks like:</p>
      <ul>
        <li>Peer A has chunks 1, 3, 5, 7, 9</li>
        <li>Peer B has chunks 1, 2, 3</li>
        <li>Peer C has chunks 1, 3, 10</li>
        <li>Peer D has chunks 1, 5, 7, 9, 10</li>
      </ul>
      <p>Suppose we only have chunks 1 and 3 so far. For deciding which peer to request chunks from, BitTorrent uses a technique called <b>rarest first</b>, meaning that we find the chunk that has the fewest copies and download that first. So in this example, 2 is the rarest chunk since only Peer B has it. The idea is that we want to distribute the rarest chunks first to equalize all the chunks, which makes the file easier to download. If only one peer is holding the rarest chunk and that peer leaves the torrent, then no one else in the torrent can download the whole file since they're missing that rarest chunk.</p>
      <p>While downloading chunks, we also have peers requesting chunks that we have so far. We will prioritize the peers who are giving us data at the highest rate. Let's say we respond to the top 4 peers; these peers are <b>unchoked</b>. However, if we only focus on our top 4, then other peers will ignore us when we want more chunks (since we're not on their top 4). So we will also randomly choose a peer to respond to; this peer is <b>optimistically unchoked</b>. The idea is that we want to (optimistically) become someone else's unchoked peer by being nice to them (giving them our chunks) so that they will be nice to us (and give us their chunks). This also helps new peers who don't have the power to upload at high rates, since they will occasionally receive chunks as an optimistically unchoked peer.</p>
      <p>This process of prioritizing the top peers is called tit-for-tat.</p>
      <h2 id="cdn">Video Streaming and Content Distribution Networks</h2>
      <h3>Internet Video</h3>
      <p>A video is a bunch of images displayed really quickly (typically 30 images per second). Images are made up of pixels, where a pixel contains the image's color at that pixel's location. And computers see pixels as a bunch of bits.</p>
      <p>Videos can be compressed by reducing the number of pixels in the images. Reducing the number of pixels reduces the number of bits, thereby reducing the bit rate of the video. Bit rate can be thought of as the number of bits being shown per second. The more bits there are, the more pixels there are, which means a better video quality. Compression is used to create different versions of the same video, with each version having a different quality.</p>
      <p>In order for a video to be able to be played continuously, the average throughput of the network must be at least as large as the bit rate. In other words, the network must be able to transfer bits as fast as the video can show them.</p>
      <h3>HTTP Streaming and DASH</h3>
      <p>Streaming is playing videos without downloading the whole video first. Since a video is likely too large to send all at once, the server storing the video sends it to the client in chunks at a time. The client plays whatever chunks it has received and this repeats until the whole video is sent.</p>
      <p>The problem with this is that all clients receive the same video, regardless of the strength of their Internet connection (i.e. bandwidth). So clients with low bandwidth will struggle to play videos with high bit rates. This is where DASH comes in. In <b>Dynamic Adaptive Streaming over HTTP</b>, videos are compressed into different versions, so that clients with high bandwidth can play videos with high bit rates and clients with low bandwidth can play videos with low bit rates. This also works if the client's bandwidth fluctuates; when the bandwidth is high, the client will get the bytes from the high-quality video and when the bandwidth is low, the client will switch to getting bytes from the lower-quality video.</p>
      <p>The client knows about the different versions of the video because there is a <b>manifest file</b> on the server. The manifest file has the URL and bit rate of all the versions of the video.</p>
      <h3>Content Distribution Networks</h3>
      <p>Let's say an Internet video company (like YouTube) has a huge data center where they store and stream all their videos. Of course, there are several problems with having just one data center.</p>
      <p>The data center can't possibly be conveniently located for everyone in the world. The farther away a client is, the more links there are for the bytes to go through. And the more links there are, the more likely it is for the bytes to hit a bottleneck link, reducing throughput.</p>
      <p>Popular videos will also be sent over and over again, which is costly and inefficient because the company has to pay their ISP for sending the same bytes over and over again.</p>
      <p>And finally, a single data center is a single point of failure.</p>
      <p>It's probably obvious at this point, but <b>Content Distribution Networks</b> (<b>CDNs</b>) are networks of distributed servers and data centers, which we'll call clusters. These clusters all have copies of the videos so that clients can connect to the best CDN for them.</p>
      <p>There are two main strategies for placing clusters. The first is <b>enter deep</b>, where the clusters are "deep" into the access network. With many clusters placed in access ISPs, they are physically located closer to us, reducing the number of links and routers. The drawback to this strategy is that there are many clusters that need to be deployed and managed.</p>
      <p>The second strategy is <b>bring home</b>, where the clusters "bring the ISPs home" by sitting closer to the network core in Internet exchange points (IXPs). This is easier to manage because there are fewer clusters, but there are more links and routers, which means possibly higher delay and lower throughput.</p>
      <p>The clusters typically don't have copies of every single video. If the cluster needs a video it doesn't have, it pulls the video from the server or another cluster.</p>
      <h4>CDN Operation</h4>
      <p>Clients don't establish connections to CDNs directly. CDNs intercept the request so that it can find the best cluster for that client.</p>
      <p>Suppose a video company called NetCinema uses a third-party CDN company called KingCDN. Let's say we wanted to watch a video whose URL is "http://video.netcinema.com/6Y7B23V".</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/networking/cdn.png">
      </div>
      <ol>
        <li>Our computer sends a DNS query for video.netcinema.com</li>
        <li>The local DNS server contacts an authoritative DNS server for NetCinema.</li>
        <li>The authoritative DNS server sees the word "video" in the hostname and returns a hostname in the KingCDN's domain, e.g., a1105.kingcdn.com</li>
        <li>The local DNS server contacts an authoritative DNS server for KingCDN to get the IP address of a1105.kingcdn.com</li>
        <li>The authoritative DNS server for KingCDN returns the IP address</li>
        <li>The local DNS server sends the IP address back to our browser</li>
        <li>Our browser sets up the TCP connection and plays the video</li>
      </ol>
      <h4>Cluster Selection Strategies</h4>
      <p>So how do CDNs determine what the best cluster is for a client? One simple way is to pick the cluster that is geographically closest to the client's local DNS. This works most of the time, but sometimes, clusters can be physically close to a client and have a ton of links and routers between them. Also, some clients can use local DNSs located far away from them, so the cluster that's closest to the local DNS isn't close to the client.</p>
      <p>The better approach is to do some research. CDNs can perform <b>real-time measurements</b> of delay and performance between clusters and clients by sending pings to local DNSs. This doesn't always work though since local DNSs can be configured to ignore these pings.</p>
    </div>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
