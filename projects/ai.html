<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
                          _   _
     _  _    _     _  _  | | | |   __     ____
    | |/ \  | |_  | |/_| | | | |  /  \   /    \
    | |   | |  _| |  /   | | | | | || | |  ||  |
    | |   | | |_  | |    | | | | | || | |  ||  |
    |_|   | |___| |_|    |_| |_|  \__/   \____/|
                                               |
                                          ____/
    -->
    <title>ntrllog | Artificial Intelligence</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
    <link rel="shortcut icon" type="image/png" href="../pictures/favicon.ico"/>
    <link href="../css/content.css" rel="stylesheet">
  </head>
  <body>
    <div class="dropdown ln-fixed-right">
      <button class="btn btn-secondary dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">ToC</button>
      <ul class="dropdown-menu">
        <li><a class="dropdown-item" href="#whatisit">What is AI?</a></li>
        <li><a class="dropdown-item" href="#agentsandenvironments">Agents and Environments</a></li>
        <li><a class="dropdown-item" href="#goodbot">Good Behavior: The Concept of Rationality</a></li>
        <li><a class="dropdown-item" href="#environments">The Nature of Environments</a></li>
        <li><a class="dropdown-item" href="#agentstructure">The Structure of Agents</a></li>
        <li><a class="dropdown-item" href="#problemsolvingagents">Problem-Solving Agents</a></li>
        <li><a class="dropdown-item" href="#exampleproblems">Example Problems</a></li>
        <li><a class="dropdown-item" href="#searchalgorithms">Search Algorithms</a></li>
        <li><a class="dropdown-item" href="#uninformedsearchstrategies">Uninformed Search Strategies</a></li>
        <li><a class="dropdown-item" href="#informedsearchstrategies">Informed (Heuristic) Search Strategies</a></li>
        <li><a class="dropdown-item" href="#heuristicfunctions">Heuristic Functions</a></li>
        <li><a class="dropdown-item" href="#localsearch">Local Search and Optimization Problems</a></li>
        <li><a class="dropdown-item" href="#localsearchincontinuousspaces">Local Search in Continuous Spaces</a></li>
        <li><a class="dropdown-item" href="#nondeterministicsearch">Search with Nondeterministic Actions</a></li>
        <li><a class="dropdown-item" href="#partiallyobservablesearch">Search in Partially Observable Environments</a></li>
        <li><a class="dropdown-item" href="#onlinesearch">Online Search Agents and Unknown Environments</a></li>
        <li><a class="dropdown-item" href="#gametheory">Game Theory</a></li>
        <li><a class="dropdown-item" href="#optimaldecisionsingames">Optimal Decisions in Games</a></li>
        <li><a class="dropdown-item" href="#heuristicalphabetasearch">Heuristic Alpha-Beta Tree Search</a></li>
      </ul>
    </div>
    <div class="container ln-line-height">
      <a href="projects.html"><i class="fas fa-long-arrow-alt-left fa-2x"></i></a>
      <h1>Artificial Intelligence</h1>
      <hr>
      <p>Shortcut to this page: <a href="ai.html">ntrllog.netlify.app/ai</a></p>
      <p>Info provided by <a href="https://aima.cs.berkeley.edu" target="_blank">Artificial Intelligence: A Modern Approach</a> and Professor Francisco Guzman (CSULA)</p>
      <h2 id="whatisit">What is AI?</h2>
      <p>Well, it kinda depends on what we think makes a machine "intelligent". Is it the ability for the machine to act like a human? Or is it the ability for the machine to always do the mathematically-optimal thing (<b>rationality</b>)? And is intelligence a way of thinking or a way of behaving? These are the four main dimensions that researchers are using to think about AI.</p>
      <h3>Acting humanly: The Turing test approach</h3>
      <p>Let's say we ask a question to a computer and some people and get back their responses. If we can't tell whether the responses came from a person or from a computer, then the computer passes the <b>Turing test</b>.</p>
      <div class="ln-box">
        <p>In order to pass the Turing test, a computer would need:</p>
        <ul>
          <li><b>natural language processing</b>: communicate in a human language</li>
          <li><b>knowledge representation</b>: store what it knows or hears</li>
          <li><b>automated reasoning</b>: answer questions and draw new conclusions</li>
          <li><b>machine learning</b>: adapt to new circumstances and detect and extrapolate patterns</li>
        </ul>
        <p>There's also a <b>total Turing test</b>, which tests if a computer can <em>physically</em> behave like a human. To pass the total test, a computer would also need:</p>
        <ul>
          <li><b>computer vision</b> and speech recognition</li>
          <li><b>robotics</b>: manipulate objects and move around</li>
        </ul>
      </div>
      <h3>Thinking humanly: The cognitive modeling approach</h3>
      <p>While the Turing test is just focused on the computer's results, some people are more interested in the computer's process. When performing a task, does a computer go through the same thought process as a human would?</p>
      <div class="ln-box">
        <p>If the input-output behavior matches human behavior, then that could give us some insight into how our brains work.</p>
      </div>
      <div class="ln-box">
        <p>We can learn about how humans think by:</p>
        <ul>
          <li><b>introspection</b>: thinking about our thoughts as they go by</li>
          <li><b>psychological experiments</b>: observing a person in action</li>
          <li><b>brain imaging</b>: observing the brain in action</li>
        </ul>
        <p><b>Cognitive science</b> combines computer models and experimental techniques from psychology to study the human mind.</p>
      </div>
      <h3>Thinking rationally: The "laws of thought" approach</h3>
      <p>The world of <b>logic</b> is pretty much, "if these statements are true, then this has to be true". If we give a computer some assumptions, can it reason its way to a correct conclusion?</p>
      <div class="ln-box">
        <p>In things like chess and math, all the rules are known, so it's relatively easy to make logical choices and statements. But in the real world, there's a lot of uncertainty. Interestingly, the theory of <b>probability</b> allows for rigorous reasoning with uncertain information. With this, we can use raw perceptual information to understand how the world works and make predictions.</p>
      </div>
      <h3>Acting rationally: The rational agent approach</h3>
      <p>A computer agent is defined to be something that can operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change, and create and pursue goals. A <b>rational agent</b> acts to achieve the best outcome.</p>
      <p>There's a subtle distinction between "thinking" rationally and "acting" rationally. Thinking rationally is about making correct inferences. (What's implied here is that there is some (slow) deliberate reasoning process involved.) But it's possible to act rationally without having to make inferences. For example, recoiling from a hot stove, which is more of a reflex action than something you come to a conclusion about.</p>
      <div class="ln-box">
        <p>The rational agent approach is how most researchers approach studying AI. This is because rationality (as opposed to humanity) is easier to approach scientifically, and the rational agent approach kinda encapsulates all the other approaches. For example, in order to act rationally, a computer needs the skills that is required for it to act like a human.</p>
      </div>
      <p>So AI is the science of making machines act rationally, a.k.a. <b>doing the right thing</b>. The "right" thing is specified by us to the agent as an objective that it should complete. This way of thinking about AI is considered the <b>standard model</b>.</p>
      <h3>Beneficial machines</h3>
      <p>But the problem with the standard model is that it only works when the objective is fully specified. In the real world, it's hard to specify a goal that is complete and correct. For example, if the primary goal of a self-driving car is safety, then, of course, it's safest to just not leave the house at all, but that's not helpful; there needs to be some tradeoff. Inherently, there is a <b>value alignment problem</b>: the values put into the machine must be aligned with our values. That is, the machine's job shouldn't be to just complete its objective; it should also consider the costs, consequences, and benefits that would affect us.</p>
      <h2 id="agentsandenvironments">Agents and Environments</h2>
      <p>A little more formally, an <b>agent</b> perceives its <b>environment</b> through <b>sensors</b> and acts upon that environment through <b>actuators</b>. Its <b>percept sequence</b> is the complete history of everything it has ever perceived. The agent looks at its percept sequence (and its built-in knowledge) to determine what action to take.</p>
      <div class="ln-box">
        <p>Mathematically, an agent's behavior is defined by an <b>agent function</b>, which maps a percept sequence to an action.</p>
      </div>
      <p>Let's take a look at a robotic vacuum cleaner. Suppose the floor is divided into two squares `A` and `B`, and that the vacuum cleaner is on square `A`. We can create a table that (theoretically) lists all the percept sequences and the actions that (we think) should be taken for each one.</p>
      <table class="table">
        <tr>
          <th>Percept sequence</th>
          <th>Action</th>
        </tr>
        <tr>
          <td>Square `A` is clean</td>
          <td>Move right</td>
        </tr>
        <tr>
          <td>Square `A` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>Square `B` is clean</td>
          <td>Move left</td>
        </tr>
        <tr>
          <td>Square `B` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is clean</td>
          <td>Move right</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>`vdots`</td>
          <td>`vdots`</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is clean, Square `A` is clean</td>
          <td>Move right</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is clean, Square `A` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>`vdots`</td>
          <td>`vdots`</td>
        </tr>
      </table>
      <p>In this very simplified scenario, it's pretty easy to see what the right actions should be. But in general, it's a bit harder to know what the correct action should be.</p>
      <h2 id="goodbot">Good Behavior: The Concept of Rationality</h2>
      <p>So how can we know what the "right" action should be? How do we know that an action is "right"?</p>
      <h3>Performance Measures</h3>
      <p>Generally, we assess how good an agent is by the consequences of its actions (<b>consequentialism</b>). If the resulting environment is desirable, then the agent is good. (good bot)</p>
      <p>A <b>performance measure</b> is a way to evaluate the agent's behavior in an environment. We have to be a little bit thoughtful about how we come up with a performance measure though. For the vacuum cleaner, it might sound like a good idea to measure performance by how much dirt is cleaned up in an eight-hour shift. The problem with this is that a rational agent would maximize performance by repeatedly cleaning up the dirt and dumping it on the floor. A better performance measure would be to reward the agent for a clean floor. For example, we could give it one point for each clean square, incentivizing it to maximize the number of points (and thus the number of clean squares).</p>
      <p>As we can see, it's better to come up with performance measures based on what we want to be achieved in the environment, rather than what we think the agent should do.</p>
      <h3>Rationality</h3>
      <p>Formally, for each possible percept sequence, a rational agent should use the evidence provided by the percept sequence and its built-in knowledge to select an action that maximizes its performance measure (<b>definition of a rational agent</b>). We can assess whether an agent is rational by looking at four things:</p>
      <ul>
        <li>the performance measure that defines success</li>
        <li>the agent's prior knowledge of the environment</li>
        <li>the actions that the agent can perform</li>
        <li>the agent's percept sequence to date</li>
      </ul>
      <p>For our vacuum cleaner:</p>
      <ul>
        <li>the performance measure awards one point for each clean square</li>
        <li>the "geography" of the environment is known (but not the dirt distribution)</li>
        <li>the actions are moving right, moving left, and cleaning</li>
        <li>the agent's percept sequence consists of what square it's at and whether it's clean</li>
      </ul>
      <p>Under these circumstances, we can expect our vacuum cleaner to do at least as well as any other agent.</p>
      <div class="ln-box">
        <p>Suppose that we change the performance measure to deduct one point for each movement. Once all the squares are clean, the vacuum cleaner will just keep moving back and forth (we never defined "stopping" as an action). Then it would be irrational.</p>
      </div>
      <h3>Omniscience, learning, and autonomy</h3>
      <p>One thing that's been sorta glossed over is that the agent doesn't know what will <em>actually</em> happen if it performs an action, i.e., it's not omniscient. So it's more correct to say that a rational agent tries to maximize <em>expected</em> performance.</p>
      <p>In order to be able to maximize expected performance, the agent needs to be able to <b>gather information</b> and <b>learn</b>.</p>
      <div class="ln-box">
        <p>Crossing a street when it's empty is rational. It's expected that you'll reach the other side. But if you get hit by a car because you didn't look before crossing... well... then that's not rational.</p>
      </div>
      <p>If an agent only relies on its built-in knowledge, then it lacks <b>autonomy</b>. In order to be autonomous, the agent should build on its existing knowledge by learning. By being able to learn, a rational agent can be expected to succeed in a variety of environments.</p>
      <h2 id="environments">The Nature of Environments</h2>
      <p>A <b>task environment</b> is the problem that we are trying to solve by using a rational agent.</p>
      <h3>Specifying the task environment</h3>
      <p>We can specify a task environment by specifying the performance measure, the environment, and the agent's actuators and sensors (<b>PEAS</b>). Yes, these are the four things we listed out when defining how an agent can be rational. The vacuum cleaner example was fairly simple though, so we can look at something more complicated, like an automated taxi.</p>
      <ul>
        <li><b>performance measure</b>: correct destination, minimize fuel consumption, minimize time/cost, minimize impact on other drivers, maximize passenger safety and comfort, maximize profits</li>
        <li><b>environment</b>: roads, traffic, obstacles, police, pedestrians, passengers, weather</li>
        <li><b>actuators</b>: steering, accelerator, brake, signal, horn, display screen, speech</li>
        <li><b>sensors</b>: cameras, speedometer, GPS, accelerometer, radar, microphone, touchscreen</li>
      </ul>
      <h3>Properties of task environments</h3>
      <p>A task environment can be <b>fully observable</b> or <b>partially observable</b>. If the agent's sensors give it access to the complete state of the environment, then the task environment is fully observable. Noisy and inaccurate sensors would make it partially observable. Or maybe the capability simply isn't available. For example, a vacuum with a local sensor can't tell whether there is dirt in other squares and an automated taxi can't know what other drivers are thinking. Basically, if the agent has to make a guess, then the environment is partially observable.</p>
      <div class="ln-box">
        <p>An environment can also be <b>unobservable</b> if the agent has no sensors.</p>
      </div>
      <p>A task environment can be <b>single-agent</b> or <b>multiagent</b>. This one's pretty self-explanatory.</p>
      <div class="ln-box">
        <p>Though there is a small subtlety. Does an automated taxi have to view another car as another agent or can it think of it as just another "object"? The answer depends on whether the "object's" performance measure is affected by the automated taxi. If it is, then it must be viewed as an agent.</p>
      </div>
      <div class="ln-box">
        <p>A multiagent environment can be <b>competitive</b> or <b>cooperative</b>. In a competitive environment, maximizing/improving one agent's performance measure minimizes/worsens another agent's performance measure. In a cooperative environment, all agents' performance measures can be maximized.</p>
      </div>
      <p>A task environment can be <b>deterministic</b> or <b>nondeterministic</b>. If the next state of the environment is completely determined by the current state and the agent's action, then it is deterministic. If something unpredictable can happen, then it is nondeterministic. For example, driving is nondeterministic because traffic cannot be predicted exactly. But if you want to argue that traffic is predictable, then some more reasons that driving is nondeterministic are that the tires may blow out unexpectedly or that the engine may seize up unexpectedly.</p>
      <div class="ln-box">
        <p>A <b>stochastic</b> environment is a nondeterministic environment that deals with probabilities. "There's a `25%` chance of rain" is stochastic, while "There's a chance of rain" is nondeterministic.</p>
      </div>
      <p>A task environment can be <b>episodic</b> or <b>sequential</b>. Classifying images is episodic because classifying the current image doesn't depend on the decisions for past images nor does it affect the decisions of future images. Chess and driving are sequential because the available moves depend on past moves, and the next move affects future moves.</p>
      <p>A task environment can be <b>static</b> or <b>dynamic</b>. A static environment doesn't change while the agent is deciding on its next action, whereas a dynamic one does. Driving is dynamic because the other cars are always moving. Chess is static. As time passes, if the environment doesn't change, but the agent's performance score does, then the environment is <b>semidynamic</b>. Timed chess is semidynamic.</p>
      <p>A task environment can be <b>discrete</b> or <b>continuous</b>. This refers to the state of the environment, the way time is handled, and the percepts and actions of the agent. Chess has a finite number of distinct states, percepts, and actions, which means it is discrete. Whereas in driving, speed and location range over continuous values.</p>
      <p>A task environment can be <b>known</b> or <b>unknown</b>. (Well, strictly speaking, not really.) This is not referring to the environment itself, but to the agent's knowledge about the rules of its environment. In a known environment, the outcomes (or outcome probabilities) for all actions are given.</p>
      <div class="ln-box">
        <p>Known vs unknown is not the same as fully vs partially observable. A known environment can be partially observable, like solitaire, where we know the rules, but we can't see the cards that are turned over. An unknown environment can be fully observable, like a new video game, where the screen shows the entire game state, but we don't know what the buttons do yet until we try them.</p>
      </div>
      <div class="ln-box">
        <p>The hardest case is partially observable, multiagent, nondeterministic, sequential, dynamic, continuous, and unknown. Driving is all of these, except the environment is known.</p>
      </div>
      <h2 id="agentstructure">The Structure of Agents</h2>
      <p>Now we get to go over the more technical aspects of how agents work.</p>
      <p>Recall that an agent function maps percept sequences to actions. An <b>agent program</b> is code that implements the agent function. This program runs on some sort of computing device with sensors and actuators, referred to as the <b>agent architecture</b>.</p>
      <div class="ln-center">
        <p>agent = architecture + program</p>
      </div>
      <p>In general, the architecture makes the percepts from the sensors available to the program, runs the program, and sends the program's action choices to the actuators.</p>
      <h3>Agent programs</h3>
      <p>A simple way to design agent programs is to have them take the current percept as input and return an action. Here is some sample pseudocode for an agent program:</p>
      <div class="ln-flex-center">
        <p><code>function TABLE-DRIVEN-AGENT(percept)<br>&nbsp;&nbsp;append percept to the end of percepts<br>&nbsp;&nbsp;action = LOOKUP(percepts, table)<br>&nbsp;&nbsp;return action</code></p>
      </div>
      <div class="ln-center">
        <p><code>percepts</code>: a sequence, initially empty</p>
        <p><code>table</code>: a table of actions, indexed by percept sequences, initially fully specified</p>
      </div>
      <div class="ln-box">
        <p>The <code>table</code> is the same table that we drew out for the vacuum cleaner (in the "Agents and Environments" section). Using a table to list out all the percepts and actions is a way to represent the agent function, but it is typically not practical to build such a table because it must contain every possible percept sequence. Here's why.</p>
        <p>Let `P` be the set of possible percepts and let `T` be the total number of percepts the agent will receive.</p>
        <p>If the agent receives `1` percept, then there are `|P|` possible percepts that it could be.</p>
        <p>If the agent receives `2` percepts, then there are `|P|^2` possible percepts that they could be.</p>
        <p>If the agent receives `3` percepts, then there are `|P|^3` possible percepts that they could be.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/table_entries.png">
          <p>Suppose each shape is a percept. For each percept at `t=1`, there are `|P|` percepts at `t=2` that could be paired with that percept. So `|P|*|P|=|P|^2` total percepts.</p>
          <p>Extending that reasoning, for each of the `|P|^2` percept pairs at `t=1,2`, there are `|P|` percepts at `t=3` that could be paired with that percept pair. So `|P|^2*|P|=|P|^3` total percepts.</p>
        </div>
        <p>In general, there are `sum_(t=1)^T |P|^t` possible percept sequences, meaning that the table will have this many entries. An hour's worth of visual input from a single camera can result in at least `10^(600,000,000,000)` percept sequences. Even for chess, there are at least `10^(150)` percept sequences.</p>
        <p>So yeah, building such a table is not really possible.</p>
      </div>
      <p>Theoretically though, <code>TABLE-DRIVEN-AGENT</code> does do what we want, which is to implement the agent function. The challenge is to write programs that do this without using a table.</p>
      <div class="ln-box">
        <p>Before the 1970s, people were apparently using huge tables of square roots to do their math. Now there are calculators that run a five-line program implementing Newton's method to calculate square roots.</p>
        <p>Can AI do this for general intelligent behavior?</p>
      </div>
      <h3>Simple reflex agents</h3>
      <p>This is the simplest kind of agent. A <b>simple reflex agent</b> select actions based only on the current percept; it ignores the rest of the percept history. Here's an agent program that implements the agent function for our vacuum:</p>
      <div class="ln-flex-center">
        <p><code>function REFLEX-VACUUM-AGENT([location, status])<br>&nbsp;&nbsp;if status == Dirty then return Clean<br>&nbsp;&nbsp;else if location == A then return Move right<br>&nbsp;&nbsp;else if location == B then return Move left</code></p>
      </div>
      <div class="ln-box">
        <p>Note that since the agent is ignoring the percept history, there are only `4` total possible percepts (not `4^T`) at all times. Square `A` is clean, square `A` is dirty, square `B` is clean, and square `B` is dirty.</p>
      </div>
      <p>Simple reflex behaviors can be modeled as "if-then" statements. If the car in front is braking, then initiate braking. Because of their structure, they are called <b>condition-action rules</b> (or <b>situation-action rules</b> or <b>if-then rules</b>).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/simple_reflex_agent.png">
      </div>
      <p><code>REFLEX-VACUUM-AGENT</code> is specific to the world of just two squares. Here's a more general and flexible way of writing agent programs:</p>
      <div class="ln-flex-center">
        <p><code>function SIMPLE-REFLEX-AGENT(percept)<br>&nbsp;&nbsp;state = INTERPRET-INPUT(percept)<br>&nbsp;&nbsp;rule = RULE-MATCH(state, rules)<br>&nbsp;&nbsp;action = rule.ACTION<br>&nbsp;&nbsp;return action</code></p>
      </div>
      <div class="ln-center">
        <p><code>rules</code>: a set of condition-action rules</p>
      </div>
      <p>This approach involves designing a general-purpose interpreter that can convert a percept to a state. Then we provide the rules for specific states.</p>
      <p>While simple reflex agents are, well, simple, they only work well in fully observable environments. The state of the environment has to be easily identifiable, otherwise the wrong rule and action will be applied. For example, it may be hard for the agent to tell if a car is braking or if it's just the taillights that are on.</p>
      <p>For our vacuum example, suppose the vacuum didn't have a location sensor. Then it won't know which square it's on. If it starts at square `A` and chooses to move left, it will fail forever (and likewise if it starts at square `B` and chooses to move right).</p>
      <p>Infinite loops like these often happen in partially observable environments and can be avoided by having the agent <b>randomize</b> its actions. Randomization can be rational in some multiagent environments, but it is usually not rational in single-agent environments.</p>
      <h3>Model-based reflex agents</h3>
      <p>To deal with partial observability, the agent can keep track of the part of the world it can't see now. In order to do this, the agent needs to maintain some sort of <b>internal state</b> that depends on the percept history. For a car changing lanes, the agent needs to keep track of where the other cars are if it can't see them all at once.</p>
      <p>In order to update its internal state, the agent needs to know "how the world works". When the agent turns the steering wheel clockwise, the car turns to the right (effect of agent action). When it's raining, the car's camera can get wet (how the world evolves independent of agent's actions). This is a <b>transition model</b> of the world.</p>
      <p>The agent also needs to know how to interpret the information it gets from its percepts to understand the current state of the world. When the car in front of the agent brakes, there will be several red regions in the forward-facing camera image. When the camera gets wet, there will be droplet-shaped objects in the image. This knowledge is called a <b>sensor model</b>.</p>
      <p>A <b>model-based agent</b> uses the transition model and sensor model to keep track of the state of the world.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/model_based_reflex_agent.png">
      </div>
      <div class="ln-box">
        <p>"What the world is like now" is really the agent's best guess.</p>
      </div>
      <div class="ln-flex-center">
        <p><code>function MODEL-BASED-REFLEX-AGENT(percept)<br>&nbsp;&nbsp;state = UPDATE-STATE(state, action, percept, transition_model, sensor_model)<br>&nbsp;&nbsp;rule = RULE-MATCH(state, rules)<br>&nbsp;&nbsp;action = rule.ACTION<br>&nbsp;&nbsp;return action</code></p>
      </div>
      <div class="ln-center">
        <p><code>state</code>: the agent's current conception of the world state</p>
        <p><code>transition_model</code>: a description of how the next state depends on the current state and action</p>
        <p><code>sensor_model</code>: a description of how the current world state is reflected in the agent's percepts</p>
        <p><code>rules</code>: a set of condition-action rules</p>
        <p><code>action</code>: the agent's most recent action, initially none</p>
      </div>
      <h3>Goal-based agents</h3>
      <p>Sometimes, having information about the current state of the environment is not enough to decide what to do. For example, knowing that there's a fork in the road is not enough; the agent needs to know whether it should go left or right. The correct decision depends on the <b>goal</b>.</p>
      <p>Goal-based decision making is different from following condition-action rules because it involves thinking about the future. A reflex agent brakes when it sees brake lights because that's specified in the rules. But a goal-based agent brakes when it sees brake lights because that's how it thinks it will avoid hitting other cars.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/goal_based_agent.png">
      </div>
      <div class="ln-box">
        <p>Goal-based agents are also more flexible than reflex agents. If we want a goal-based agent to drive to a different destination, then we just have to update the goal. But if we want a reflex agent to go to a different destination, we have to update all the rules so that they go to the new destination.</p>
      </div>
      <h3>Utility-based agents</h3>
      <p>While achieving the goal is what we want the agent to do, there are many different actions that can be taken to get there. And those different actions can affect how "happy" or "unhappy" the agent will be. This level of "happiness" is referred to as <b>utility</b>.</p>
      <div class="ln-box">
        <p>For example, a quicker/safer/more reliable/cheaper route makes the agent "happier" because it optimizes its performance measure.</p>
      </div>
      <div class="ln-box">
        <p>A <b>utility function</b> maps environment states to a desirability score. If there are conflicting goals (e.g., speed and safety), the utility function specifies the appropriate tradeoff.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/utility_based_agent.png">
      </div>
      <div class="ln-box">
        <p>An agent won't know exactly how "happy" it will be if it takes an action. So it's more correct to say that a rational utility-based agent chooses the action that maximizes <b>expected utility</b> (as opposed to just utility).</p>
      </div>
      <h3>Learning agents</h3>
      <p>The preferred way to build agents (model-based, goal-based, utility-based, etc.) is to build them as a learning agent and then teach them.</p>
      <p>A learning agent can be divided into four conceptual components: the learning element, the performance element, the critic, and the problem generator. The <b>learning element</b> is responsible for making improvements. The <b>performance element</b> is responsible for selecting actions. The <b>critic</b> provides feedback on how the agent is doing and determines how the performance element should be modified to do better.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/learning_agent.png">
      </div>
      <p>The <b>problem generator</b> is responsible for suggesting actions that will lead to new and informative experiences. This allows the agent to explore some suboptimal actions in the short run that may become much better actions in the long run.</p>
      <div class="ln-box">
        <p>The performance standard can be seen as a mapping from a percept to a <b>reward</b> or <b>penalty</b>. This allows the critic to learn what is good and bad.</p>
        <p>The performance standard is a fixed external thing that is not a part of the agent.</p>
      </div>
      <hr>
      <p>For now, we'll assume that the environments are episodic, single agent, fully observable, deterministic, static, discrete, and known.</p>
      <hr>
      <h2 id="problemsolvingagents">Problem-Solving Agents</h2>
      <p>If the agent doesn't know what the correct action to take should be, it should do some problem solving:</p>
      <ol>
        <li><b>Goal formulation</b>: figure out what the goal is to limit the number of actions that need to be considered</li>
        <li><b>Problem formulation</b>: describe the states and actions (come up with an abstract model of the world)</li>
        <li><b>Search</b>: try different sequences of actions to find one (the <b>solution</b>) that achieves the goal</li>
        <li><b>Execution</b>: just do it</li>
      </ol>
      <div class="ln-box">
        <p>In a fully observable, deterministic, known environment, the solution to any problem is a fixed sequence of actions. For example, traveling from one place to another.</p>
        <p>In a partially observable or nondeterministic environment, a solution would be a branching strategy (if/else) that recommends different future actions depending on what percepts the agent sees. For example, the agent might come across a sign that says the road is closed, so it would need a backup plan.</p>
      </div>
      <h3>Search problems and solutions</h3>
      <p>Formally, a <b>search</b> problem can be defined by specifying:</p>
      <ul>
        <li>the <b>state space</b>: the set of possible states that the environment can be in</li>
        <li>the <b>initial state</b> that the agent starts in</li>
        <li>a set of one or more <b>goal states</b></li>
        <li>the <b>actions</b> available to the agent</li>
        <li>a <b>transition model</b> that describes what each action does</li>
        <li>an <b>action cost function</b> that describes the cost of performing an action
          <ul>
            <li>the cost should reflect the agent's performance measure, such as distance, time, or monetary cost</li>
          </ul>
        </li>
      </ul>
      <p>A sequence of actions forms a <b>path</b>, and a <b>solution</b> is a path from the initial state to a goal state. The cost of a path is the sum of the costs of each action on that path, so an <b>optimal solution</b> is the path with the lowest cost among all solutions. Everything can be represented as a <b>graph</b> where the vertices are the states and the edges are the actions.</p>
      <h2 id="exampleproblems">Example Problems</h2>
      <h3>Standardized problems</h3>
      <p>A <b>grid world</b> problem is an array of cells where agents can move from cell to cell and each cell can contain objects. Our little vacuum example is an example of a grid world problem.</p>
      <ul>
        <li><b>states</b>: there are `8` different states depending on which objects are in which cells
          <ul>
            <li>for each of the two cells, it can contain the vacuum or not and it can contain dirt or not (`2*2*2=8`)</li>
          </ul>
        </li>
        <li><b>initial state</b>: any state can be chosen as the initial state</li>
        <li><b>actions</b>: move left, move right, clean</li>
        <li><b>transition model</b>: cleaning removes dirt from the agent's cell and move left/right are self-explanatory</li>
        <li><b>goal states</b>: all the states where every cell is clean
          <ul>
            <li>both cells are clean and the vacuum ends up in the left cell</li>
            <li>both cells are clean and the vacuum ends up in the right cell</li>
          </ul>
        </li>
        <li><b>action cost</b>: each action costs `1`</li>
      </ul>
      <div class="ln-box">
        <p>This one's interesting. Donald Knuth conjectured that a sequence of square root, floor, and factorial operations can be applied to the number `4` to reach any positive integer. For example, we can reach `5` by applying eight operations:</p>
        <div class="ln-center">
          <p>`lfloorsqrt(sqrt(sqrt(sqrt(sqrt((4!)!)))))rfloor=5`</p>
        </div>
        <ul>
          <li><b>states</b>: positive real numbers</li>
          <li><b>initial state</b>: `4`</li>
          <li><b>actions</b>: apply square root, floor, or factorial operation</li>
          <li><b>transition model</b>: the effect of applying the operations are mathematically defined</li>
          <li><b>goal state</b>: the desired positive integer</li>
          <li><b>action cost</b>: each action costs `1`</li>
        </ul>
      </div>
      <h3>Real-world problems</h3>
      <p>A common type of real-world problem is the <b>route-finding problem</b> (finding an optimal path from point A to point B). For example, airline travel:</p>
      <ul>
        <li><b>states</b>: location and current time</li>
        <li><b>initial state</b>: our home airport</li>
        <li><b>actions</b>: take any flight from the current location</li>
        <li><b>transition model</b>: the flight's destination will become our new current location and the flight's arrival time will become our new current time</li>
        <li><b>goal state</b>: our destination
          <ul>
            <li>the goal can also be "arrive at the destination on a nonstop flight"</li>
          </ul>
        </li>
        <li><b>action cost</b>: combination of monetary cost, waiting time, flight time, seat quality, time of day, type of airplane, frequent-flyer reward points, etc.</li>
      </ul>
      <div class="ln-box">
        <p><b>Touring problems</b> is an extension of the route-finding problem where multiple locations must be visited instead of just one destination. The <b>traveling salesperson problem</b> is a popular example.</p>
      </div>
      <h2 id="searchalgorithms">Search Algorithms</h2>
      <p>Search problems can be represented as a <b>search tree</b>, where a <b>node</b> represents a state and the edges represent the actions. For Knuth's conjecture:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/search_tree.png">
        <p>The purple nodes are nodes that have been expanded. The green nodes are nodes that are on the frontier, i.e., they have not been expanded yet.</p>
      </div>
      <p>When we're at a node, we can <b>expand</b> the node to consider the available states that are possible (<b>generate</b> nodes). The nodes that have not been expanded yet make up the <b>frontier</b> of the search tree.</p>
      <h3>Best-first search</h3>
      <p>So how do we decide which path to take? A good general approach is to perform <b>best-first search</b>. In this type of search, we define an <b>evaluation function</b>, which, in a sense, is a measure of how close we are to reaching the goal state. When we apply the evaluation function to each frontier node, the node with the minimum value of the evaluation function should be the next node to go to.</p>
      <div class="ln-box">
        <p>One example of an evaluation function for Knuth's conjecture could be</p>
        <div class="ln-center">
          <p>`f(n) = abs(n-G)`</p>
        </div>
        <p>where `n` is the current node and `G` is the goal. This evaluation function is measuring the distance between where we are and where we want to be, so it makes sense to choose the node that minimizes this distance. Unfortunately, this isn't a very helpful evaluation function to use because it would tell us to keep applying the floor operation.</p>
      </div>
      <h3>Search data structures</h3>
      <p>We can implement a node by storing:</p>
      <ul>
        <li>state: the state of the node</li>
        <li>parent: the node that generated this node</li>
        <li>action: the action that was applied to the parent to generate this node</li>
        <li>path-cost: the total cost of the path from the initial state to this node</li>
      </ul>
      <p>We can store the frontier using a <b>queue</b>. A queue is good because it provides us with the operations to check if a frontier is empty, take the top node from the frontier, look at the top node of the frontier, and add a node to the frontier.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function BEST-FIRST-SEARCH(problem, f)<br>&nbsp;&nbsp;node = NODE(STATE = problem.INITIAL)<br>&nbsp;&nbsp;frontier = priority queue ordered by f<br>&nbsp;&nbsp;reached = a lookup table<br>&nbsp;&nbsp;while not IS-EMPTY(frontier) do<br>&nbsp;&nbsp;&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s = child.STATE<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if s is not in reached or child.PATH-COST &lt; reached[s].PATH-COST then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reached[s] = child<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;return failure</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function EXPAND(problem, node)<br>&nbsp;&nbsp;s = node.STATE<br>&nbsp;&nbsp;for each action in problem.ACTIONS do<br>&nbsp;&nbsp;&nbsp;&nbsp;s' = problem.RESULT(s, action)<br>&nbsp;&nbsp;&nbsp;&nbsp;cost = node.PATH-COST + problem.ACTION-COST(s, action, s')<br>&nbsp;&nbsp;&nbsp;&nbsp;yield NODE(STATE = s', PARENT = node, ACTION = action, PATH-COST = cost)</code></p>
        </div>
      </div>
      <h3>Redundant paths</h3>
      <p>As we traversing the tree, it's possible to get stuck in a <b>cycle</b> (<b>loopy path</b>), i.e., go around in circles through <b>repeated states</b>. Here's an example of a cycle:</p>
      <div class="ln-center">
        <p>`4 ubrace(rarr)_(text(factorial)) 24 ubrace(rarr)_(text(square root)) 4.9 ubrace(rarr)_(text(floor)) 4`</p>
      </div>
      <p>A cycle is a type of <b>redundant path</b>, which is a path that has more steps than necessary. Here's an example of a redundant path:</p>
      <div class="ln-center">
        <p>`4 ubrace(rarr)_(text(floor)) 4 ubrace(rarr)_(text(square root)) 2`</p>
        <p>(This is redundant because we could've just gone to `2` directly without needing the first floor operation.)</p>
      </div>
      <p>There are three ways to handle the possibility of running into redundant paths. The preferred way is to keep track of all the states we've been to. That way, if we see it again, then we know not to go that way. (The best-first search pseudocode does this.)</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/kansas.png">
      </div>
      <p>The bad part about doing this is that there might not be enough memory to keep track of all this info. So we can check for cycles instead of redundant paths. We can do this by following the chain of parent pointers ("going up the tree") to see if the state at the end has appeared earlier in the path.</p>
      <p>Or we can choose to not even worry about choosing redundant paths if it's rare or impossible to have them. From this point on, we'll use <b>graph search</b> for search algorithms that check redundant paths and <b>tree-like search</b> for ones that don't.</p>
      <h2 id="uninformedsearchstrategies">Uninformed Search Strategies</h2>
      <p>The uninformed means that as the agent is performing the search algorithm, it doesn't know how close the current state is to the goal.</p>
      <h3>Breadth-first search</h3>
      <p>In breadth-first search, all the nodes on the same level are expanded first before going further down any paths. This is generally a good strategy to consider when all the actions have the same cost.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/breadth_first_search.gif">
      </div>
      <div class="ln-box">
        <p>Breadth-first search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is the depth of the node, i.e., the number of actions it takes to reach the node.</p>
      </div>
      <div class="ln-box">
        <p>There are several optimizations we can take advantage of when using breadth-first search. Using a first-in-first-out (FIFO) queue is faster than using a priority queue because the nodes that get added to a FIFO queue will already be in the correct order (so there's no need to perform a sort, which we would need to do with a priority queue).</p>
        <p>Best-first search waits until the node is popped off the queue before checking if it's a solution (<b>late goal test</b>). But with breadth-first search, we can check if a node is a solution as soon as it's generated (<b>early goal test</b>). This is because traversing breadth first explores all the possible paths above the generated node, guaranteeing that we've found the shortest path to it. (Think about it like this: because we're traversing breadth first, we can't say, "Would we have found a shorter way here if we had gone on that path instead?" because we already checked that path.)</p>
        <p>Because of this, breadth-first search is cost-optimal for problems where all actions have the same cost.</p>
      </div>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function BREADTH-FIRST-SEARCH(problem)<br>&nbsp;&nbsp;node = NODE(problem.INITIAL)<br>&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;frontier = a FIFO queue<br>&nbsp;&nbsp;reached = {problem.INITIAL}<br>&nbsp;&nbsp;while not IS-EMPTY(frontier) do<br>&nbsp;&nbsp;&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s = child.STATE<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if problem.IS-GOAL(s) then return child<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if s is not in reached then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add s to reached<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;return failure</code></p>
        </div>
      </div>
      <div class="ln-box">
        <p>Let `d` represent the depth of the search tree. Suppose each node generates `b` child nodes.</p>
        <p>The root node generates `b` nodes. Each of those `b` nodes generates `b` nodes (total `b^2` nodes). And each of the `b^2` nodes generates `b` nodes (total `b^3` nodes). So the total number of nodes generated is</p>
        <div class="ln-center">
          <p>`1+b+b^2+b^3+...+b^d=O(b^d)`</p>
        </div>
        <p>Time and space complexity are exponential, which is bad. (All the generated nodes have to remain in memory.)</p>
      </div>
      <h3>Dijkstra's algorithm or uniform-cost search</h3>
      <p>If actions have different costs, then <b>uniform-cost search</b> is a better consideration. The main idea behind this is that the least-cost paths are explored first. (I go into more step-by-step detail about how Dijkstra's algorithm works in my Computer Networking resource.)</p>
      <p>Uniform-cost search will always find the optimal solution, but the main problem with it is that it's inefficient. It will expand a lot of nodes.</p>
      <div class="ln-box">
        <p>Uniform-cost search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is the cost of the path from the root to the current node.</p>
      </div>
      <div class="ln-box">
        <p>The worst-case time and space complexity is `O(b^(1+lfloorC^**//epsilonrfloor))` where `C^**` is the cost of the optimal solution and `epsilon` is a lower bound on the cost of all actions.</p>
        <p>Since costs are positive (`gt 0`), going down one path by one node may be "deeper" than going down another path by one node, so we can't use the same `d` notation for depth. So `C^(**)/epsilon` is an upper bound on how deep the solution is, i.e., the max number of actions that are taken to reach the solution.</p>
      </div>
      <h3>Depth-first search</h3>
      <p>In <b>depth-first search</b>, the deepest node in the frontier is expanded first. Basically, one path at a time is picked and followed until it ends. So if a goal state is found, then that path is chosen as the solution. This also means that the path chosen may not be the cheapest path available. (What if we had gone down that path instead?)</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/depth_first_search.gif">
      </div>
      <div class="ln-box">
        <p>So why should this algorithm even be considered? It's because depth-first search doesn't require a lot of memory. There's no need for a reached table and the frontier is very small.</p>
        <p>The frontier of breadth-first search can be thought of as the surface of an expanding sphere while the frontier of depth-first search can be thought of as the radius of that sphere.</p>
        <p>The memory complexity is `O(bm)` where `b` is the branching factor (number of nodes on a branch) and `m` is the maximum depth of the tree. Whenever we expand a node, we add `b` generated children to the frontier. Then we move to one of the children, generate its `b` children, and add them to the frontier. And so on until the bottom. This happens `m` times. So at the bottom, there are `m` groups of `b` children in the frontier.</p>
      </div>
      <div class="ln-box">
        <p>Depth-first search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is the negative of the depth.</p>
      </div>
      <h3>Depth-limited and iterative deepening search</h3>
      <p>Another problem with depth-first search is that it can keep going down one path infinitely. To prevent this, we can decide to stop going down a path once we reach a certain point. This is <b>depth-limited search</b>.</p>
      <div class="ln-box">
        <p>The time complexity is `O(b^l)` and the space complexity is `O(bl)` for the same reasons as depth-first search.</p>
      </div>
      <p>But how far is too far? If we stop too early, we may not find a solution and if we choose to stop very late, it will take a long time. <b>Iterative deepening search</b> keeps trying different stopping points until a solution is found or until no solution is found (if this is the case, then we have gone deep enough to the point where all paths can be fully explored).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/iterative_deepening_search.gif">
      </div>
      <div class="ln-box">
        <p>It may seem a bit repetitive (or wasteful) to keep generating the top-level nodes. For example, `B` and `C` are generated `3` times at limits `1`, `2`, and `3`. However, this isn't really that big of an issue since the majority of the nodes are at the bottom, and those are only generated a few times compared to the top-level nodes.</p>
        <p>In general, the nodes at the bottom level are generated once, those on the next-to-bottom level are generated twice, ..., and the children of the root are generated `d` times. So the total number of nodes generated is</p>
        <div class="ln-center">
          <p>`(d)b^1+(d-1)b^2+(d-2)b^3+...+b^d=O(b^d)`</p>
        </div>
        <p>So the time complexity is `O(b^d)` when there's a solution (and `O(b^m)` when there isn't).</p>
      </div>
      <div class="ln-box">
        <p>The memory complexity is `O(bd)` when there is a solution and `O(bm)` when there isn't (for finite state spaces).</p>
      </div>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function DEPTH-LIMITED-SEARCH(problem, l)<br>&nbsp;&nbsp;frontier = a LIFO queue<br>&nbsp;&nbsp;result = failure<br>&nbsp;&nbsp;while not IS-EMPTY(frontier) do<br>&nbsp;&nbsp;&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;&nbsp;&nbsp;if DEPTH(node) &gt; l then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result = cutoff<br>&nbsp;&nbsp;&nbsp;&nbsp;else if not IS-CYCLE(node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;return result</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function ITERATIVE-DEEPENING-SEARCH(problem)<br>&nbsp;&nbsp;for depth = 0 to infinity do<br>&nbsp;&nbsp;&nbsp;&nbsp;result = DEPTH-LIMITED-SEARCH(problem, depth)<br>&nbsp;&nbsp;&nbsp;&nbsp;if result != cutoff then return result</code></p>
        </div>
        <p><code>cutoff</code> is a string used to denote that there might be a solution at a deeper level than `l`.</p>
      </div>
      <p>Iterative deepening is the preferred uninformed search method when the search state space is larger than can fit in memory and the depth of the solution is not known.</p>
      <h3>Bidirectional search</h3>
      <p>So far, all the algorithms started at the initial state and moved towards a goal state. But what we can also do is start from the initial state <em>and</em> the goal state(s) and hope that we meet somewhere in the middle. This is <b>bidirectional search</b>.</p>
      <p>We need to keep track of two frontiers and two tables of reached states. There is a solution when the two frontiers meet.</p>
      <div class="ln-box">
        <p>Even though we have to keep track of two frontiers, each frontier is only half the size of the whole tree. So the time and space complexity is `O(b^(d//2))`, which is less than `O(b^d)`.</p>
      </div>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function BIBF-SEARCH(problem_F, f_F, problem_B, f_B)<br>&nbsp;&nbsp;node_F = NODE(problem_F.INITIAL)<br>&nbsp;&nbsp;node_B = NODE(problem_B.INITIAL)<br>&nbsp;&nbsp;frontier_F = a priority queue ordered by f_F<br>&nbsp;&nbsp;frontier_B = a priority queue ordered by f_B<br>&nbsp;&nbsp;reached_F = a lookup table<br>&nbsp;&nbsp;reached_B = a lookup table<br>&nbsp;&nbsp;solution = failure<br>&nbsp;&nbsp;while not TERMINATED(solution, frontier_F, frontier_B) do<br>&nbsp;&nbsp;&nbsp;&nbsp;if f_F(TOP(frontier_F)) &lt; f_B(TOP(frontier_B)) then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;solution = PROCEED(F, problem_F, frontier_F, reached_F, reached_B, solution)<br>&nbsp;&nbsp;&nbsp;&nbsp;else solution = PROCEED(B, problem_B, frontier_B, reached_B, reached_F, solution)<br>&nbsp;&nbsp;return solution</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function PROCEED(dir, problem, frontier, reached, reached_2, solution)<br>&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;s = child.STATE<br>&nbsp;&nbsp;&nbsp;&nbsp;if s not in reached or PATH-COST(child) &lt; PATH-COST(reached[s]) then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reached[s] = child<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if s is in reached_2 then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;solution_2 = JOIN-NODES(dir, child, reached_2[s])<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if PATH-COST(solution_2) &lt; PATH-COST(solution) then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;solution = solution_2<br>&nbsp;&nbsp;return solution</code></p>
        </div>
        <p><code>TERMINATED</code> determines when to stop looking for solutions and <code>JOIN-NODES</code> joins the two paths.</p>
      </div>
      <div class="ln-box">
        <h3>Comparing uninformed search algorithms</h3>
        <p>An algorithm is <b>complete</b> if either a solution is guaranteed to be found if it exists or a failure is reported when it doesn't exist.</p>
        <table class="table">
          <tr>
            <th>Criterion</th>
            <th>Breadth-First</th>
            <th>Uniform-Cost</th>
            <th>Depth-First</th>
            <th>Depth-Limited</th>
            <th>Iterative Deepening</th>
            <th>Bidirectional</th>
          </tr>
          <tr>
            <td>Complete?</td>
            <td>Yes<sup>*</sup></td>
            <td>Yes<sup>*</sup></td>
            <td>No</td>
            <td>No</td>
            <td>Yes<sup>*</sup></td>
            <td>Yes<sup>*</sup></td>
          </tr>
          <tr>
            <td>Optimal cost?</td>
            <td>Yes<sup></sup></td>
            <td>Yes</td>
            <td>No</td>
            <td>No</td>
            <td>Yes<sup></sup></td>
            <td>Yes<sup></sup></td>
          </tr>
          <tr>
            <td>Time</td>
            <td>`O(b^d)`</td>
            <td>`O(b^(1+lfloorC^**//epsilonrfloor))`</td>
            <td>`O(b^m)`</td>
            <td>`O(b^l)`</td>
            <td>`O(b^d)`</td>
            <td>`O(b^(d//2))`</td>
          </tr>
          <tr>
            <td>Space</td>
            <td>`O(b^d)`</td>
            <td>`O(b^(1+lfloorC^**//epsilonrfloor))`</td>
            <td>`O(bm)`</td>
            <td>`O(bl)`</td>
            <td>`O(bd)`</td>
            <td>`O(b^(d//2))`</td>
          </tr>
        </table>
        <p>`b` is the branching factor. `m` is the maximum depth of the search tree. `d` is the depth of the shallowest solution, or is `m` when there is no solution. `l` is the depth limit.</p>
        <p><sup>*</sup>if `b` is finite</p>
        <p><sup></sup>if all action costs are `ge epsilon gt 0`</p>
        <p><sup></sup>if all action costs are identical</p>
        <p><sup></sup>if both directions are breadth-first or uniform-cost</p>
      </div>
      <h2 id="informedsearchstrategies">Informed (Heuristic) Search Strategies</h2>
      <p>Informed means that information is provided about how close the current state is to a goal state.</p>
      <div class="ln-box">
        <p>The information is referred to as "hints", which are given by a <b>heuristic function</b>, denoted as `h(n)`.</p>
        <div class="ln-center">
          <p>`h(n)=` estimated cost of the cheapest path from the state at node `n` to a goal state</p>
        </div>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/maze.png">
      </div>
      <p>For the next few algorithms, we'll be using this maze, where the goal is to find the shortest path from `S` to `G`. The number in each square is the number of squares it is away from the goal (ignoring walls). That is the heuristic we'll be using.</p>
      <h3>Greedy best-first search</h3>
      <p>If we know how close each node is to a goal, then it seems to makes sense to choose the node that seems closest to the goal.</p>
      <div class="ln-box">
        <p>We expand first the node with the lowest `h(n)` value.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/greedy_search_1.png">
      </div>
      <p>Since we're being greedy, we'll move to the square that looks like it's `11` squares away from the goal instead of the square that looks like it's `13` squares away from the goal.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/greedy_search_2.png">
      </div>
      <p>Now obviously, this is not the optimal path. But being greedy is not always a good thing.</p>
      <h3>A* search</h3>
      <p>The reason greedy best-first search failed is because it didn't take into consideration the cost of going to each node -- it only looked at how far away each node was to the destination. Intuitively, if it takes a long time to get to a destination that is close to your goal, is it really worth going to that destination? <b>A* search</b> looks at both the cost to get to each node and the estimated cost from each node to the destination.</p>
      <div class="ln-box">
        <p>A* search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is `g(n)+h(n)`. `g(n)` is the cost from the initial state to node `n`, and `h(n)` is the estimated cost of the shortest path from `n` to a goal state.</p>
        <p>`f(n)=` estimated cost of the best path that continues from `n` to a goal</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/a_star_search_1.png">
      </div>
      <p>Now, the numbers in each square represent the cost of going to that square plus the estimated distance to the goal, i.e., the number of squares it is away from the goal (ignoring walls).</p>
      <p>Just like in greedy search, we'll be going up again. But don't worry, things will be different this time.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/a_star_search_2.png">
      </div>
      <p>At this point, we see that it's no longer worth continuing on this path because there is a cheaper option (`19`) at the fork. So we'll start going that way instead.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/a_star_search_3.png">
      </div>
      <p>Even though we wasted some time going down the longer path, we still found the optimal solution.</p>
      <div class="ln-box">
        <p>Whether A* is cost-optimal depends on the heuristic. If the heuristic is <b>admissible</b>, then A* is cost-optimal. Admissible means that the heuristic never overestimates the cost to reach a goal, i.e., the estimated cost to reach the goal is `le` the actual cost to reach the goal. Admissible heuristics are referred to as being optimistic.</p>
      </div>
      <div class="ln-box">
        <p>We can show A* is cost-optimal with an admissible heuristic by using a proof by contradiction.</p>
        <p>Let `C^**` be the cost of the optimal path. Suppose for the sake of contradiction that the algorithm returns a path with cost `C gt C^**`. This means that there is some node `n` that is on the optimal path and is unexpanded (if we had expanded it, we would've found the optimal path and returned `C^**` instead).</p>
        <p>Let `g^(ast)(n)` be the cost of the optimal path from the start to `n`, and let `h^(ast)(n)` be the cost of the optimal path from `n` to the nearest goal.</p>
        <div class="ln-center">
          <p>`f(n) gt C^**` (otherwise `n` would've been expanded)</p>
          <p>`f(n) = g(n)+h(n)` (by definition)</p>
          <p>`f(n) = g^(ast)(n)+h(n)` (because `n` is on the optimal path)</p>
          <p>`f(n) le g^(ast)(n)+h^(ast)(n)` (because of admissibility, `h(n) le h^(ast)(n)`)</p>
          <p>`f(n) le C^**` (by definition, `C^(ast)=g^(ast)(n)+h^(ast)(n)`)</p>
        </div>
        <p>The first and last lines form a contradiction. So it must be the case that the algorithm returns a path with cost `C le C^**`, i.e., A* is cost-optimal.</p>
      </div>
      <div class="ln-box">
        <p>A heuristic can also be <b>consistent</b>, which is slightly stronger than being admissible (so every consistent heuristic is admissible). A heuristic `h(n)` is consistent if, for every node `n` and every successor `n'` of `n` generated by action `a`,</p>
        <div class="ln-center">
          <p>`h(n) le c(n,a,n') + h'(n)`</p>
        </div>
        <p>What this is saying is that the estimated cost to the goal from our current position should be less than (or equal to) the cost of going to another location plus the estimated cost of going to the goal from that location. In other words, adding a stop should make the trip longer unless that stop is on the way to the destination.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/consistent_heuristic.png">
        </div>
        <p>Consistency is a form of the <b>triangle inequality</b>, which says that a side of a triangle can't be longer than the sum of the other two sides.</p>
      </div>
      <div class="ln-box">
        <p>Here's an example of how things can be not optimal if the heuristic is not admissible/consistent.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/not_admissible.png">
        </div>
        <p>When we're starting at `S`, we can either choose to go to `A` or to `G` directly. Since the estimated cost of `A rarr G` is `6`, it looks more expensive to go to `A` first, so we go to `G` directly instead (`f(A)=1+6=7 gt f(G)=5+0=5`).</p>
        <p>However, `S rarr G` is not the optimal path! `S rarr G` costs `5` while `S rarr A rarr G` costs `4`.</p>
        <p>This happened because we overestimated the distance from `A` to `G`.</p>
      </div>
      <div class="ln-box">
        <p>As we go down a path, the `g` costs should be increasing because action costs are always positive. For our maze example, the closer we get to `G`, the farther we get from `S`. So the distance from `S` to our current position should always be increasing as we keep moving. The `g` costs are (strictly) <b>monotonic</b>.</p>
        <p>So does this mean that `f=g+h` will also monotonically increase? As we move from `n` to `n'`, the cost goes from `g(n)+h(n)` to `g(n)+c(n,a,n')+h(n')`. Canceling out the `g(n)`, we see that the path's cost will be monotonically increasing if and only if `h(n) le c(n,a,n')+h(n')`, i.e., if and only if the heuristic is consistent.</p>
        <p>So if A* search is using a consistent heuristic, then:</p>
        <ul>
          <li>all nodes that can be reached from the initial state and satisfy `f(n) lt C^(**)` will be expanded (<b>surely expanded nodes</b>)
            <ul>
              <li>this is because `f` is monotonically increasing
                <ul>
                  <li>if `f` wasn't monotonically increasing, then if we expand a node, there could be a successor with a lower `f` value, which means there was a better path to get to that successor that we missed (i.e., a node with `f(n) lt C^(**)` wasn't expanded)
                    <ul>
                      <li>but A* works by picking the nodes with the lowest `f`-values to expand, so we couldn't have missed that better path -- there's a contradiction</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>this means that all potential paths with lower costs are explored (ensuring cost-optimality and completeness I think)</li>
            </ul>
          </li>
          <li>some nodes where `f(n)=C^(**)` might be expanded</li>
          <li>nodes where `f(n) gt C^(**)` are not expanded</li>
        </ul>
        <p>These three things lead to A* with a consistent heuristic being referred to as <b>optimally efficient</b>.</p>
      </div>
      <h3>Satisficing search: Inadmissible heuristics and weighted A*</h3>
      <p>Even though A* is complete, cost-optimal, and optimally efficient, the one big problem with A* is that it expands a lot of nodes, which takes time and uses up space. We can expand fewer nodes by using an <b>inadmissible heuristic</b>, which may overestimate the estimated cost to the goal. Intuitively, if the estimated costs of some of the nodes are higher, then there will be fewer nodes that look favorable to expand. Of course, if we overestimate, then there's a chance we might miss an optimal path, but this is the price to pay for expanding fewer nodes. We might end up with suboptimal, but "good enough" solutions (<b>satisficing</b> solutions).</p>
      <p>Inadmissible heuristics are potentially more accurate. Consider a heuristic that uses a straight-line distance to estimate the distance of a road between two cities. The path between two cities is almost never a straight line, so the length of the actual path will be greater than the length of the straight line. Meaning that overestimating the straight-line distance by a little bit will make it closer to the length of the actual path.</p>
      <p>We don't actually need an inadmissible heuristic to take advantage of the faster speed and lower memory usage. We can simulate an inadmissible heuristic by adding some weight to the heuristic.</p>
      <div class="ln-box ln-center">
        <p>`f(n)=g(n)+W*h(n)`</p>
        <p>for some `W gt 1`</p>
      </div>
      <p>This is <b>weighted A* search</b>.</p>
      <div class="ln-box">
        <p>In general, if the optimal cost is `C^(**)`, weighted A* search will find a solution that costs between `C^(**)` and `W*C^(**)`. In practice, it's usually much closer to `C^(**)` than to `W*C^(**)`.</p>
      </div>
      <div class="ln-box">
        <p>Weighted A* can be seen as a generalization of some of the other search algorithms:</p>
        <table class="table">
          <tr>
            <td>A* search</td>
            <td>`g(n)+h(n)`</td>
            <td>`W=1`</td>
          </tr>
          <tr>
            <td>Uniform-cost search</td>
            <td>`g(n)`</td>
            <td>`W=0`</td>
          </tr>
          <tr>
            <td>Greedy best-first search</td>
            <td>`h(n)`</td>
            <td>`W=oo`</td>
          </tr>
          <tr>
            <td>Weighted A* search</td>
            <td>`g(n)+W*h(n)`</td>
            <td>`1 lt W lt oo`</td>
          </tr>
        </table>
      </div>
      <h3>Memory-bounded search</h3>
      <p><b>Beam search</b> keeps only `k` nodes with the best `f`-scores in the frontier. This means that we may be excluding cost-optimal paths (and even solutions), but it's the price to pay to limit the number of expanded nodes. Though for many problems, it can find near-optimal solutions.</p>
      <p><b>Iterative-deepening A* search</b> (IDA*) is a commonly used algorithm for problems with limited memory. Where iterative deepening search uses depth as the cutoff, iterative-deepening A* search uses `f`-cost as the cutoff. Specifically, the cutoff value is the smallest `f`-cost of any node that exceeded the cutoff on the previous iteration.</p>
      <p>Once the cutoff is reached, IDA* starts over from the beginning. <b>Recursive best-first search</b> (RBFS) also uses `f`-values to determine how far down a path it should go, but once the cutoff is reached, it steps back and takes another path (the one with the second-lowest `f`-value) instead of starting over.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function RECURSIVE-BEST-FIRST-SEARCH(problem)<br>&nbsp;&nbsp;solution, fvalue = RBFS(problem, NODE(problem.INITIAL), infinity)<br>&nbsp;&nbsp;return solution</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function RBFS(problem, node, f_limit)<br>&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;successors = LIST(EXPAND(node))<br>&nbsp;&nbsp;if successors is empty then return failure, infinity<br>&nbsp;&nbsp;for each s in successors do<br>&nbsp;&nbsp;&nbsp;&nbsp;s.f = max(s.PATH-COST + h(s), node.f)<br>&nbsp;&nbsp;while true do<br>&nbsp;&nbsp;&nbsp;&nbsp;best = the node in successors with the lowest f-value<br>&nbsp;&nbsp;&nbsp;&nbsp;if best.f &gt; f_limit then return failure, best.f<br>&nbsp;&nbsp;&nbsp;&nbsp;alternative = the second lowest f-value among successors<br>&nbsp;&nbsp;&nbsp;&nbsp;result, best.f = RBFS(problem, best, min(f_limit, alternative))<br>&nbsp;&nbsp;&nbsp;&nbsp;if result != failure then return result, best_f</code></p>
        </div>
      </div>
      <h3>Bidirectional heuristic search</h3>
      <p>Bidirectional search is sometimes more efficient than unidirectional search, sometimes not. If we're using a very good heuristic, then A* search will perform well and adding bidirectional search won't add any more value. If we're using an average heuristic, bidirectional search tends to expand fewer nodes. If we're using a bad heuristic, then both directional and unidirectional search will be equally bad.</p>
      <div class="ln-box">
        <p>If we want to use bidrectional search with a heuristic, it turns out that using `f(n)=g(n)+h(n)` as the evaluation function doesn't guarantee that we'll find an optimal-cost solution. Instead, we need two different evaluation functions, one for each direction:</p>
        <div class="ln-center">
          <p>`f_F(n)=g_F(n)+h_F(n)` for going in the forward direction (initial state to goal)</p>
          <p>`f_B(n)=g_B(n)+h_B(n)` for going in the backward direction (goal to initial state)</p>
        </div>
        <p>This is a <b>front-to-end</b> search.</p>
      </div>
      <div class="ln-box">
        <p>Suppose the heuristic is admissible. Let's say that we're in the middle of running the algorithm and the forward direction is currently at node `j` and the backward direction is currently at node `t`. We can define a lower bound on the cost of a solution path `text(initial) rarr ... rarr j rarr ... rarr t rarr ... rarr text(goal)` as</p>
        <div class="ln-center">
          <p>`lb(j,t)=max(g_F(j)+g_B(t), f_F(j), f_B(t))`</p>
        </div>
        <p>The optimal cost must be at least the cost of `text(initial) rarr ... rarr j` plus the cost of `text(goal) rarr ... rarr t`.</p>
        <p>Since we're using an admissible heuristic, the estimated costs are underestimated. So the optimal cost must also be at least the cost of `text(initial) rarr ... rarr j` plus the estimated cost of `j rarr ... rarr text(goal)`. The same reasoning applies for `f_B(t)`.</p>
        <p>So for any pair of nodes `j,t` with `lb(j,t) lt C^(**)`, either `j` or `t` must be expanded because the path that goes through them is a potential optimal solution. The problem is that we don't know for sure which node is better to expand -- that part's just up to luck.</p>
        <p>Either way, this lower bound gives us an idea for an evaluation function we could use:</p>
        <div class="ln-center">
          <p>`f_2(n)=max(2g(n), g(n)+h(n))`</p>
        </div>
        <p>This function guarantees that a node with `g(n) gt C^(**)/2` will never be expanded. If `g(n) gt C^(**)/2`, then `2g(n) gt C^(**)`.</p>
        <ul>
          <li>if `2g(n) ge g(n)+h(n)`, then `f_2(n)=2g(n) gt C^(**)`</li>
          <li>if `2g(n) lt g(n)+h(n)`, then `f_2(n)=g(n)+h(n) gt 2g(n) gt C^(**)`</li>
        </ul>
        <p>So `f_2(n) gt C^(**)` for nodes with `g(n) gt C^(**)/2`. But nodes on optimal paths will have `f_2(n) le C^(**)`, so those will be expanded first.</p>
      </div>
      <div class="ln-box">
        <p>Here's another example of greedy search, A* search, and RBFS from the textbook.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/map.png">
        </div>
        <p>The map uses straight lines, but it's just a drawing. The roads aren't necessarily straight lines.</p>
        <div class="row">
          <div class="col-sm">
            <table class="table">
              <tr>
                <th>City</th>
                <th>Straight-line distance to B</th>
              </tr>
              <tr>
                <td>A</td>
                <td>366</td>
              </tr>
              <tr>
                <td>B</td>
                <td>0</td>
              </tr>
              <tr>
                <td>C</td>
                <td>160</td>
              </tr>
              <tr>
                <td>D</td>
                <td>242</td>
              </tr>
              <tr>
                <td>E</td>
                <td>161</td>
              </tr>
              <tr>
                <td>F</td>
                <td>176</td>
              </tr>
              <tr>
                <td>G</td>
                <td>77</td>
              </tr>
              <tr>
                <td>H</td>
                <td>151</td>
              </tr>
              <tr>
                <td>I</td>
                <td>226</td>
              </tr>
              <tr>
                <td>L</td>
                <td>244</td>
              </tr>
            </table>
          </div>
          <div class="col-sm">
            <table class="table">
              <tr>
                <th>City</th>
                <th>Straight-line distance to B</th>
              </tr>
              <tr>
                <td>M</td>
                <td>241</td>
              </tr>
              <tr>
                <td>N</td>
                <td>234</td>
              </tr>
              <tr>
                <td>O</td>
                <td>380</td>
              </tr>
              <tr>
                <td>P</td>
                <td>100</td>
              </tr>
              <tr>
                <td>R</td>
                <td>193</td>
              </tr>
              <tr>
                <td>S</td>
                <td>253</td>
              </tr>
              <tr>
                <td>T</td>
                <td>329</td>
              </tr>
              <tr>
                <td>U</td>
                <td>80</td>
              </tr>
              <tr>
                <td>V</td>
                <td>199</td>
              </tr>
              <tr>
                <td>Z</td>
                <td>374</td>
              </tr>
            </table>
          </div>
        </div>
        <h3>Greedy best-first search</h3>
        <p>Let's say we're starting at `A` and we want to go to `B`. If we use the <b>straight-line distance</b> heuristic, then we get the path `A ubrace(rarr)_(140) S ubrace(rarr)_(99) F ubrace(rarr)_(211) B`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/greedy_best_first_search.gif">
          <p>The numbers are the straight-line distances from that node to `B`.</p>
        </div>
        <p>The cost of that path is `450`, but that's not the path with the least cost. The path `A ubrace(rarr)_(140) S ubrace(rarr)_(80) R ubrace(rarr)_(97) P ubrace(rarr)_(101) B` has a lower cost with `418`.</p>
        <h3>A* Search</h3>
        <div class="ln-center">
          <img class="img-fluid" src="../pictures/ai/a_star_search.gif">
        </div>
        <p>Notice that `B` is found when expanding `F`, but since there's a lower cost available to consider, we don't accept the `F rarr B` path as a solution yet.</p>
        <p>A* is efficient because it <b>prunes</b> away nodes that are not necessary for finding an optimal solution. Here, `T` and `Z` are pruned away, but would've been expanded in uniform-cost or breadth-first search.</p>
        <h3>Recursive best-first search</h3>
        <div class="ln-center">
          <img class="img-fluid" src="../pictures/ai/rbfs.gif">
        </div>
        <p>Notice that when `P` is the successor with the lowest `f`-value, `P`'s `f`-value is higher than <code>f_limit</code>. So as we go back up to consider the alternative path, we update `R`'s `f`-value so we can check later if it's worth reexpanding `R`. (The same thing happens for `F`.) The `f`-value that is used as replacement is called the <b>backed-up value</b>, and it's the best `f`-value of the node's children.</p>
        <p>RBFS looks similar to A* search in that it looks for the lowest `f`-value. The difference is that RBFS only looks at the `f`-values of the current node's successors while A* search looks at all the nodes in the frontier.</p>
      </div>
      <h2 id="heuristicfunctions">Heuristic Functions</h2>
      <p>Here, we'll look at how the accuracy of a heuristic affects search performance and how we can come up with heuristics in the first place. We'll use the 8-puzzle as our main example. The 8-puzzle is a `3xx3` sliding-tile puzzle with `8` tiles and `1` blank space. We move tiles horizontally and vertically into the blank space until the goal state is reached (each move has a cost of `1`).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/8_puzzle.png">
        <p>The shortest solution has `26` moves.</p>
      </div>
      <p>Two commonly-used heuristics are</p>
      <ul>
        <li>`h_1=` the number of misplaced tiles (blank space not included)
          <ul>
            <li>for our example, `h_1=8` because all `8` tiles are misplaced</li>
            <li>this is admissible because the number of moves needed to reach a goal state will always be `ge` the number of misplaced tiles
              <ul>
                <li>if all `8` misplaced tiles need just `1` move to get to the correct spot, then the estimated cost is still not overestimated</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>`h_2=` the sum of the distances of the tiles from their goal positions
          <ul>
            <li>for our example, `h_2=3+1+2+2+2+3+3+2=18`
              <ul>
                <li>`1` is `3` squares away from its destination, `2` is `1` square away from its destination, `3` is `2` squares away from its destination, ...</li>
              </ul>
            </li>
            <li>this is sometimes called the <b>city-block distance</b> or <b>Manhattan distance</b></li>
            <li>this is admissible because the calculation of the distance doesn't take obstacles into account
              <ul>
                <li>`8` is `2` squares away from its destination, but it may actually need `ge 2` moves</li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
      <h3>The effect of heuristic accuracy on performance</h3>
      <p>One way to assess the quality of a heuristic is to look at the <b>effective branching factor</b> `b^(**)`. It can loosely be thought of as the average number of nodes that are generated at each depth. A good heuristic would have `b^(**)` close to `1`, which means that not a lot of nodes are being generated the farther we go down a path (we're fairly successful in finding good paths).</p>
      <div class="ln-box">
        <p>If A* finds a solution at depth `5` using `52` nodes, then the effective branching factor is `1.92`.</p>
      </div>
      <p>It turns out that `h_2` (the Manhattan distance) is always better than `h_1` (the number of misplaced tiles). This means that `h_2` <b>dominates</b> `h_1`. Because of this, A* search using `h_2` will never expand more nodes than A* using `h_1` (except for when there are ties).</p>
      <div class="ln-box">
        <p>Earlier we saw that every node with `f(n) lt C^(**)` will surely be expanded (assuming consistent heuristic).</p>
        <div class="ln-center">
          <p>`f(n) lt C^(**)`</p>
          <p>`g(n)+h(n) lt C^(**)`</p>
          <p>`h(n) lt C^(**)-g(n)`</p>
        </div>
        <p>So every node with `h(n) lt C^(**)-g(n)` will surely be expanded. By definition, `h_2(n) ge h_1(n)`, so every node that is surely expanded by A* with `h_2` is also surely expanded by A* with `h_1`. Mathematically, this is because `h_1(n) le h_2(n) lt C^(**)-g(n)`. But because `h_1` is small, there are more nodes that are `lt C^(**)-g(n)`, so A* with `h_1` will expand other nodes that A* with `h_2` wouldn't have expanded.</p>
        <p>So it's generally better to use a heuristic function with higher values, assuming it's consistent and it's not too computationally intensive.</p>
      </div>
      <h3>Generating heuristics from relaxed problems</h3>
      <p>So how do we come up with heuristics like `h_1` and `h_2` in the first place? (Perhaps more importantly, is it possible for a computer to come up with such a heuristic?)</p>
      <p>Let's make the 8-puzzle simpler so that tiles can be moved anywhere, even if there's another tile occupying the spot. Notice now that `h_1` would give the exact length of the shortest solution.</p>
      <p>Let's consider another simplified version where the tiles can still only move `1` square but, now, in any direction, even if the square is occupied. Notice that `h_2` would give the exact length of the shortest solution.</p>
      <p>These simplified versions, where there are fewer restrictions on the actions, are <b>relaxed problems</b>. The state-space graph of a relaxed problem is a supergraph of the original state space because the removal of restrictions allows for more possible states, resulting in more edges being added. This means that an optimal solution in the original problem is also an optimal solution in the relaxed problem. But the relaxed problem may have better solutions (this happens when the added edges provide shortcuts).</p>
      <p>That last part is how we can come up with heuristics. The cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem. (Also, if the heuristic is an exact cost for the relaxed problem, it must obey the triangle inequality, making it consistent.)</p>
      <p>So how can we come up with relaxed problems systematically? Let's describe the 8-puzzle actions as follows: a tile can move from square `X` to square `Y` if `X` is adjacent to `Y` and `Y` is blank. From this, we can generate three relaxed problems simply by removing one or both of the conditions:</p>
      <ul>
        <li>A tile can move from square `X` to square `Y` if `X` is adjacent to `Y`</li>
        <li>A tile can move from square `X` to square `Y` if `Y` is blank</li>
        <li>A tile can move from square `X` to square `Y`</li>
      </ul>
      <p>From the first one, we can derive `h_2` (the Manhattan distance) and from the third one, we can derive `h_1` (the number of misplaced tiles).</p>
      <div class="ln-box">
        <p>Ideally, relaxed problems generated by this technique should be solveable without search.</p>
      </div>
      <div class="ln-box">
        <p>If we generate a bunch of relaxed problems, we now also have a bunch of heuristics. If none of them are much better than the others, then which one should be used? One thing we can do is to pick the heuristic that gives the highest value:</p>
        <div class="ln-center">
          <p>`h(n)=max{h_1(n), ..., h_k(n)}`</p>
        </div>
        <p>But if it takes too long to compute all the heuristics, then we can also randomly pick one or use machine learning to predict which heuristic will be the best.</p>
      </div>
      <h3>Generating heuristics from subproblems: Pattern databases</h3>
      <p>Let's consider a <b>subproblem</b> of the 8-puzzle, where the goal is to get only tiles `1,2,3,4,` and the blank space into the correct spots.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/8_puzzle_subproblem.png">
      </div>
      <p>Since there are fewer tiles that have to be in a certain spot, the cost of the optimal solution of this subproblem is a lower bound on the cost of the complete problem, meaning that this could make a good admissible heuristic.</p>
      <p>For every possible subproblem, we can store the exact costs of their solutions in a <b>pattern database</b>. So when we're trying to solve the complete problem, we could look in our database for the subproblem that matches our current state and use the exact cost of the subproblem as the heuristic for the complete problem.</p>
      <p>We could have multiple databases for each subproblem type. For example, one database could be all the subproblem instances with tiles `1,2,3,4`. Another database could be all the subproblem instances with tiles `5,6,7,8`. Another database could be all the subproblem instances with tiles `2,4,6,8`. And so on. Then the heuristics from these databases can be combined by taking the maximum value. Doing this turns out to be more accurate than the Manhattan distance.</p>
      <h3>Generating heuristics with landmarks</h3>
      <p>This is kinda similar to using pattern databases, but kinda makes more sense when viewed in the context of route-finding problems.</p>
      <p>We can pick a few <b>landmark points</b> and <b>precompute</b> the optimal costs to these landmarks. More formally, for each landmark `L` and for each other vertex `v` in the graph, we compute and store `C^(**)(v,L)`. This may sound time-consuming, but it only needs to be done once and it will be infinitely useful.</p>
      <p>So if we're trying to estimate the cost to the goal, we can look at the cost to get there from a landmark.</p>
      <div class="ln-box">
        <p>The heuristic would be</p>
        <div class="ln-center">
          <p>`h_L(n)=min_(L in text(landmarks)) C^(**)(n,L)+C^(**)(L,text(goal))`</p>
        </div>
        <p>This heuristic is inadmissible though. If the optimal path is through a landmark, then this heuristic will be exact. If it isn't, then it will overestimate the optimal cost (the exact cost of a non-optimal path is greater than the exact cost of an optimal path).</p>
      </div>
      <div class="ln-box">
        <p>However, we can come up with an admissible heuristic:</p>
        <div class="ln-center">
          <p>`h_(DH)(n)=max_(L in text(landmarks)) abs(C^(**)(n,L)-C^(**)(L,text(goal)))`</p>
        </div>
        <p>This is a <b>differential heuristic</b> (differential because of the subtraction). To understand it intuitively, consider a landmark that is beyond the goal. If the goal happens to be on the optimal path to the landmark, then the path `n rarr text(goal) rarr L` minus the path `text(goal) rarr L` gives us the optimal path `n rarr text(goal)`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/differential_heuristic.png">
        </div>
        <p>If the goal is not on the optimal path to the landmark, then we would be subtracting by a larger value, resulting in a cost `lt` the optimal cost, i.e., the cost is underestimated.</p>
        <p>Landmarks that are not beyond the goal will already underestimate the cost to the goal, but these landmarks are not likely to be considered since we're selecting the max.</p>
      </div>
      <p>A good way to decide landmark points are by looking at user requests for frequently requested landmarks.</p>
      <h3>Learning heuristics from experience</h3>
      <p>Sometimes, we can just develop an intuition for something if we do it many times. From the perspective of computers, they can pick up on patterns in past data and come up with a heuristic that seems to align with those patterns. A lot of this seems to depend on luck, so the heuristics are not expected to be admissible (much less consistent).</p>
      <div class="ln-box">
        <p>For example, the number of misplaced tiles might be helpful in predicting the actual cost. For further example, we might find that when there are `5` misplaced tiles, the average solution cost is around `14`.</p>
        <p>Another feature that might be useful is the number of pairs of adjacent tiles that are not adjacent in the goal state.</p>
        <p>If we have multiple features, we can combine them to generate a heuristic by using a linear combination of them:</p>
        <div class="ln-center">
          <p>`h(n)=c_1x_1(n)+c_2x_2(n)+...+c_kx_k(n)`</p>
          <p>where each `x_i` is a feature</p>
        </div>
        <p>Machine learning would be used to figure out the best values for the constants.</p>
      </div>
      <div class="ln-box">
        <h3>Summary of Chapter 3</h3>
        <ul>
          <li>A well-defined search problem consists of five parts: the <b>initial state</b>, a set of <b>actions</b>, a <b>transition model</b> that describes the results of those actions, a set of <b>goal states</b>, and an <b>action cost function</b>.</li>
          <li>The environment of the problem is represented by a <b>state space graph</b>. A <b>path</b> through the state space from the initial state to a goal state is a <b>solution</b>.</li>
          <li><b>Uninformed search</b> methods have access only to the problem definition. Algorithms build a search tree to find a solution but differ based on which node they expand first.
            <ul>
              <li><b>Best-first search</b> expands the node with the lowest value given by an <b>evaluation function</b>.</li>
              <li><b>Breadth-first search</b> expands the shallowest nodes first. It is complete, optimal for unit action costs, but has exponential space complexity.</li>
              <li><b>Uniform-cost search</b> expands the node with the lowest path cost, `g(n)`, and is optimal for general action costs.</li>
              <li><b>Depth-first search</b> expands the deepest unexpanded node first. It is neither complete nor optimal, but has linear space complexity. <b>Depth-limited search</b> adds a depth bound.</li>
              <li><b>Iterative deepening search</b> calls depth-first search with increasing depth limits until a goal is found. It is complete when full cycle checking is done, optimal for unit action costs, has exponential time complexity, and has linear space complexity.</li>
              <li><b>Bidirectional search</b> expands two frontiers, one around the initial state and one around the goal, stopping when the two frontiers meet.</li>
            </ul>
          </li>
          <li><b>Informed search</b> methods have access to a <b>heuristic</b> function `h(n)` that estimates the cost of a solution from `n`.
            <ul>
              <li><b>Greedy best-first search</b> expands the node with the lowest value of `h(n)`. It is not optimal but is often efficient.</li>
              <li><b>A* search</b> expands the node with the lowest value of `f(n)=g(n)+h(n)`. It is complete and optimal (if `h(n)` is admissible), but uses up a lot of space.</li>
              <li><b>IDA*</b> (iterative deepening A* search) is an iterative deepening version of A* that addresses the space complexity issue.</li>
              <li><b>RBFS</b> (recursive best-first search) is a robust, optimal search algorithm that uses limited amounts of memory.</li>
              <li><b>Beam search</b> puts a limit on the size of the frontier. This makes it incomplete and suboptimal, but it often finds reasonably good solutions and is generally faster than complete searches.</li>
              <li><b>Weighted A* search</b> expands fewer nodes but sacrifices optimality.</li>
            </ul>
          </li>
          <li>The performance of heuristic search algorithms depends on the quality of the heuristic function.</li>
          <li>Good heuristics can be obtained by relaxing the problem definition, by storing precomputed solution costs for subproblems in a pattern database, by defining landmarks, or by learning from experience.</li>
        </ul>
      </div>
      <hr>
      <p>Earlier, we assumed that the environments are episodic, single agent, fully observable, deterministic, static, discrete, and known. Now we'll start relaxing these constraints to get closer to the real world.</p>
      <hr>
      <h2 id="localsearch">Local Search and Optimization Problems</h2>
      <p>There are some search problems where we only care about what a goal state looks like (and we don't care what the path looks like). For example, the 8-queens problem is to find a valid placement of 8 queens such that no queens are attacking each other. Once we know what a solution looks like, it's trivial to reconstruct the steps to get there.</p>
      <p><b>Local search</b> algorithms work without keeping track of the paths or the states that have been reached. This means that they don't explore the whole search space systematically, i.e., they are not complete and may not find a solution. However, they use very little memory and can find reasonable solutions in problems with large or infinite state spaces.</p>
      <p>Local search also works for <b>optimization problems</b>, where the goal is to find the best state according to some <b>objective function</b>. The objective function represents what we're trying to maximize/minimize.</p>
      <h3>Hill-climbing search</h3>
      <p>Let's take a look at a sample <b>state-space landscape</b>.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/state_space_landscape.png">
      </div>
      <p>One goal might be to find the highest peak, which is the <b>global maximum</b>. This process is called <b>hill climbing</b>.</p>
      <div class="ln-box">
        <p>If the goal is to find the lowest valley (<b>global minimum</b>), then the process is called <b>gradient descent</b>.</p>
      </div>
      <p>The hill climbing search algorithm is pretty simple. It keeps moving forward until the next state it finds has a smaller value than the value of the current state.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function HILL-CLIMBING(problem)<br>&nbsp;&nbsp;current = problem.INITIAL<br>&nbsp;&nbsp;while true do<br>&nbsp;&nbsp;&nbsp;&nbsp;neighbor = a highest-valued successor state of current<br>&nbsp;&nbsp;&nbsp;&nbsp;if VALUE(neighbor) &leq; VALUE(current) then return current<br>&nbsp;&nbsp;&nbsp;&nbsp;current = neighbor</code></p>
        </div>
        <p>This particular version of hill climbing is <b>steepest-ascent hill climbing</b> because it heads in the direction of the steepest ascent.</p>
      </div>
      <div class="ln-box">
        <p>Hill climbing is sometimes also called <b>greedy local search</b> because it gets a good neighbor state without thinking ahead about where to go next.</p>
      </div>
      <p>There are some situations where the hill climbing algorithm can get stuck before finding a solution. One situation is when a <b>local maximum</b> is reached. In this situation, every possible move results in a worser state.</p>
      <p>Another situation is when a <b>plateau</b> is reached. A plateau can be a flat local maximum or a <b>shoulder</b>, where there is an uphill exit. In this situation, every possible move doesn't result in a better state (except if the plateau is a shoulder and we're right at the edge of the uphill exit).</p>
      <p>One way to get unstuck is to allow <b>sideways moves</b>, i.e., to allow the algorithm to keep moving as long as the next state isn't worse. The idea is to hope that the plateau is a shoulder. This won't work if the plateau is a local maximum though, in which case we really are stuck.</p>
      <p>Another way (kinda) is to just start over, but from a different starting point. This is <b>random-restart hill climbing</b>.</p>
      <div class="ln-box">
        <p>Interestingly, random-restart hill climbing is complete with probability `1` because it will eventually generate a goal state as the initial state.</p>
      </div>
      <div class="ln-box">
        <p><b>Stochastic hill climbing</b> chooses a random uphill move instead of the steepest uphill move.</p>
        <p><b>First-choice hill climbing</b> generates successors randomly until one of them is better than the current state.</p>
      </div>
      <h3>Simulated annealing</h3>
      <div class="ln-box">
        <p>Annealing is the process of hardening metals and glass by heating them to a high temperature and then gradually cooling them.</p>
      </div>
      <p>Sometimes, it's necessary to make worse moves that put us in a temporarily worse state so that we can get to a better position.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function SIMULATED_ANNEALING(problem, schedule)<br>&nbsp;&nbsp;current = problem.INITIAL<br>&nbsp;&nbsp;for t = 1 to infinity do<br>&nbsp;&nbsp;&nbsp;&nbsp;T = schedule(t)<br>&nbsp;&nbsp;&nbsp;&nbsp;if T = 0 then return current<br>&nbsp;&nbsp;&nbsp;&nbsp;next = a randomly selected successor of current<br>&nbsp;&nbsp;&nbsp;&nbsp;&Delta;E = VALUE(current) - VALUE(next)<br>&nbsp;&nbsp;&nbsp;&nbsp;if &Delta;E &gt; 0 then current = next<br>&nbsp;&nbsp;&nbsp;&nbsp;else current = next only with probability</code> `e^(-DeltaE//T)`</p>
        </div>
        <p><code>T</code> represents the "temperature", corresponding to the temperature described in annealing.</p>
      </div>
      <p>In the simulated-annealing algorithm, we pick moves randomly instead of picking the best move. If the move puts us in a better situation, then we'll accept it. But if the move puts us in a worse position, then we'll consider it, depending on how bad of a move it is (measured by `DeltaE`). And as time goes on, we'll be less accepting of bad moves.</p>
      <div class="ln-box">
        <p>It's called simulated annealing because the algorithm starts with a high "temperature", meaning that there's a high level of randomness in exploring the search space. As time goes on, the "temperature" gets lower, reducing the random moves.</p>
      </div>
      <h3>Local beam search</h3>
      <p><b>Local beam search</b> starts with `k` randomly generated states, generates all their successors (if one of them is a goal, then we're done), selects the best `k` successors, and repeats.</p>
      <div class="ln-box">
        <p>Local beam search kinda looks like running `k` random-restart searches in parallel. However, it actually isn't. All random-restart searches are independent of each other, while each local beam search thread passes information to each other.</p>
      </div>
      <h3>Evolutionary algorithms</h3>
      <p><b>Evolutionary algorithms</b> are based on the ideas of natural selection in biology. There is a population of individuals (states), and the fittest (highest valued) individuals produce offspring (successor states). And just like in real life, the successor states contain properties of the parent states that produced them.</p>
      <p>The process of combining the parent states to produce successor states is called <b>recombination</b>. One common way of implementing this is to split each parent at a <b>crossover point</b> and use the four resulting pieces to form two children.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/evolutionary_algorithm.png">
        <p>The numbers in the fitness function are sample fitness scores assigned by the fitness function to each state. The fitness scores are then converted to a probability.</p>
      </div>
      <p>Below is an example of how an evolutionary algorithm can be used for the 8-queens problem. We can encode each queen's positions as a number from `1-8`.</p>
      <div class="ln-center">
        <div class="row">
          <div class="col-sm">
            <img class="img-fluid" src="../pictures/ai/evolutionary_algorithm_chess_1.png">
            <p>`327|52411`</p>
          </div>
          <div class="col-sm">
            <img class="img-fluid" src="../pictures/ai/evolutionary_algorithm_chess_2.png">
            <p>`247|48552`</p>
          </div>
          <div class="col-sm">
            <img class="img-fluid" src="../pictures/ai/evolutionary_algorithm_chess_3.png">
            <p>`327|48552`</p>
          </div>
        </div>
        <p>If we use nonattacking queens as the fitness function, then the sample fitness scores are the number of nonattacking pairs of queens.</p>
      </div>
      <h2 id="localsearchincontinuousspaces">Local Search in Continuous Spaces</h2>
      <p>Most real-world problems take place in a continuous environment, not in a discrete one like the examples we've seen so far. A continuous environment can have a continuous action space (e.g., a robotic arm moving anywhere between `0` and `180` degrees), a continuous state space (e.g., the current position of a robot in a room), or both.</p>
      <div class="ln-box">
        <p>A continuous action space has an infinite branching factor, so most of the algorithms we've seen so far won't work (except for first-choice hill climbing and simulated annealing).</p>
      </div>
      <p>One way to deal with a continuous state space is to <b>discretize</b> it.</p>
      <p>For example, suppose we want to place three new airports in an area such that each city is as close as possible to its nearest airport. The state space is defined by the coordinates of the three airports: `(x_1, y_1),(x_2, y_2),(x_3, y_3)`. To discretize this, we could predefine fixed points on a grid and only allow the airports to be at those fixed points.</p>
      <p>One way to deal with a continuous action space is to choose successor states randomly by moving in a random direction by a small amount. As we move, we look at how the value of the objective function changes to see if we're going in the right direction. This is <b>empirical gradient</b> search.</p>
      <div class="ln-box">
        <p>Steepest-ascent hill climbing can be seen as a specific case of empirical gradient search when the state space is discretized.</p>
      </div>
      <div class="ln-box">
        <p>The objective function for any particular state is</p>
        <div class="ln-center">
          <p>`f(x_1, y_1, x_2, y_2, x_3, y_3) = sum_(i=1)^3sum_(c in C_i)(x_i-x_c)^2+(y_i-y_c)^2`</p>
        </div>
        <p>Note that this objective function is only correct locally, not globally. As the location of an airport moves, the closest cities to that airport change and have to be recomputed.</p>
      </div>
      <div class="ln-box">
        <p>As in the example above, the objective function can often be expressed in a math equation. This means that we can use calculus to find the steepest ascent rather than moving randomly to try and find it.</p>
        <p>For the above objective function, the gradient is calculated by</p>
        <div class="ln-center">
          <p>`grad f = ((del f)/(del x_1),(del f)/(del y_1),(del f)/(del x_2),(del f)/(del y_2),(del f)/(del x_3),(del f)/(del y_3))`</p>
        </div>
      </div>
      <h2 id="nondeterministicsearch">Search with Nondeterministic Actions</h2>
      <p>Now, we'll look at nondeterministic environments. When the environment is nondeterministic, the agent doesn't know for sure what state it will end up in after performing an action. So the agent will be thinking something like, "If I do action `a` I'll end up in state `s_2`, `s_4`, or `s_5`. The states that the agent believes are possible are called <b>belief states</b>.</p>
      <p>Instead of a sequence of states, the solution to a problem is a <b>conditional plan</b> that specifies what to do depending on the situation.</p>
      <h3>The erratic vacuum world</h3>
      <p>We're going back to our vacuum world! But this time, our vacuum is a little unpredictable. When it cleans a dirty square, it can sometimes clean an adjacent square as well. If it cleans a clean square, it can sometimes deposit dirt on it.</p>
      <h3>AND-OR search trees</h3>
      <div class="ln-center">
        <img class="img-fluid" src="../pictures/ai/and_or_search_tree.png">
        <p>Yes, the red oval is our vacuum cleaner. The states are labeled from `1-8`.</p>
      </div>
      <p>Notice that there's no single sequence of actions that can solve the problem. But this conditional plan does:</p>
      <div class="ln-center">
        <p>[Clean, if State == 5 then [Right, Clean] else []]</p>
      </div>
      <p>As the name suggests, an AND-OR search tree has OR nodes and AND nodes. <b>OR nodes</b> (represented as rectangles in the example above) are thought of as, "at this node, I can do this <em>or</em> that". <b>AND nodes</b> (represented as circles) are thought of as, "at this node, this <em>and</em> this can happen".</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function AND-OR-SEARCH(problem)<br>&nbsp;&nbsp;return OR-SEARCH(problem, problem.INITIAL, [])</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function OR-SEARCH(problem, state, path)<br>&nbsp;&nbsp;if problem.IS-GOAL(state) then return empty plan<br>&nbsp;&nbsp;if IS-CYCLE(path) return faliure<br>&nbsp;&nbsp;for each action in problem.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;plan = AND-SEARCH(problem, RESULTS(state, action), [state] + path)<br>&nbsp;&nbsp;&nbsp;&nbsp;if plan != failure then return [action] + plan<br>&nbsp;&nbsp;return failure</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function AND-SEARCH(problem, states, path)<br>&nbsp;&nbsp;for each s_i in states do<br>&nbsp;&nbsp;&nbsp;&nbsp;plan_i = OR-SEARCH(problem, s_i, path)<br>&nbsp;&nbsp;&nbsp;&nbsp;if plan_i == failure then return failure<br>&nbsp;&nbsp;return [if s_1 then plan_1 else if s_2 then plan_2 else ...if s_n-1 then plan_n-1 else plan_n]</code></p>
        </div>
        <p>This is a recursive, depth-first algorithm for AND-OR graph search.</p>
        <p>One thing to note is that we're returning failure when there's a cycle. This doesn't mean that a cycle implies no solution. It means that if there is a solution, it should exist on a path before the cycle, so there's no point in continuing to explore this path. This also ensures that the algorithm doesn't run infinitely.</p>
      </div>
      <h3>Try, try again</h3>
      <p>Forget our erratic vacuum for a moment. Let's go back to our normal vacuum, but this time, the floor is slippery. So sometimes, when the vacuum tries to move, it stays in place instead.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/cyclic_search_tree.png">
      </div>
      <p>Here, if we're in state `5` and the vacuum moves right, it can either move right or stay in place. If it stays in place, we still want it to move right. So the solution here is to keep moving right until it works.</p>
      <div class="ln-center">
        <p>[Clean, while State == 5 do Right, Clean]</p>
      </div>
      <p>This is a cyclic plan that happens to also be a solution, i.e., a <b>cyclic solution</b>.</p>
      <h2 id="partiallyobservablesearch">Search in Partially Observable Environments</h2>
      <p>Now, we'll look at what happens when the environment is partially observable. When the environment is partially observable, the agent doesn't know for sure what state it's in. So the agent will be thinking something like, "I'm either in state `s_1` or `s_3`".</p>
      <h3>Searching with no observation</h3>
      <p>If the agent has no sensors at all, then the problem that the agent is trying to solve is called a <b>sensorless problem</b>. It might seem impossible for an agent to be able to do anything without sensors, but, in fact, there are sensorless solutions that are common and useful because they don't rely on sensors working properly.</p>
      <p>So let's go back to when the vacuum was working normally. Except, the vacuum now has no sensors, so it doesn't know its own location or which squares have dirt (including the square it's currently on). But let's say the vacuum knows what all the possible states look like.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/vacuum_states.png">
      </div>
      <p>When it starts out, the vacuum's initial belief state is `{1, 2, 3, 4, 5, 6, 7, 8}`. No matter what state it's in, if the vacuum moves right, then it will be in one of the states `{2, 4, 6, 8}`. After cleaning, it will be in `{4,8}`. Then after moving left and cleaning, it will be in state `7`. So no matter what state the vacuum starts in, we can <b>coerce</b> the world into state `7` with [Right, Clean, Left, Clean].</p>
      <p>The solution to a sensorless problem is a sequence of actions, not a conditional plan. This is because the agent can't sense anything, and thus, can't tell when it needs to go with another plan.</p>
      <p>So the solutions can be found by using the informed and uninformed search algorithms from before. We just have to search in the space of belief states instead of physical states.</p>
      <div class="ln-box">
        <p>In belief-state space, the problem is fully observable because combining all the information about what states the agent could be in provides the agent with a complete picture of the environment.</p>
      </div>
      <p>If we transform a physical problem `P` (with `text(Actions)_P`, `text(Result)_P`) into a belief-state problem, we have:</p>
      <ul>
        <li>states: every possible subset of physical states
          <ul>
            <li>if `P` has `N` states, then the belief-state problem has `2^N` belief states
              <ul>
                <li>note that this is not saying that a belief state has `2^N` physical states</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>initial state: usually, the belief state consisting of all states in `P`</li>
        <li>goal test: all states in the current belief state must be a goal state</li>
        <li>action cost: to simplify things, we'll assume the cost of an action is the same in all states</li>
      </ul>
      <p>Defining the actions is a little more involved. A belief state can have multiple physical states, so there can be multiple actions available from each state. Inherently, there can be some actions that are possible in some of the states but not others. If illegal actions don't have an effect on the environment, then we can combine all the actions from each state and say that those are the available actions for the current belief state. Otherwise, we can find the actions that are common in all the states and say that those are the available actions.</p>
      <p>For example, in our vacuum world, let's say the belief state is `{s_1, s_2}`. All the possible actions from both states are to move left, move right, and clean. However, it's not really useful for a vacuum in `s_1` to move left. We've been assuming that if the vacuum can't move in a certain direction, it just stays still. So in this case, it's okay to combine all the actions and say that the available actions for `{s_1,s_2}` are to move left, move right, and clean.</p>
      <p>On the other hand, if the vacuum can fall off the edge, then we don't want it to move left if it's in `s_1`. Likewise, we don't want it to move right if it's in `s_2`. The only action in common from all states is to clean, so the available actions for the belief state in this case would be to just clean.</p>
      <div class="ln-box">
        <p>`text(Actions)(b)=uuu_(s in b) text(Actions)_P(s)` or `text(Actions)(b)=nnn_(s in b) text(Actions)_P(s)` depending on which makes sense.</p>
      </div>
      <p>Defining the transition model is also a little more involved depending on whether the actions are deterministic or not. For deterministic actions, there is one result state for each of the states in the belief state. For nondeterministic actions, it's the combination of all the possible result states for each of the states.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_deterministic.png">
        <p>The transition model for moving right in the normal vacuum world.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_nondeterministic.png">
        <p>The transition model for moving right in the slippery vacuum world.</p>
      </div>
      <div class="ln-box">
        <p>deterministic: `b' = text(Result)(b,a)={s' : s' = text(Result)_P(s,a) and s in b}`</p>
        <p>nondeterministic: `b' = text(Result)(b,a)={s' : s' in text(Results)_P(s,a) and s in b} = uuu_(s in b)text(Results)_P(s,a)`</p>
      </div>
      <p>By transforming a physical problem into a belief-state problem with these definitions, we can use the informed and uninformed search algorithms to find a solution. The operative word being "can". As mentioned earlier, there are a total of `2^N` possible belief states to search through. Furthermore, a belief state can have up to `N` states, so it can be hard to store multiple belief states in memory.</p>
      <p>An alternative is to use <b>incremental belief-state search</b> algorithms. For example, if the initial belief state is `{1,2,3,4,5,6,7,8}`, we can start by finding a solution for state `1` and see if it works for every other state. If it doesn't, then we go back to state `1`, find another solution, and repeat until the solution works for all `8` states.</p>
      <h3>Searching in partially observable environments</h3>
      <p>Now we'll upgrade our vacuum to have local sensors that can tell it which square it's on and whether it's clean or dirty. Suppose the vacuum gets the information [L, Dirty] from its sensors. From this, the vacuum can either be in state `1` or state `3`. So it's belief state will be `{1,3}`.</p>
      <p>The transition model between belief states can be seen as a three-stage process:</p>
      <ul>
        <li><b>prediction</b> stage: see what states are possible from performing on action on the current belief state</li>
        <li><b>possible percepts</b> stage: see what percepts are possible from each of the states</li>
        <li><b>update</b> stage: compute the belief states that are possible based on the possible percepts</li>
      </ul>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_deterministic_partially_observable.png">
        <p>The transition model for moving right in the normal (local sensing) vacuum world.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_nondeterministic_partially_observable.png">
        <p>The transition model for moving right in the slippery (local sensing) vacuum world.</p>
      </div>
      <div class="ln-box">
        <p>prediction stage: `hat b = text(Result)(b,a)` (the `text(Result)` function can also be called `text(Predict)`)</p>
        <p>possible percepts stage: `text(Possible-Percepts)(hat b) = {o : o = text(Percept)(s) and s in hat b}`</p>
        <p>update stage: `text(Update)(hat b,o) = {s : o = text(Percept)(s) and s in hat b}`</p>
        <p>transition model: `text(Results)(b,a) = {b_o : b_o = text(Update)(text(Predict)(b,a), o) and o in text(Possible-Percepts)(text(Predict)(b,a))}`</p>
      </div>
      <h3>Solving partially observable problems</h3>
      <p>Because the current belief state can yield multiple belief states as the next possible states, we can treat the multiple belief states as if they were the result of a nondeterministic action and, therefore, use the AND-OR search algorithm. As a result, the solution is a conditional plan. For example, [Clean, Right, if state == 6 then Clean else []] is a solution to the search tree below.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/and_or_partially_observable.png">
        <p>Remember that the transition model returns one belief state per possible percept.</p>
      </div>
      <h2 id="onlinesearch">Online Search Agents and Unknown Environments</h2>
      <p>In this context, online isn't referring to the Internet. An <b>online search</b> agent makes an action, observes the environment, and computes the next action. As opposed to an <b>offline search</b> agent that computes a complete solution first before taking any action. Mostly everything we've seen so far has been offline search algorithms.</p>
      <p>Online search is good for (semi-)dynamic environments where the agent can't sit around doing nothing for too long. It's also good for nondeterministic environments because it allows the agent to focus on the percepts that it actually receives rather than the percepts that it can receive.</p>
      <p>On the flip side, more planning reduces the chances of things going wrong.</p>
      <hr>
        <p>We'll briefly return to deterministic and fully observable environments here.</p>
      <hr>
      <h3>Online search problems</h3>
      <p>The agent only knows the actions that are available in the current state, the cost of going to the next state (once the next available states have been computed), and whether or not the current state is a goal state. The agent cannot know the result of going to another state until it's in that state. The agent might have access to a heuristic function.</p>
      <p>An online algorithm can only generate successors for the state that it's currently in. On the other hand, offline algorithms, such as A* search, can expand a node in one part of the space and then immediately expand a node in a distant part of the space.</p>
      <p>Online explorers are vulnerable to <b>dead ends</b>, which are states from which no goal state is reachable. Dead ends are a result of <b>irreversible</b> actions, i.e., actions that prevent returning to the previous state.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/dead_end.png">
      </div>
      <p>An environment with no dead ends is <b>safely explorable</b>, which means that a goal state is reachable from every reachable state. Any environment with only reversible actions are safely explorable.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function ONLINE-DFS-AGENT(problem, s')<br>&nbsp;&nbsp;if problem.IS-GOAL(s') then return stop<br>&nbsp;&nbsp;if s' is a new state (not in untried) then untried[s'] = problem.ACTIONS(s')<br>&nbsp;&nbsp;if s is not null then <br>&nbsp;&nbsp;&nbsp;&nbsp;result[s,a] = s'<br>&nbsp;&nbsp;&nbsp;&nbsp;add s to the front of unbacktracked[s']<br>&nbsp;&nbsp;if untried[s'] is empty then<br>&nbsp;&nbsp;&nbsp;&nbsp;if unbacktracked[s'] is empty then return stop<br>&nbsp;&nbsp;&nbsp;&nbsp;else a = an action b such that result[s',b] = POP(unbacktracked[s'])<br>&nbsp;&nbsp;else a = POP(untried[s'])<br>&nbsp;&nbsp;s = s'<br>&nbsp;&nbsp;return a</code></p>
        </div>
        <p><code>result</code>: a table mapping `(s,a)` to `s'`</p>
        <p><code>untried</code>: a table mapping `s` to a list of untried actions</p>
        <p><code>unbacktracked</code>: a table mapping `s` to a list of states never backtracked to</p>
      </div>
      <h3>Online local search</h3>
      <p>Since hill-climbing search just keeps the current state in memory, it's actually already an online search algorithm. It's not a very good one though because it can result in agents getting stuck at a local maximum. And random-restart hill climbing is not an option to get unstuck because the agent can't teleport to a new start state.</p>
      <p>A <b>random walk</b> can be used to explore an environment. The agent selects an action at random and performs it.</p>
      <div class="ln-box">
        <p>Random walks are complete when the space is finite and safely explorable.</p>
      </div>
      <p>Instead of telling the agent to do random things, we can have the agent learn about its environment as it explores it. One way to do this is to store a "current best estimate" of the cost to reach the goal from each state that has been visited.</p>
      <div class="ln-box">
        <p>The current best estimate, denoted as `H(s)`, starts out being just the heuristic `h(s)`, but it gets updated as the agent learns more.</p>
      </div>
      <p>Consider this sample state space. `(a),(b),(c),(d),(e)` are five iterations of an algorithm called learning real-time A* (<b>LRTA*</b>) search that implements this current best estimate idea. If we look at `(a)`, we see that the agent is currently stuck in a local minimum.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/lrta_star.png">
        <p>Each state is labeled the current estimated cost to reach a goal (`H(s)`). Every action costs `1`.</p>
        <p>The orange state is the agent's current location. The double-circled states are states that have had their cost estimates updated.</p>
      </div>
      <p>At `(a)`, there are two actions: move left or move right. The estimated cost to reach a goal if we move left is `1+9=10` and the estimated cost to reach a goal if we move right is `1+2=3`. So we move right. Importantly, we also update the previous state's estimated cost with the lowest one (`3` vs `10`).</p>
      <p>At `(b)`, the estimated cost to the goal if we move left is `1+3=4` and the estimated cost to the goal if we move right is `1+4=5`. We move left and update the previous state's estimated cost (`4` vs `5`).</p>
      <p>We can eventually see that we get unstuck (and "flatten out" the local minimum in the process).</p>
      <div class="ln-box">
        <p>LRTA* is complete when the environment is finite and safely explorable. However, unlike A*, it is not complete if the state space is infinite.</p>
      </div>
      <div class="ln-box">
        <h3>Summary of Chapter 4</h3>
        <ul>
          <li>Local search methods such as <b>hill climbing</b> keep only a small number of states in memory. They have been applied to optimization problems, where the idea is to find a high-scoring state without worrying about the path to that state.</li>
          <li><b>Simulated annealing</b> is a stochastic local search algorithm that returns optimal solutions when given an appropriate cooling schedule.</li>
          <li>Local search methods also apply to problems in continuous spaces. For some problems that can be represented mathematically, we can use calculus to find where the gradient is zero (to find the maximum/minimum). For other problems, we can use the empirical gradient instead, which measures the difference in fitness between two nearby points.</li>
          <li>An <b>evolutionary algorithm</b> is a stochastic hill-climbing search in which a population of states generates new states through <b>mutation</b> and <b>crossover</b>, which combines pairs of states.</li>
          <li>In <b>nondeterministic</b> environments, agents can apply AND-OR search to generate <b>contingent</b> plans that reach the goal regardless of what happens during execution.</li>
          <li>When the environment is partially observable, the <b>belief state</b> represents the set of possible states that the agent might be in.</li>
          <li>Standard search algorithms can be used to solve <b>sensorless problems</b>.</li>
          <li>Belief AND-OR search can solve partially observable problems.</li>
          <li><b>Exploration problems</b> arise when the agent has no idea about the states and actions of its environment. For safely explorable environments, <b>online search</b> agents can build a map and find a goal if one exists.</li>
          <li>Updating heuristic estimates from experience is an effective way to escape from local minima.</li>
        </ul>
      </div>
      <hr>
      <p>Now, we'll look at multi-agent environments. In particular, <b>competitive environments</b> where two or more agents have conflicting goals. Problems in these environments are called <b>adversarial search</b> problems.</p>
      <hr>
      <h2 id="gametheory">Game Theory</h2>
      <p>Apparently, chess, Go, and poker are popular games to study in the AI community.</p>
      <h3>Two-player zero-sum games</h3>
      <p>Game theorists call these types of games deterministic, two-player, turn-taking, <b>perfect information</b>, <b>zero-sum games</b>. "Perfect information" means "fully observable" and "zero-sum" means that what is good for one player is just as bad for the other. A <b>move</b> is an "action" and a <b>position</b> is a "state".</p>
      <p>A game can be formally defined by specifying:</p>
      <ul>
        <li>the <b>initial state</b>, which describes how the game is set up at the start
          <ul>
            <li>`S_0`</li>
          </ul>
        </li>
        <li>the player whose turn it is to move in state `s`
          <ul>
            <li>`text(TO-MOVE)(s)`</li>
          </ul>
        </li>
        <li>the set of legal moves in state `s`
          <ul>
            <li>`text(ACTIONS)(s)`</li>
          </ul>
        </li>
        <li>the <b>transition model</b>, which defines the state resulting from taking action `a` in state `s`
          <ul>
            <li>`text(RESULT)(s,a)`</li>
          </ul>
        </li>
        <li>a <b>terminal test</b>, which is true when the game is over
          <ul>
            <li>states where the game has ended are called <b>terminal states</b></li>
            <li>`text(IS-TERMINAL)(s)`</li>
          </ul>
        </li>
        <li>a <b>utility function</b>, which assigns numerical values to the outcome of the game
          <ul>
            <li>in chess, a win is `1`, a loss is `0`, and a draw is `1/2`</li>
            <li>`text(UTILITY)(s,p)`</li>
          </ul>
        </li>
      </ul>
      <div class="ln-box">
        <p>Chess is considered a "zero-sum" game even though the sum of the outcomes is `1`. However, if we imagine that each player "paid `1/2`" to play, then the winner wins `1/2` and the loser loses `1/2`.</p>
      </div>
      <p>Instead of using a <b>search tree</b> like the one we saw earlier, we'll be using a <b>game tree</b>, which takes into account that there are multiple players. And speaking of players, we'll be calling our two players MAX and MIN.</p>
      <p>Here's an example of a (partial) game tree for tic-tac-toe:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/game_tree.png">
      </div>
      <h2 id="optimaldecisionsingames">Optimal Decisions in Games</h2>
      <p>Since each player is responding to each other's moves, the strategy for both players is a conditional plan (if that player does this, I'll do this).</p>
      <div class="ln-box">
        <p>In games that have a binary outcome (e.g., win or lose), we could use AND-OR search to generate the conditional plan. For games with multiple outcomes, we need something a little more general.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/two_ply_game_tree_1.png">
      </div>
      <p>Consider the simple game above where MAX is trying to reach one of the bottom-row nodes with the highest value and MIN is trying to have MAX end up at the node with the lowest value (depending on MAX's first move). MAX moves first and gets to pick a path `a_1`, `a_2`, or `a_3`. Then it is MIN's turn to decide which bottom-row node MAX ends up at.</p>
      <p>For example, MAX might want to reach the node with `14`, so MAX will choose `a_3`. At this point, MIN has three options `d_1`, `d_2`, `d_3`. Since MIN wants MAX to have the lowest score possible, MIN will choose `d_3`, so MAX will end up with a score of `2`.</p>
      <p>That strategy didn't really go so well for MAX, so let's look at another one. This time, MAX wants `12`, so MAX will choose `a_1`. MIN will then choose `b_1` so that MAX ends up with `3`, the lowest out of the three choices.</p>
      <p>We can see that `3` is the highest score MAX can hope to get if MIN plays optimally. And if you took the time to verify that, then you'll notice that you played the game backwards (starting from the bottom and moving up) to check.</p>
      <p>This is essentially the idea behind the <b>minimax search</b> algorithm.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/two_ply_game_tree_2.png">
        <p>`B` has a minimax value of `3` because if MAX chooses `a_1`, the highest score MAX can get is `3` if MIN plays optimally.</p>
      </div>
      <p>Each node is assigned a <b>minimax value</b> that represents how "good" that position/move is for a player, assuming the other player plays optimally. In this game, MAX wants to choose the node with the maximum minimax value and MIN wants to choose the node with the minimum minimax value.</p>
      <p>The minimax value of the terminal nodes are just the values defined by the game's `text(UTILITY)` function (the score we get when the game ends). Then the minimax values are propagated up the game tree using the same reasoning we used to analyze the game above.</p>
      <div class="ln-box">
        <h3>The minimax search algorithm</h3>
        <p>This is a recursive search algorithm that goes all the way down to the leaves and then <b>backs up</b> the minimax values up the tree as the recursion unwinds.</p>
        <div class="ln-flex-center">
          <p><code>function MINIMAX-SEARCH(game, state)<br>&nbsp;&nbsp;player = game.TO-MOVE(state)<br>&nbsp;&nbsp;value, move = MAX-VALUE(game, state)<br>&nbsp;&nbsp;return move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MAX-VALUE(game, state)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = -infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MIN-VALUE(game, game.RESULT(state, a))<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &gt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MIN-VALUE(game, state)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MAX-VALUE(game, game.RESULT(state, a))<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &lt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <p>Let `m` be the maximum depth of the tree and assume there are `b` legal moves at each point.</p>
        <p>The time complexity of minimax search is `O(b^m)` and the space complexity is `O(bm)` (the reasoning for the space complexity is the same as the reasoning for the space complexity of depth-first search; this is a depth-first algorithm after all).</p>
      </div>
      <h3>Optimal decisions in multiplayer games</h3>
      <p>To create game trees for games with more than two players, we assign a group of values (one value for each player) to each node. Consider a game (similar to the one before) where all players are trying to get to the node that will give them (individually) the highest score possible.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/three_ply_game_tree_1.png">
      </div>
      <p>Let's say we have three players `A`, `B`, and `C`. So each node will have a group of minimax values `(a,b,c)` where `a` is the minimax value for `A`, `b` is the minimax value for `B`, and `c` is the minimax value for `C`. `A` is trying to get to the node with the highest `a` value, `B` is trying to get to the node with the highest `b` value, and `C` is trying to get to the node with the highest `c` value.</p>
      <p>Suppose `A` wants to get to the node `(7,4,1)`, so `A` moves left. At this point, `B` would also want to get to the node `(7,4,1)`, so `B` moves right. From the remaining options, `C` wants to get to the node `(6,1,2)`, so `C` moves left. So the final score for all three players is `A:6`, `B:1`, `C:2`.</p>
      <p>This is the game tree assuming everyone plays optimally:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/three_ply_game_tree_2.png">
        <p>`A`'s best move is left since the minimax value of the left node `(1)` is greater than the minimax value of the right node `(0)`.</p>
      </div>
      <p>Again, the terminal nodes have their minimax values defined by the game's `text(UTILITY)` function, and the values are passed up the tree depending on which player's turn it is.</p>
      <h3>Alpha-Beta Pruning</h3>
      <p>We can actually sometimes deduce the minimax value of a node without having to fully expand it. This means that we can optimize the algorithm by not examining all the nodes. If we can deduce that it is not optimal for a player to go down a certain path, we can skip it, which resultingly <b>prunes</b> the tree.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/alpha_beta_pruning_1.png">
      </div>
      <p>Suppose we're running the algorithm and we've already discovered the minimax value of `B`. And now when we're calculating the minimax value of `C`, we find that its first child has a value of `2`. We don't need to look at the minimax values of the other two children because the minimax value of `C` will be at most `2`. If the other two children have minimax values higher than `2`, then MIN will move to the node with minimax value `2`. If the other two children have minimax values lower than `2`, then MIN will move to those nodes, making MAX's score even lower. No matter what those two children of `C` are, it's not in MAX's best interest to move to `C` because moving to `B` is a better option.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/alpha_beta_pruning_2.png">
      </div>
      <p>We do however, need to explore all of `D`'s children though. The minimax values of the first two children are `14` and `5`, both of which are higher than `3`, so MAX would still want to go down this path. It's not until we look at the last child that we can determine that this path is not optimal for MAX.</p>
      <div class="ln-box ln-center">
        <p>`text(MINIMAX)(text(root)) = max(min(3,12,8),min(2,x,y),min(14,5,2))`</p>
        <p>`= max(3,min(2,x,y),2)`</p>
        <p>`= max(3,z,2)` where `z = min(2,x,y) le 2`</p>
        <p>`=3`</p>
      </div>
      <div class="ln-box">
        <p>It is called <b>alpha-beta pruning</b> because `alpha` and `beta` are used as bounds to determine whether a part of the tree can be pruned. `alpha` is the highest minimax value found so far and `beta` is the lowest minimax value found so far.</p>
        <div class="ln-flex-center">
          <p><code>function ALPHA-BETA-SEARCH(game, state)<br>&nbsp;&nbsp;player = game.TO-MOVE(state)<br>&nbsp;&nbsp;value, move = MAX-VALUE(game, state, -infinity, infinity)<br>&nbsp;&nbsp;return move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MAX-VALUE(game, state, &alpha;, &beta;)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = -infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MIN-VALUE(game, game.RESULT(state, a), &alpha;, &beta;)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &gt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&alpha; = MAX(&alpha;, v)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v &ge; &beta; then return v, move<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MIN-VALUE(game, state, &alpha;, &beta;)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MAX-VALUE(game, game.RESULT(state, a), &alpha;, &beta;)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &lt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&beta; = MIN(&beta;, v)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v &le; &alpha; then return v, move<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <p>Notice that <code>if v &le; &alpha; then return v, move</code> is the reasoning we used to stop exploring `C`. `alpha` is `3` (the highest value we've seen so far) and `v` is `2`. Since `2 le 3`, we're gonna return `2`, i.e., we're assigning the minimax value of `C` to `2`.</p>
      </div>
      <h3>Move ordering</h3>
      <p>Notice that if the last child of `D` (the one with minimax value `2`) had been generated first, then we could've pruned `D`'s other children. This means that, if possible, we should try to examine successors that allow us to prune the tree.</p>
      <p>A move that prunes a tree when examined is called a <b>killer move</b>. Examining those moves first is called the killer move heuristic. It's not examining the node and then finding out that it prunes the tree; it's knowing ahead of time that examining that node will prune the tree. This information comes from past experience. Maybe we have a collection of winning moves and we find a move at our current state in that collection.</p>
      <div class="ln-box">
        <p>In the best case, alpha-beta would only need to examine `O(b^(m//2))` nodes, compared to minimax's best case of `O(b^m)`. When we prune a tree, we're kinda removing half of it.</p>
      </div>
      <h2 id="heuristicalphabetasearch">Heuristic Alpha-Beta Tree Search</h2>
      <p>Even with alpha-beta pruning and good move ordering, there are still too many states to explore for games like chess. One way to deal with this is to use a <b>Type A strategy</b>, which considers all possible moves until a certain depth. Then it uses a heuristic function to estimate the utility of the rest of the states.</p>
      <p>There's a <b>cutoff test</b> to determine when to stop searching and start estimating. This effectively treats nonterminal nodes as if they were terminal nodes.</p>
      <h3>Evaluation functions</h3>
      <p>A heuristic evaluation function gives us an estimate of a state's expected utility.</p>
      <div class="ln-box">
        <p>`text(EVAL)(s,p)=text(UTILITY)(s,p)` for terminal states.</p>
        <p>`text(UTILITY)(text(loss),p) le text(EVAL)(s,p) le text(UTILITY)(text(win),p)` for nonterminal states.</p>
      </div>
      <p>Evaluation functions work by calculating various <b>features</b> of the state. The information from those features is used to assign a numerical value to that state. A good evaluation function should not take too long to compute (obviously). More importantly, the evaluation function should be strongly correlated with the actual chances of winning.</p>
      <p>For example, one feature we could use is the number of pieces that each player (white vs black) has remaining. If there's a move that results in white having more pieces, then that state should have a high evaluation function value for white.</p>
      <p>Of course, simply having more pieces doesn't necessarily mean it's better. It depends on what types of pieces they are. For example, a queen is much more valuable than having eight pawns and no queen. We take things like this into account by adding weight to certain features that are more important than other features.</p>
      <div class="ln-box">
        <p>Mathematically, this kind of evaluation function is called a <b>weighted linear function</b>.</p>
        <div class="ln-center">
          <p>`text(EVAL)(s) = w_1f_1(s)+w_2f_2(s)+...+w_nf_n(s)`</p>
          <p>where each `f_i` is a feature, such as "number of white bishops"</p>
        </div>
        <p>However, using a linear function only works when the features are independent of each other, which isn't the case in chess. Bishops are generally worth more when there are fewer pieces in the game, i.e., number of bishops and number of pieces remaining are not independent.</p>
      </div>
      <h3>Cutting off search</h3>
      <div class="ln-box">
        <p>To implement this, we can modify <code>ALPHA-BETA-SEARCH</code> by replacing</p>
        <div class="ln-center">
          <p><code>if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null</code></p>
        </div>
        <p>with</p>
        <div class="ln-center">
          <p><code>if game.IS-CUTOFF(state, depth) then return game.EVAL(state, player), null</code></p>
        </div>
      </div>
      <p>Using an evaluation function that just counts the number of pieces is very simple and can lead to errors. For example, suppose the algorithm stops at the cutoff and finds a state where black has more pieces than white, so black moves to that state. But then white moves to capture black's queen! If the algorithm had been able to look ahead, then it would've known that it was not a favorable position after all.</p>
      <p>Positions where a pending move can wildly swing the evaluation are called nonquiescent positions. The evaluation function should be used only for <b>quiescent</b> positions, and search should continue until quiescent positions are reached.</p>
    </div>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
