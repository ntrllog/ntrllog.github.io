<!DOCTYPE html>
<html lang="en">
  <head>
    <!--
                                      _
        /\     _             _   _   | |             __    __
       /  \   | |      /\   | \ | |  | |       /\   |  \  /  |
      /    \  | |     /  \  |  \| |  | |      /  \  | |\\//| |
     / ____ \ | |__  / __ \ | |\  |  | |___  / __ \ | | \/ | |
    /_/    \_\|____|/_/  \_\|_| \_|  |_____|/_/  \_\|_|    |_|
                          _   _
     _  _    _     _  _  | | | |   __     ____
    | |/ \  | |_  | |/_| | | | |  /  \   /    \
    | |   | |  _| |  /   | | | | | || | |  ||  |
    | |   | | |_  | |    | | | | | || | |  ||  |
    |_|   | |___| |_|    |_| |_|  \__/   \____/|
                                               |
                                          ____/
    -->
    <title>ntrllog | Artificial Intelligence</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
    <link rel="shortcut icon" type="image/png" href="../pictures/favicon.ico"/>
    <link href="../css/content.css" rel="stylesheet">
  </head>
  <body>
    <div class="dropdown ln-fixed-right">
      <button class="btn btn-secondary dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">ToC</button>
      <ul class="dropdown-menu">
        <li><a class="dropdown-item" href="#whatisit">What is AI?</a></li>
        <li><a class="dropdown-item" href="#agentsandenvironments">Agents and Environments</a></li>
        <li><a class="dropdown-item" href="#goodbot">Good Behavior: The Concept of Rationality</a></li>
        <li><a class="dropdown-item" href="#environments">The Nature of Environments</a></li>
        <li><a class="dropdown-item" href="#agentstructure">The Structure of Agents</a></li>
        <li><a class="dropdown-item" href="#problemsolvingagents">Problem-Solving Agents</a></li>
        <li><a class="dropdown-item" href="#exampleproblems">Example Problems</a></li>
        <li><a class="dropdown-item" href="#searchalgorithms">Search Algorithms</a></li>
        <li><a class="dropdown-item" href="#uninformedsearchstrategies">Uninformed Search Strategies</a></li>
        <li><a class="dropdown-item" href="#informedsearchstrategies">Informed (Heuristic) Search Strategies</a></li>
        <li><a class="dropdown-item" href="#heuristicfunctions">Heuristic Functions</a></li>
        <li><a class="dropdown-item" href="#localsearch">Local Search and Optimization Problems</a></li>
        <li><a class="dropdown-item" href="#localsearchincontinuousspaces">Local Search in Continuous Spaces</a></li>
        <li><a class="dropdown-item" href="#nondeterministicsearch">Search with Nondeterministic Actions</a></li>
        <li><a class="dropdown-item" href="#partiallyobservablesearch">Search in Partially Observable Environments</a></li>
        <li><a class="dropdown-item" href="#onlinesearch">Online Search Agents and Unknown Environments</a></li>
        <li><a class="dropdown-item" href="#constraintsatisfaction">Constraint Satisfaction Problems</a></li>
        <li><a class="dropdown-item" href="#constraintpropagation">Constraint Propagation: Inference in CSPs</a></li>
        <li><a class="dropdown-item" href="#backtrackingsearch">Backtracking Search for CSPs</a></li>
        <li><a class="dropdown-item" href="#localsearchcsp">Local Search for CSPs</a></li>
        <li><a class="dropdown-item" href="#gametheory">Game Theory</a></li>
        <li><a class="dropdown-item" href="#optimaldecisionsingames">Optimal Decisions in Games</a></li>
        <li><a class="dropdown-item" href="#heuristicalphabetasearch">Heuristic Alpha-Beta Tree Search</a></li>
        <li><a class="dropdown-item" href="#montecarlosearch">Monte Carlo Tree Search</a></li>
        <li><a class="dropdown-item" href="#stochasticgames">Stochastic Games</a></li>
        <li><a class="dropdown-item" href="#partiallyobservablegames">Partially Observable Games</a></li>
        <li><a class="dropdown-item" href="#knowledgebasedagents">Knowledge-Based Agents</a></li>
        <li><a class="dropdown-item" href="#thewumpusworld">The Wumpus World</a></li>
        <li><a class="dropdown-item" href="#logic">Logic</a></li>
        <li><a class="dropdown-item" href="#propositionallogic">Propositional Logic: A Very Simple Logic</a></li>
        <li><a class="dropdown-item" href="#propositionaltheoremproving">Propositional Theorem Proving</a></li>
        <li><a class="dropdown-item" href="#firstorderlogic">First-Order Logic</a></li>
        <li><a class="dropdown-item" href="#syntaxandsemanticsoffirstorderlogic">Syntax and Semantics of First-Order Logic</a></li>
        <li><a class="dropdown-item" href="#usingfirstorderlogic">Using First-Order Logic</a></li>
        <li><a class="dropdown-item" href="#inferenceinfirstorderlogic">Propositional vs. First-Order Inference</a></li>
        <li><a class="dropdown-item" href="#resolution">Resolution</a></li>
      </ul>
    </div>
    <div class="container ln-line-height">
      <a href="projects.html"><i class="fas fa-long-arrow-alt-left fa-2x"></i></a>
      <h1>Artificial Intelligence</h1>
      <hr>
      <p>Shortcut to this page: <a href="ai.html">ntrllog.netlify.app/ai</a></p>
      <p>Info provided by <a href="https://aima.cs.berkeley.edu" target="_blank">Artificial Intelligence: A Modern Approach</a> and Professor Francisco Guzman (CSULA)</p>
      <h2 id="whatisit">What is AI?</h2>
      <p>Well, it kinda depends on what we think makes a machine "intelligent". Is it the ability for the machine to act like a human? Or is it the ability for the machine to always do the mathematically-optimal thing? And is intelligence a way of thinking or a way of behaving? These are the four main dimensions that researchers are using to think about AI.</p>
      <h3>Acting humanly: The Turing test approach</h3>
      <p>Let's say we ask a question to a computer and some people and get back their responses. If we can't tell whether the responses came from a person or from a computer, then the computer passes the <b>Turing test</b>.</p>
      <div class="ln-box">
        <p>In order to pass the Turing test, a computer would need:</p>
        <ul>
          <li><b>natural language processing</b>: communicate in a human language</li>
          <li><b>knowledge representation</b>: store what it knows or hears</li>
          <li><b>automated reasoning</b>: answer questions and draw new conclusions</li>
          <li><b>machine learning</b>: adapt to new circumstances and detect and extrapolate patterns</li>
        </ul>
        <p>There's also a <b>total Turing test</b>, which tests if a computer can <em>physically</em> behave like a human. To pass the total test, a computer would also need:</p>
        <ul>
          <li><b>computer vision</b> and speech recognition</li>
          <li><b>robotics</b>: manipulate objects and move around</li>
        </ul>
      </div>
      <h3>Thinking humanly: The cognitive modeling approach</h3>
      <p>While the Turing test is just focused on the computer's results, some people are more interested in the computer's process. When performing a task, does a computer go through the same thought process as a human would?</p>
      <div class="ln-box">
        <p>If the input-output behavior matches human behavior, then that could give us some insight into how our brains work.</p>
      </div>
      <div class="ln-box">
        <p>We can learn about how humans think by:</p>
        <ul>
          <li><b>introspection</b>: thinking about our thoughts as they go by</li>
          <li><b>psychological experiments</b>: observing a person in action</li>
          <li><b>brain imaging</b>: observing the brain in action</li>
        </ul>
        <p><b>Cognitive science</b> combines computer models and experimental techniques from psychology to study the human mind.</p>
      </div>
      <h3>Thinking rationally: The "laws of thought" approach</h3>
      <p>The world of <b>logic</b> is pretty much, "if these statements are true, then this has to be true". If we give a computer some assumptions, can it reason its way to a correct conclusion?</p>
      <div class="ln-box">
        <p>In things like chess and math, all the rules are known, so it's relatively easy to make logical choices and statements. But in the real world, there's a lot of uncertainty. Interestingly, the theory of <b>probability</b> allows for rigorous reasoning with uncertain information. With this, we can use raw perceptual information to understand how the world works and make predictions.</p>
      </div>
      <h3>Acting rationally: The rational agent approach</h3>
      <p>A computer agent is defined to be something that can operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change, and create and pursue goals. A <b>rational agent</b> acts to achieve the best outcome.</p>
      <p>There's a subtle distinction between "thinking" rationally and "acting" rationally. Thinking rationally is about making correct inferences. (What's implied here is that there is some (slow) deliberate reasoning process involved.) But it's possible to act rationally without having to make inferences. For example, recoiling from a hot stove, which is more of a reflex action than something you come to a conclusion about.</p>
      <div class="ln-box">
        <p>The rational agent approach is how most researchers approach studying AI. This is because rationality (as opposed to humanity) is easier to approach scientifically, and the rational agent approach kinda encapsulates all the other approaches. For example, in order to act rationally, a computer needs the skills that is required for it to act like a human.</p>
      </div>
      <p>So AI is the science of making machines act rationally, a.k.a. <b>doing the right thing</b>. The "right" thing is specified by us to the agent as an objective that it should complete. This way of thinking about AI is considered the <b>standard model</b>.</p>
      <h3>Beneficial machines</h3>
      <p>But the problem with the standard model is that it only works when the objective is fully specified. In the real world, it's hard to specify a goal that is complete and correct. For example, if the primary goal of a self-driving car is safety, then, of course, it's safest to just not leave the house at all, but that's not helpful; there needs to be some tradeoff. Inherently, there is a <b>value alignment problem</b>: the values put into the machine must be aligned with our values. That is, the machine's job shouldn't be to just complete its objective; it should also consider the costs, consequences, and benefits that would affect us.</p>
      <h2 id="agentsandenvironments">Agents and Environments</h2>
      <p>A little more formally, an <b>agent</b> perceives its <b>environment</b> through <b>sensors</b> and acts upon that environment through <b>actuators</b>. Its <b>percept sequence</b> is the complete history of everything it has ever perceived. The agent looks at its percept sequence (and its built-in knowledge) to determine what action to take.</p>
      <div class="ln-box">
        <p>Mathematically, an agent's behavior is defined by an <b>agent function</b>, which maps a percept sequence to an action.</p>
      </div>
      <p>Let's take a look at a robotic vacuum cleaner. Suppose the floor is divided into two squares `A` and `B`, and that the vacuum cleaner is on square `A`. We can create a table that (theoretically) lists all the percept sequences and the actions that (we think) should be taken for each one.</p>
      <table class="table">
        <tr>
          <th>Percept sequence</th>
          <th>Action</th>
        </tr>
        <tr>
          <td>Square `A` is clean</td>
          <td>Move right</td>
        </tr>
        <tr>
          <td>Square `A` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>Square `B` is clean</td>
          <td>Move left</td>
        </tr>
        <tr>
          <td>Square `B` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is clean</td>
          <td>Move right</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>`vdots`</td>
          <td>`vdots`</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is clean, Square `A` is clean</td>
          <td>Move right</td>
        </tr>
        <tr>
          <td>Square `A` is clean, Square `A` is clean, Square `A` is dirty</td>
          <td>Clean</td>
        </tr>
        <tr>
          <td>`vdots`</td>
          <td>`vdots`</td>
        </tr>
      </table>
      <p>In this very simplified scenario, it's pretty easy to see what the right actions should be. But in general, it's a bit harder to know what the correct action should be.</p>
      <h2 id="goodbot">Good Behavior: The Concept of Rationality</h2>
      <p>So how can we know what the "right" action should be? How do we know that an action is "right"?</p>
      <h3>Performance Measures</h3>
      <p>Generally, we assess how good an agent is by the consequences of its actions (<b>consequentialism</b>). If the resulting environment is desirable, then the agent is good. (good bot)</p>
      <p>A <b>performance measure</b> is a way to evaluate the agent's behavior in an environment. We have to be a little bit thoughtful about how we come up with a performance measure though. For the vacuum cleaner, it might sound like a good idea to measure performance by how much dirt is cleaned up in an eight-hour shift. The problem with this is that a rational agent would maximize performance by repeatedly cleaning up the dirt and dumping it on the floor. A better performance measure would be to reward the agent for a clean floor. For example, we could give it one point for each clean square, incentivizing it to maximize the number of points (and thus the number of clean squares).</p>
      <p>As we can see, it's better to come up with performance measures based on what we want to be achieved in the environment, rather than what we think the agent should do.</p>
      <h3>Rationality</h3>
      <p>Formally, for each possible percept sequence, a rational agent should use the evidence provided by the percept sequence and its built-in knowledge to select an action that maximizes its performance measure (<b>definition of a rational agent</b>). We can assess whether an agent is rational by looking at four things:</p>
      <ul>
        <li>the performance measure that defines success</li>
        <li>the agent's prior knowledge of the environment</li>
        <li>the actions that the agent can perform</li>
        <li>the agent's percept sequence to date</li>
      </ul>
      <p>For our vacuum cleaner:</p>
      <ul>
        <li>the performance measure awards one point for each clean square</li>
        <li>the "geography" of the environment is known (but not the dirt distribution)</li>
        <li>the actions are moving right, moving left, and cleaning</li>
        <li>the agent's percept sequence consists of what square it's at and whether it's clean</li>
      </ul>
      <p>Under these circumstances, we can expect our vacuum cleaner to do at least as well as any other agent.</p>
      <div class="ln-box">
        <p>Suppose that we change the performance measure to deduct one point for each movement. Once all the squares are clean, the vacuum cleaner will just keep moving back and forth (we never defined "stopping" as an action). Then it would be irrational.</p>
      </div>
      <h3>Omniscience, learning, and autonomy</h3>
      <p>One thing that's been sorta glossed over is that the agent doesn't know what will <em>actually</em> happen if it performs an action, i.e., it's not omniscient. So it's more correct to say that a rational agent tries to maximize <em>expected</em> performance.</p>
      <p>In order to be able to maximize expected performance, the agent needs to be able to <b>gather information</b> and <b>learn</b>.</p>
      <div class="ln-box">
        <p>Crossing a street when it's empty is rational. It's expected that you'll reach the other side. But if you get hit by a car because you didn't look before crossing... well... then that's not rational.</p>
      </div>
      <p>If an agent only relies on its built-in knowledge, then it lacks <b>autonomy</b>. In order to be autonomous, the agent should build on its existing knowledge by learning. By being able to learn, a rational agent can be expected to succeed in a variety of environments.</p>
      <h2 id="environments">The Nature of Environments</h2>
      <p>A <b>task environment</b> is the problem that we are trying to solve by using a rational agent.</p>
      <h3>Specifying the task environment</h3>
      <p>We can specify a task environment by specifying the performance measure, the environment, and the agent's actuators and sensors (<b>PEAS</b>). Yes, these are the four things we listed out when defining how an agent can be rational. The vacuum cleaner example was fairly simple though, so we can look at something more complicated, like an automated taxi.</p>
      <ul>
        <li><b>performance measure</b>: correct destination, minimize fuel consumption, minimize time/cost, minimize impact on other drivers, maximize passenger safety and comfort, maximize profits</li>
        <li><b>environment</b>: roads, traffic, obstacles, police, pedestrians, passengers, weather</li>
        <li><b>actuators</b>: steering, accelerator, brake, signal, horn, display screen, speech</li>
        <li><b>sensors</b>: cameras, speedometer, GPS, accelerometer, radar, microphone, touchscreen</li>
      </ul>
      <h3>Properties of task environments</h3>
      <p>A task environment can be <b>fully observable</b> or <b>partially observable</b>. If the agent's sensors give it access to the complete state of the environment, then the task environment is fully observable. Noisy and inaccurate sensors would make it partially observable. Or maybe the capability simply isn't available. For example, a vacuum with a local sensor can't tell whether there is dirt in other squares and an automated taxi can't know what other drivers are thinking. Basically, if the agent has to make a guess, then the environment is partially observable.</p>
      <div class="ln-box">
        <p>An environment can also be <b>unobservable</b> if the agent has no sensors.</p>
      </div>
      <p>A task environment can be <b>single-agent</b> or <b>multiagent</b>. This one's pretty self-explanatory.</p>
      <div class="ln-box">
        <p>Though there is a small subtlety. Does an automated taxi have to view another car as another agent or can it think of it as just another "object"? The answer depends on whether the "object's" performance measure is affected by the automated taxi. If it is, then it must be viewed as an agent.</p>
      </div>
      <div class="ln-box">
        <p>A multiagent environment can be <b>competitive</b> or <b>cooperative</b>. In a competitive environment, maximizing/improving one agent's performance measure minimizes/worsens another agent's performance measure. In a cooperative environment, all agents' performance measures can be maximized.</p>
      </div>
      <p>A task environment can be <b>deterministic</b> or <b>nondeterministic</b>. If the next state of the environment is completely determined by the current state and the agent's action, then it is deterministic. If something unpredictable can happen, then it is nondeterministic. For example, driving is nondeterministic because traffic cannot be predicted exactly. But if you want to argue that traffic is predictable, then some more reasons that driving is nondeterministic are that the tires may blow out unexpectedly or that the engine may seize up unexpectedly.</p>
      <div class="ln-box">
        <p>A <b>stochastic</b> environment is a nondeterministic environment that deals with probabilities. "There's a `25%` chance of rain" is stochastic, while "There's a chance of rain" is nondeterministic.</p>
      </div>
      <p>A task environment can be <b>episodic</b> or <b>sequential</b>. Classifying images is episodic because classifying the current image doesn't depend on the decisions for past images nor does it affect the decisions of future images. Chess and driving are sequential because the available moves depend on past moves, and the next move affects future moves.</p>
      <p>A task environment can be <b>static</b> or <b>dynamic</b>. A static environment doesn't change while the agent is deciding on its next action, whereas a dynamic one does. Driving is dynamic because the other cars are always moving. Chess is static. As time passes, if the environment doesn't change, but the agent's performance score does, then the environment is <b>semidynamic</b>. Timed chess is semidynamic.</p>
      <p>A task environment can be <b>discrete</b> or <b>continuous</b>. This refers to the state of the environment, the way time is handled, and the percepts and actions of the agent. Chess has a finite number of distinct states, percepts, and actions, which means it is discrete. Whereas in driving, speed and location range over continuous values.</p>
      <p>A task environment can be <b>known</b> or <b>unknown</b>. (Well, strictly speaking, not really.) This is not referring to the environment itself, but to the agent's knowledge about the rules of its environment. In a known environment, the outcomes (or outcome probabilities) for all actions are given.</p>
      <div class="ln-box">
        <p>Known vs unknown is not the same as fully vs partially observable. A known environment can be partially observable, like solitaire, where we know the rules, but we can't see the cards that are turned over. An unknown environment can be fully observable, like a new video game, where the screen shows the entire game state, but we don't know what the buttons do yet until we try them.</p>
      </div>
      <div class="ln-box">
        <p>The hardest case is partially observable, multiagent, nondeterministic, sequential, dynamic, continuous, and unknown. Driving is all of these, except the environment is known.</p>
      </div>
      <h2 id="agentstructure">The Structure of Agents</h2>
      <p>Now we get to go over the more technical aspects of how agents work.</p>
      <p>Recall that an agent function maps percept sequences to actions. An <b>agent program</b> is code that implements the agent function. This program runs on some sort of computing device with sensors and actuators, referred to as the <b>agent architecture</b>.</p>
      <div class="ln-center">
        <p>agent = architecture + program</p>
      </div>
      <p>In general, the architecture makes the percepts from the sensors available to the program, runs the program, and sends the program's action choices to the actuators.</p>
      <h3>Agent programs</h3>
      <p>A simple way to design agent programs is to have them take the current percept as input and return an action. Here is some sample pseudocode for an agent program:</p>
      <div class="ln-flex-center">
        <p><code>function TABLE-DRIVEN-AGENT(percept)<br>&nbsp;&nbsp;append percept to the end of percepts<br>&nbsp;&nbsp;action = LOOKUP(percepts, table)<br>&nbsp;&nbsp;return action</code></p>
      </div>
      <div class="ln-center">
        <p><code>percepts</code>: a sequence, initially empty</p>
        <p><code>table</code>: a table of actions, indexed by percept sequences, initially fully specified</p>
      </div>
      <div class="ln-box">
        <p>The <code>table</code> is the same table that we drew out for the vacuum cleaner (in the "Agents and Environments" section). Using a table to list out all the percepts and actions is a way to represent the agent function, but it is typically not practical to build such a table because it must contain every possible percept sequence. Here's why.</p>
        <p>Let `P` be the set of possible percepts and let `T` be the total number of percepts the agent will receive.</p>
        <p>If the agent receives `1` percept, then there are `|P|` possible percepts that it could be.</p>
        <p>If the agent receives `2` percepts, then there are `|P|^2` possible percepts that they could be.</p>
        <p>If the agent receives `3` percepts, then there are `|P|^3` possible percepts that they could be.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/table_entries.png">
          <p>Suppose each shape is a percept. For each percept at `t=1`, there are `|P|` percepts at `t=2` that could be paired with that percept. So `|P|*|P|=|P|^2` total percepts.</p>
          <p>Extending that reasoning, for each of the `|P|^2` percept pairs at `t=1,2`, there are `|P|` percepts at `t=3` that could be paired with that percept pair. So `|P|^2*|P|=|P|^3` total percepts.</p>
        </div>
        <p>In general, there are `sum_(t=1)^T |P|^t` possible percept sequences, meaning that the table will have this many entries. An hour's worth of visual input from a single camera can result in at least `10^(600,000,000,000)` percept sequences. Even for chess, there are at least `10^(150)` percept sequences.</p>
        <p>So yeah, building such a table is not really possible.</p>
      </div>
      <p>Theoretically though, <code>TABLE-DRIVEN-AGENT</code> does do what we want, which is to implement the agent function. The challenge is to write programs that do this without using a table.</p>
      <div class="ln-box">
        <p>Before the 1970s, people were apparently using huge tables of square roots to do their math. Now there are calculators that run a five-line program implementing Newton's method to calculate square roots.</p>
        <p>Can AI do this for general intelligent behavior?</p>
      </div>
      <h3>Simple reflex agents</h3>
      <p>This is the simplest kind of agent. A <b>simple reflex agent</b> selects actions based only on the current percept; it ignores the rest of the percept history. Here's an agent program that implements the agent function for our vacuum:</p>
      <div class="ln-flex-center">
        <p><code>function REFLEX-VACUUM-AGENT([location, status])<br>&nbsp;&nbsp;if status == Dirty then return Clean<br>&nbsp;&nbsp;else if location == A then return Move right<br>&nbsp;&nbsp;else if location == B then return Move left</code></p>
      </div>
      <div class="ln-box">
        <p>Note that since the agent is ignoring the percept history, there are only `4` total possible percepts (not `4^T`) at all times. Square `A` is clean, square `A` is dirty, square `B` is clean, and square `B` is dirty.</p>
      </div>
      <p>Simple reflex behaviors can be modeled as "if-then" statements. If the car in front is braking, then initiate braking. Because of their structure, they are called <b>condition-action rules</b> (or <b>situation-action rules</b> or <b>if-then rules</b>).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/simple_reflex_agent.png">
      </div>
      <p><code>REFLEX-VACUUM-AGENT</code> is specific to the world of just two squares. Here's a more general and flexible way of writing agent programs:</p>
      <div class="ln-flex-center">
        <p><code>function SIMPLE-REFLEX-AGENT(percept)<br>&nbsp;&nbsp;state = INTERPRET-INPUT(percept)<br>&nbsp;&nbsp;rule = RULE-MATCH(state, rules)<br>&nbsp;&nbsp;action = rule.ACTION<br>&nbsp;&nbsp;return action</code></p>
      </div>
      <div class="ln-center">
        <p><code>rules</code>: a set of condition-action rules</p>
      </div>
      <p>This approach involves designing a general-purpose interpreter that can convert a percept to a state. Then we provide the rules for specific states.</p>
      <p>While simple reflex agents are, well, simple, they only work well in fully observable environments. The state of the environment has to be easily identifiable, otherwise the wrong rule and action will be applied. For example, it may be hard for the agent to tell if a car is braking or if it's just the taillights that are on.</p>
      <p>For our vacuum example, suppose the vacuum didn't have a location sensor. Then it won't know which square it's on. If it starts at square `A` and chooses to move left, it will fail forever (and likewise if it starts at square `B` and chooses to move right).</p>
      <p>Infinite loops like these often happen in partially observable environments and can be avoided by having the agent <b>randomize</b> its actions. Randomization can be rational in some multiagent environments, but it is usually not rational in single-agent environments.</p>
      <h3>Model-based reflex agents</h3>
      <p>To deal with partial observability, the agent can keep track of the part of the world it can't see now. In order to do this, the agent needs to maintain some sort of <b>internal state</b> that depends on the percept history. For a car changing lanes, the agent needs to keep track of where the other cars are if it can't see them all at once.</p>
      <p>In order to update its internal state, the agent needs to know "how the world works". When the agent turns the steering wheel clockwise, the car turns to the right (effect of agent action). When it's raining, the car's camera can get wet (how the world evolves independent of agent's actions). This is a <b>transition model</b> of the world.</p>
      <p>The agent also needs to know how to interpret the information it gets from its percepts to understand the current state of the world. When the car in front of the agent brakes, there will be several red regions in the forward-facing camera image. When the camera gets wet, there will be droplet-shaped objects in the image. This knowledge is called a <b>sensor model</b>.</p>
      <p>A <b>model-based agent</b> uses the transition model and sensor model to keep track of the state of the world.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/model_based_reflex_agent.png">
      </div>
      <div class="ln-box">
        <p>"What the world is like now" is really the agent's best guess.</p>
      </div>
      <div class="ln-flex-center">
        <p><code>function MODEL-BASED-REFLEX-AGENT(percept)<br>&nbsp;&nbsp;state = UPDATE-STATE(state, action, percept, transition_model, sensor_model)<br>&nbsp;&nbsp;rule = RULE-MATCH(state, rules)<br>&nbsp;&nbsp;action = rule.ACTION<br>&nbsp;&nbsp;return action</code></p>
      </div>
      <div class="ln-center">
        <p><code>state</code>: the agent's current conception of the world state</p>
        <p><code>transition_model</code>: a description of how the next state depends on the current state and action</p>
        <p><code>sensor_model</code>: a description of how the current world state is reflected in the agent's percepts</p>
        <p><code>rules</code>: a set of condition-action rules</p>
        <p><code>action</code>: the agent's most recent action, initially none</p>
      </div>
      <h3>Goal-based agents</h3>
      <p>Sometimes, having information about the current state of the environment is not enough to decide what to do. For example, knowing that there's a fork in the road is not enough; the agent needs to know whether it should go left or right. The correct decision depends on the <b>goal</b>.</p>
      <p>Goal-based decision making is different from following condition-action rules because it involves thinking about the future. A reflex agent brakes when it sees brake lights because that's specified in the rules. But a goal-based agent brakes when it sees brake lights because that's how it thinks it will avoid hitting other cars.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/goal_based_agent.png">
      </div>
      <div class="ln-box">
        <p>Goal-based agents are also more flexible than reflex agents. If we want a goal-based agent to drive to a different destination, then we just have to update the goal. But if we want a reflex agent to go to a different destination, we have to update all the rules so that they go to the new destination.</p>
      </div>
      <h3>Utility-based agents</h3>
      <p>While achieving the goal is what we want the agent to do, there are many different actions that can be taken to get there. And those different actions can affect how "happy" or "unhappy" the agent will be. This level of "happiness" is referred to as <b>utility</b>.</p>
      <div class="ln-box">
        <p>For example, a quicker/safer/more reliable/cheaper route makes the agent "happier" because it optimizes its performance measure.</p>
      </div>
      <div class="ln-box">
        <p>A <b>utility function</b> maps environment states to a desirability score. If there are conflicting goals (e.g., speed and safety), the utility function specifies the appropriate tradeoff.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/utility_based_agent.png">
      </div>
      <div class="ln-box">
        <p>An agent won't know exactly how "happy" it will be if it takes an action. So it's more correct to say that a rational utility-based agent chooses the action that maximizes <b>expected utility</b> (as opposed to just utility).</p>
      </div>
      <h3>Learning agents</h3>
      <p>The preferred way to build agents (model-based, goal-based, utility-based, etc.) is to build them as a learning agent and then teach them.</p>
      <p>A learning agent can be divided into four conceptual components: the learning element, the performance element, the critic, and the problem generator. The <b>learning element</b> is responsible for making improvements. The <b>performance element</b> is responsible for selecting actions. The <b>critic</b> provides feedback on how the agent is doing and determines how the performance element should be modified to do better.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/learning_agent.png">
      </div>
      <p>The <b>problem generator</b> is responsible for suggesting actions that will lead to new and informative experiences. This allows the agent to explore some suboptimal actions in the short run that may become much better actions in the long run.</p>
      <div class="ln-box">
        <p>The performance standard can be seen as a mapping from a percept to a <b>reward</b> or <b>penalty</b>. This allows the critic to learn what is good and bad.</p>
        <p>The performance standard is a fixed external thing that is not a part of the agent.</p>
      </div>
      <hr>
      <p>For now, we'll assume that the environments are episodic, single agent, fully observable, deterministic, static, discrete, and known.</p>
      <hr>
      <h2 id="problemsolvingagents">Problem-Solving Agents</h2>
      <p>If the agent doesn't know what the correct action to take should be, it should do some problem solving:</p>
      <ol>
        <li><b>Goal formulation</b>: figure out what the goal is to limit the number of actions that need to be considered</li>
        <li><b>Problem formulation</b>: describe the states and actions (come up with an abstract model of the world)</li>
        <li><b>Search</b>: try different sequences of actions to find one (the <b>solution</b>) that achieves the goal</li>
        <li><b>Execution</b>: just do it</li>
      </ol>
      <div class="ln-box">
        <p>In a fully observable, deterministic, known environment, the solution to any problem is a fixed sequence of actions. For example, traveling from one place to another.</p>
        <p>In a partially observable or nondeterministic environment, a solution would be a branching strategy (if/else) that recommends different future actions depending on what percepts the agent sees. For example, the agent might come across a sign that says the road is closed, so it would need a backup plan.</p>
      </div>
      <h3>Search problems and solutions</h3>
      <p>Formally, a <b>search</b> problem can be defined by specifying:</p>
      <ul>
        <li>the <b>state space</b>: the set of possible states that the environment can be in</li>
        <li>the <b>initial state</b> that the agent starts in</li>
        <li>a set of one or more <b>goal states</b></li>
        <li>the <b>actions</b> available to the agent</li>
        <li>a <b>transition model</b> that describes what each action does</li>
        <li>an <b>action cost function</b> that describes the cost of performing an action
          <ul>
            <li>the cost should reflect the agent's performance measure, such as distance, time, or monetary cost</li>
          </ul>
        </li>
      </ul>
      <p>A sequence of actions forms a <b>path</b>, and a <b>solution</b> is a path from the initial state to a goal state. The cost of a path is the sum of the costs of each action on that path, so an <b>optimal solution</b> is the path with the lowest cost among all solutions. Everything can be represented as a <b>graph</b> where the vertices are the states and the edges are the actions.</p>
      <h2 id="exampleproblems">Example Problems</h2>
      <h3>Standardized problems</h3>
      <p>A <b>grid world</b> problem is an array of cells where agents can move from cell to cell and each cell can contain objects. Our little vacuum example is an example of a grid world problem.</p>
      <ul>
        <li><b>states</b>: there are `8` different states depending on which objects are in which cells
          <ul>
            <li>for each of the two cells, it can contain the vacuum or not and it can contain dirt or not (`2*2*2=8`)</li>
          </ul>
        </li>
        <li><b>initial state</b>: any state can be chosen as the initial state</li>
        <li><b>actions</b>: move left, move right, clean</li>
        <li><b>transition model</b>: cleaning removes dirt from the agent's cell and move left/right are self-explanatory</li>
        <li><b>goal states</b>: all the states where every cell is clean
          <ul>
            <li>both cells are clean and the vacuum ends up in the left cell</li>
            <li>both cells are clean and the vacuum ends up in the right cell</li>
          </ul>
        </li>
        <li><b>action cost</b>: each action costs `1`</li>
      </ul>
      <div class="ln-box">
        <p>This one's interesting. Donald Knuth conjectured that a sequence of square root, floor, and factorial operations can be applied to the number `4` to reach any positive integer. For example, we can reach `5` by applying eight operations:</p>
        <div class="ln-center">
          <p>`lfloorsqrt(sqrt(sqrt(sqrt(sqrt((4!)!)))))rfloor=5`</p>
        </div>
        <ul>
          <li><b>states</b>: positive real numbers</li>
          <li><b>initial state</b>: `4`</li>
          <li><b>actions</b>: apply square root, floor, or factorial operation</li>
          <li><b>transition model</b>: the effect of applying the operations are mathematically defined</li>
          <li><b>goal state</b>: the desired positive integer</li>
          <li><b>action cost</b>: each action costs `1`</li>
        </ul>
      </div>
      <h3>Real-world problems</h3>
      <p>A common type of real-world problem is the <b>route-finding problem</b> (finding an optimal path from point A to point B). For example, airline travel:</p>
      <ul>
        <li><b>states</b>: location and current time</li>
        <li><b>initial state</b>: our home airport</li>
        <li><b>actions</b>: take any flight from the current location</li>
        <li><b>transition model</b>: the flight's destination will become our new current location and the flight's arrival time will become our new current time</li>
        <li><b>goal state</b>: our destination
          <ul>
            <li>the goal can also be "arrive at the destination on a nonstop flight"</li>
          </ul>
        </li>
        <li><b>action cost</b>: combination of monetary cost, waiting time, flight time, seat quality, time of day, type of airplane, frequent-flyer reward points, etc.</li>
      </ul>
      <div class="ln-box">
        <p><b>Touring problems</b> is an extension of the route-finding problem where multiple locations must be visited instead of just one destination. The <b>traveling salesperson problem</b> is a popular example.</p>
      </div>
      <h2 id="searchalgorithms">Search Algorithms</h2>
      <p>Search problems can be represented as a <b>search tree</b>, where a <b>node</b> represents a state and the edges represent the actions. For Knuth's conjecture:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/search_tree.png">
        <p>The purple nodes are nodes that have been expanded. The green nodes are nodes that are on the frontier, i.e., they have not been expanded yet.</p>
      </div>
      <p>When we're at a node, we can <b>expand</b> the node to consider the available states that are possible (<b>generate</b> nodes). The nodes that have not been expanded yet make up the <b>frontier</b> of the search tree.</p>
      <h3>Best-first search</h3>
      <p>So how do we decide which path to take? A good general approach is to perform <b>best-first search</b>. In this type of search, we define an <b>evaluation function</b>, which, in a sense, is a measure of how close we are to reaching the goal state. When we apply the evaluation function to each frontier node, the node with the minimum value of the evaluation function should be the next node to go to.</p>
      <div class="ln-box">
        <p>One example of an evaluation function for Knuth's conjecture could be</p>
        <div class="ln-center">
          <p>`f(n) = abs(n-G)`</p>
        </div>
        <p>where `n` is the current node and `G` is the goal. This evaluation function is measuring the distance between where we are and where we want to be, so it makes sense to choose the node that minimizes this distance. Unfortunately, this isn't a very helpful evaluation function to use because it would tell us to keep applying the floor operation.</p>
      </div>
      <h3>Search data structures</h3>
      <p>We can implement a node by storing:</p>
      <ul>
        <li>state: the state of the node</li>
        <li>parent: the node that generated this node</li>
        <li>action: the action that was applied to the parent to generate this node</li>
        <li>path-cost: the total cost of the path from the initial state to this node</li>
      </ul>
      <p>We can store the frontier using a <b>queue</b>. A queue is good because it provides us with the operations to check if a frontier is empty, take the top node from the frontier, look at the top node of the frontier, and add a node to the frontier.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function BEST-FIRST-SEARCH(problem, f)<br>&nbsp;&nbsp;node = NODE(STATE = problem.INITIAL)<br>&nbsp;&nbsp;frontier = priority queue ordered by f<br>&nbsp;&nbsp;reached = a lookup table<br>&nbsp;&nbsp;while not IS-EMPTY(frontier) do<br>&nbsp;&nbsp;&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s = child.STATE<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if s is not in reached or child.PATH-COST &lt; reached[s].PATH-COST then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reached[s] = child<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;return failure</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function EXPAND(problem, node)<br>&nbsp;&nbsp;s = node.STATE<br>&nbsp;&nbsp;for each action in problem.ACTIONS do<br>&nbsp;&nbsp;&nbsp;&nbsp;s' = problem.RESULT(s, action)<br>&nbsp;&nbsp;&nbsp;&nbsp;cost = node.PATH-COST + problem.ACTION-COST(s, action, s')<br>&nbsp;&nbsp;&nbsp;&nbsp;yield NODE(STATE = s', PARENT = node, ACTION = action, PATH-COST = cost)</code></p>
        </div>
      </div>
      <h3>Redundant paths</h3>
      <p>As we traversing the tree, it's possible to get stuck in a <b>cycle</b> (<b>loopy path</b>), i.e., go around in circles through <b>repeated states</b>. Here's an example of a cycle:</p>
      <div class="ln-center">
        <p>`4 ubrace(rarr)_(text(factorial)) 24 ubrace(rarr)_(text(square root)) 4.9 ubrace(rarr)_(text(floor)) 4`</p>
      </div>
      <p>A cycle is a type of <b>redundant path</b>, which is a path that has more steps than necessary. Here's an example of a redundant path:</p>
      <div class="ln-center">
        <p>`4 ubrace(rarr)_(text(floor)) 4 ubrace(rarr)_(text(square root)) 2`</p>
        <p>(This is redundant because we could've just gone to `2` directly without needing the first floor operation.)</p>
      </div>
      <p>There are three ways to handle the possibility of running into redundant paths. The preferred way is to keep track of all the states we've been to. That way, if we see it again, then we know not to go that way. (The best-first search pseudocode does this.)</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/kansas.png">
      </div>
      <p>The bad part about doing this is that there might not be enough memory to keep track of all this info. So we can check for cycles instead of redundant paths. We can do this by following the chain of parent pointers ("going up the tree") to see if the state at the end has appeared earlier in the path.</p>
      <p>Or we can choose to not even worry about choosing redundant paths if it's rare or impossible to have them. From this point on, we'll use <b>graph search</b> for search algorithms that check redundant paths and <b>tree-like search</b> for ones that don't.</p>
      <h2 id="uninformedsearchstrategies">Uninformed Search Strategies</h2>
      <p>The uninformed means that as the agent is performing the search algorithm, it doesn't know how close the current state is to the goal.</p>
      <h3>Breadth-first search</h3>
      <p>In breadth-first search, all the nodes on the same level are expanded first before going further down any paths. This is generally a good strategy to consider when all the actions have the same cost.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/breadth_first_search.gif">
      </div>
      <div class="ln-box">
        <p>Breadth-first search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is the depth of the node, i.e., the number of actions it takes to reach the node.</p>
      </div>
      <div class="ln-box">
        <p>There are several optimizations we can take advantage of when using breadth-first search. Using a first-in-first-out (FIFO) queue is faster than using a priority queue because the nodes that get added to a FIFO queue will already be in the correct order (so there's no need to perform a sort, which we would need to do with a priority queue).</p>
        <p>Best-first search waits until the node is popped off the queue before checking if it's a solution (<b>late goal test</b>). But with breadth-first search, we can check if a node is a solution as soon as it's generated (<b>early goal test</b>). This is because traversing breadth first explores all the possible paths above the generated node, guaranteeing that we've found the shortest path to it. (Think about it like this: because we're traversing breadth first, we can't say, "Would we have found a shorter way here if we had gone on that path instead?" because we already checked that path.)</p>
        <p>Because of this, breadth-first search is cost-optimal for problems where all actions have the same cost.</p>
      </div>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function BREADTH-FIRST-SEARCH(problem)<br>&nbsp;&nbsp;node = NODE(problem.INITIAL)<br>&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;frontier = a FIFO queue<br>&nbsp;&nbsp;reached = {problem.INITIAL}<br>&nbsp;&nbsp;while not IS-EMPTY(frontier) do<br>&nbsp;&nbsp;&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s = child.STATE<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if problem.IS-GOAL(s) then return child<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if s is not in reached then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add s to reached<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;return failure</code></p>
        </div>
      </div>
      <div class="ln-box">
        <p>Let `d` represent the depth of the search tree. Suppose each node generates `b` child nodes.</p>
        <p>The root node generates `b` nodes. Each of those `b` nodes generates `b` nodes (total `b^2` nodes). And each of the `b^2` nodes generates `b` nodes (total `b^3` nodes). So the total number of nodes generated is</p>
        <div class="ln-center">
          <p>`1+b+b^2+b^3+...+b^d=O(b^d)`</p>
        </div>
        <p>Time and space complexity are exponential, which is bad. (All the generated nodes have to remain in memory.)</p>
      </div>
      <h3>Dijkstra's algorithm or uniform-cost search</h3>
      <p>If actions have different costs, then <b>uniform-cost search</b> is a better consideration. The main idea behind this is that the least-cost paths are explored first. (I go into more step-by-step detail about how Dijkstra's algorithm works in my Computer Networking resource.)</p>
      <p>Uniform-cost search will always find the optimal solution, but the main problem with it is that it's inefficient. It will expand a lot of nodes.</p>
      <div class="ln-box">
        <p>Uniform-cost search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is the cost of the path from the root to the current node.</p>
      </div>
      <div class="ln-box">
        <p>The worst-case time and space complexity is `O(b^(1+lfloorC^**//epsilonrfloor))` where `C^**` is the cost of the optimal solution and `epsilon` is a lower bound on the cost of all actions.</p>
        <p>Since costs are positive (`gt 0`), going down one path by one node may be "deeper" than going down another path by one node, so we can't use the same `d` notation for depth. So `C^(**)/epsilon` is an upper bound on how deep the solution is, i.e., the max number of actions that are taken to reach the solution.</p>
      </div>
      <h3>Depth-first search</h3>
      <p>In <b>depth-first search</b>, the deepest node in the frontier is expanded first. Basically, one path at a time is picked and followed until it ends. So if a goal state is found, then that path is chosen as the solution. This also means that the path chosen may not be the cheapest path available. (What if we had gone down that path instead?)</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/depth_first_search.gif">
      </div>
      <div class="ln-box">
        <p>So why should this algorithm even be considered? It's because depth-first search doesn't require a lot of memory. There's no need for a reached table and the frontier is very small.</p>
        <p>The frontier of breadth-first search can be thought of as the surface of an expanding sphere while the frontier of depth-first search can be thought of as the radius of that sphere.</p>
        <p>The memory complexity is `O(bm)` where `b` is the branching factor (number of nodes on a branch) and `m` is the maximum depth of the tree. Whenever we expand a node, we add `b` generated children to the frontier. Then we move to one of the children, generate its `b` children, and add them to the frontier. And so on until the bottom. This happens `m` times. So at the bottom, there are `m` groups of `b` children in the frontier.</p>
      </div>
      <div class="ln-box">
        <p>Depth-first search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is the negative of the depth.</p>
      </div>
      <h3>Depth-limited and iterative deepening search</h3>
      <p>Another problem with depth-first search is that it can keep going down one path infinitely. To prevent this, we can decide to stop going down a path once we reach a certain point. This is <b>depth-limited search</b>.</p>
      <div class="ln-box">
        <p>The time complexity is `O(b^l)` and the space complexity is `O(bl)` for the same reasons as depth-first search.</p>
      </div>
      <p>But how far is too far? If we stop too early, we may not find a solution and if we choose to stop very late, it will take a long time. <b>Iterative deepening search</b> keeps trying different stopping points until a solution is found or until no solution is found (if this is the case, then we have gone deep enough to the point where all paths can be fully explored).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/iterative_deepening_search.gif">
      </div>
      <div class="ln-box">
        <p>It may seem a bit repetitive (or wasteful) to keep generating the top-level nodes. For example, `B` and `C` are generated `3` times at limits `1`, `2`, and `3`. However, this isn't really that big of an issue since the majority of the nodes are at the bottom, and those are only generated a few times compared to the top-level nodes.</p>
        <p>In general, the nodes at the bottom level are generated once, those on the next-to-bottom level are generated twice, ..., and the children of the root are generated `d` times. So the total number of nodes generated is</p>
        <div class="ln-center">
          <p>`(d)b^1+(d-1)b^2+(d-2)b^3+...+b^d=O(b^d)`</p>
        </div>
        <p>So the time complexity is `O(b^d)` when there's a solution (and `O(b^m)` when there isn't).</p>
      </div>
      <div class="ln-box">
        <p>The memory complexity is `O(bd)` when there is a solution and `O(bm)` when there isn't (for finite state spaces).</p>
      </div>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function DEPTH-LIMITED-SEARCH(problem, l)<br>&nbsp;&nbsp;frontier = a LIFO queue<br>&nbsp;&nbsp;result = failure<br>&nbsp;&nbsp;while not IS-EMPTY(frontier) do<br>&nbsp;&nbsp;&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;&nbsp;&nbsp;if DEPTH(node) &gt; l then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result = cutoff<br>&nbsp;&nbsp;&nbsp;&nbsp;else if not IS-CYCLE(node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;return result</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function ITERATIVE-DEEPENING-SEARCH(problem)<br>&nbsp;&nbsp;for depth = 0 to infinity do<br>&nbsp;&nbsp;&nbsp;&nbsp;result = DEPTH-LIMITED-SEARCH(problem, depth)<br>&nbsp;&nbsp;&nbsp;&nbsp;if result != cutoff then return result</code></p>
        </div>
        <p><code>cutoff</code> is a string used to denote that there might be a solution at a deeper level than `l`.</p>
      </div>
      <p>Iterative deepening is the preferred uninformed search method when the search state space is larger than can fit in memory and the depth of the solution is not known.</p>
      <h3>Bidirectional search</h3>
      <p>So far, all the algorithms started at the initial state and moved towards a goal state. But what we can also do is start from the initial state <em>and</em> the goal state(s) and hope that we meet somewhere in the middle. This is <b>bidirectional search</b>.</p>
      <p>We need to keep track of two frontiers and two tables of reached states. There is a solution when the two frontiers meet.</p>
      <div class="ln-box">
        <p>Even though we have to keep track of two frontiers, each frontier is only half the size of the whole tree. So the time and space complexity is `O(b^(d//2))`, which is less than `O(b^d)`.</p>
      </div>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function BIBF-SEARCH(problem_F, f_F, problem_B, f_B)<br>&nbsp;&nbsp;node_F = NODE(problem_F.INITIAL)<br>&nbsp;&nbsp;node_B = NODE(problem_B.INITIAL)<br>&nbsp;&nbsp;frontier_F = a priority queue ordered by f_F<br>&nbsp;&nbsp;frontier_B = a priority queue ordered by f_B<br>&nbsp;&nbsp;reached_F = a lookup table<br>&nbsp;&nbsp;reached_B = a lookup table<br>&nbsp;&nbsp;solution = failure<br>&nbsp;&nbsp;while not TERMINATED(solution, frontier_F, frontier_B) do<br>&nbsp;&nbsp;&nbsp;&nbsp;if f_F(TOP(frontier_F)) &lt; f_B(TOP(frontier_B)) then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;solution = PROCEED(F, problem_F, frontier_F, reached_F, reached_B, solution)<br>&nbsp;&nbsp;&nbsp;&nbsp;else solution = PROCEED(B, problem_B, frontier_B, reached_B, reached_F, solution)<br>&nbsp;&nbsp;return solution</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function PROCEED(dir, problem, frontier, reached, reached_2, solution)<br>&nbsp;&nbsp;node = POP(frontier)<br>&nbsp;&nbsp;for each child in EXPAND(problem, node) do<br>&nbsp;&nbsp;&nbsp;&nbsp;s = child.STATE<br>&nbsp;&nbsp;&nbsp;&nbsp;if s not in reached or PATH-COST(child) &lt; PATH-COST(reached[s]) then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reached[s] = child<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add child to frontier<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if s is in reached_2 then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;solution_2 = JOIN-NODES(dir, child, reached_2[s])<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if PATH-COST(solution_2) &lt; PATH-COST(solution) then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;solution = solution_2<br>&nbsp;&nbsp;return solution</code></p>
        </div>
        <p><code>TERMINATED</code> determines when to stop looking for solutions and <code>JOIN-NODES</code> joins the two paths.</p>
      </div>
      <div class="ln-box">
        <h3>Comparing uninformed search algorithms</h3>
        <p>An algorithm is <b>complete</b> if either a solution is guaranteed to be found if it exists or a failure is reported when it doesn't exist.</p>
        <table class="table">
          <tr>
            <th>Criterion</th>
            <th>Breadth-First</th>
            <th>Uniform-Cost</th>
            <th>Depth-First</th>
            <th>Depth-Limited</th>
            <th>Iterative Deepening</th>
            <th>Bidirectional</th>
          </tr>
          <tr>
            <td>Complete?</td>
            <td>Yes<sup>*</sup></td>
            <td>Yes<sup>*</sup></td>
            <td>No</td>
            <td>No</td>
            <td>Yes<sup>*</sup></td>
            <td>Yes<sup>*</sup></td>
          </tr>
          <tr>
            <td>Optimal cost?</td>
            <td>Yes<sup></sup></td>
            <td>Yes</td>
            <td>No</td>
            <td>No</td>
            <td>Yes<sup></sup></td>
            <td>Yes<sup></sup></td>
          </tr>
          <tr>
            <td>Time</td>
            <td>`O(b^d)`</td>
            <td>`O(b^(1+lfloorC^**//epsilonrfloor))`</td>
            <td>`O(b^m)`</td>
            <td>`O(b^l)`</td>
            <td>`O(b^d)`</td>
            <td>`O(b^(d//2))`</td>
          </tr>
          <tr>
            <td>Space</td>
            <td>`O(b^d)`</td>
            <td>`O(b^(1+lfloorC^**//epsilonrfloor))`</td>
            <td>`O(bm)`</td>
            <td>`O(bl)`</td>
            <td>`O(bd)`</td>
            <td>`O(b^(d//2))`</td>
          </tr>
        </table>
        <p>`b` is the branching factor. `m` is the maximum depth of the search tree. `d` is the depth of the shallowest solution, or is `m` when there is no solution. `l` is the depth limit.</p>
        <p><sup>*</sup>if `b` is finite</p>
        <p><sup></sup>if all action costs are `ge epsilon gt 0`</p>
        <p><sup></sup>if all action costs are identical</p>
        <p><sup></sup>if both directions are breadth-first or uniform-cost</p>
      </div>
      <h2 id="informedsearchstrategies">Informed (Heuristic) Search Strategies</h2>
      <p>Informed means that information is provided about how close the current state is to a goal state.</p>
      <div class="ln-box">
        <p>The information is referred to as "hints", which are given by a <b>heuristic function</b>, denoted as `h(n)`.</p>
        <div class="ln-center">
          <p>`h(n)=` estimated cost of the cheapest path from the state at node `n` to a goal state</p>
        </div>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/maze.png">
      </div>
      <p>For the next few algorithms, we'll be using this maze, where the goal is to find the shortest path from `S` to `G`. The number in each square is the number of squares it is away from the goal (ignoring walls). That is the heuristic we'll be using.</p>
      <h3>Greedy best-first search</h3>
      <p>If we know how close each node is to a goal, then it seems to makes sense to choose the node that seems closest to the goal.</p>
      <div class="ln-box">
        <p>We expand first the node with the lowest `h(n)` value.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/greedy_search_1.png">
      </div>
      <p>Since we're being greedy, we'll move to the square that looks like it's `11` squares away from the goal instead of the square that looks like it's `13` squares away from the goal.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/greedy_search_2.png">
      </div>
      <p>Now obviously, this is not the optimal path. But being greedy is not always a good thing.</p>
      <h3>A* search</h3>
      <p>The reason greedy best-first search failed is because it didn't take into consideration the cost of going to each node -- it only looked at how far away each node was to the destination. Intuitively, if it takes a long time to get to a destination that is close to your goal, is it really worth going to that destination? <b>A* search</b> looks at both the cost to get to each node and the estimated cost from each node to the destination.</p>
      <div class="ln-box">
        <p>A* search can be implemented as a call to <code>BEST-FIRST-SEARCH</code> where the evaluation function is `g(n)+h(n)`. `g(n)` is the cost from the initial state to node `n`, and `h(n)` is the estimated cost of the shortest path from `n` to a goal state.</p>
        <p>`f(n)=` estimated cost of the best path that continues from `n` to a goal</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/a_star_search_1.png">
      </div>
      <p>Now, the numbers in each square represent the cost of going to that square plus the estimated distance to the goal, i.e., the number of squares it is away from the goal (ignoring walls).</p>
      <p>Just like in greedy search, we'll be going up again. But don't worry, things will be different this time.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/a_star_search_2.png">
      </div>
      <p>At this point, we see that it's no longer worth continuing on this path because there is a cheaper option (`19`) at the fork. So we'll start going that way instead.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/a_star_search_3.png">
      </div>
      <p>Even though we wasted some time going down the longer path, we still found the optimal solution.</p>
      <div class="ln-box">
        <p>Whether A* is cost-optimal depends on the heuristic. If the heuristic is <b>admissible</b>, then A* is cost-optimal. Admissible means that the heuristic never overestimates the cost to reach a goal, i.e., the estimated cost to reach the goal is `le` the actual cost to reach the goal. Admissible heuristics are referred to as being optimistic.</p>
      </div>
      <div class="ln-box">
        <p>We can show A* is cost-optimal with an admissible heuristic by using a proof by contradiction.</p>
        <p>Let `C^**` be the cost of the optimal path. Suppose for the sake of contradiction that the algorithm returns a path with cost `C gt C^**`. This means that there is some node `n` that is on the optimal path and is unexpanded (if we had expanded it, we would've found the optimal path and returned `C^**` instead).</p>
        <p>Let `g^(ast)(n)` be the cost of the optimal path from the start to `n`, and let `h^(ast)(n)` be the cost of the optimal path from `n` to the nearest goal.</p>
        <div class="ln-center">
          <p>`f(n) gt C^**` (otherwise `n` would've been expanded)</p>
          <p>`f(n) = g(n)+h(n)` (by definition)</p>
          <p>`f(n) = g^(ast)(n)+h(n)` (because `n` is on the optimal path)</p>
          <p>`f(n) le g^(ast)(n)+h^(ast)(n)` (because of admissibility, `h(n) le h^(ast)(n)`)</p>
          <p>`f(n) le C^**` (by definition, `C^(ast)=g^(ast)(n)+h^(ast)(n)`)</p>
        </div>
        <p>The first and last lines form a contradiction. So it must be the case that the algorithm returns a path with cost `C le C^**`, i.e., A* is cost-optimal.</p>
      </div>
      <div class="ln-box">
        <p>A heuristic can also be <b>consistent</b>, which is slightly stronger than being admissible (so every consistent heuristic is admissible). A heuristic `h(n)` is consistent if, for every node `n` and every successor `n'` of `n` generated by action `a`,</p>
        <div class="ln-center">
          <p>`h(n) le c(n,a,n') + h'(n)`</p>
        </div>
        <p>What this is saying is that the estimated cost to the goal from our current position should be less than (or equal to) the cost of going to another location plus the estimated cost of going to the goal from that location. In other words, adding a stop should make the trip longer unless that stop is on the way to the destination.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/consistent_heuristic.png">
        </div>
        <p>Consistency is a form of the <b>triangle inequality</b>, which says that a side of a triangle can't be longer than the sum of the other two sides.</p>
      </div>
      <div class="ln-box">
        <p>Here's an example of how things can be not optimal if the heuristic is not admissible/consistent.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/not_admissible.png">
        </div>
        <p>When we're starting at `S`, we can either choose to go to `A` or to `G` directly. Since the estimated cost of `A rarr G` is `6`, it looks more expensive to go to `A` first, so we go to `G` directly instead (`f(A)=1+6=7 gt f(G)=5+0=5`).</p>
        <p>However, `S rarr G` is not the optimal path! `S rarr G` costs `5` while `S rarr A rarr G` costs `4`.</p>
        <p>This happened because we overestimated the distance from `A` to `G`.</p>
      </div>
      <div class="ln-box">
        <p>As we go down a path, the `g` costs should be increasing because action costs are always positive. For our maze example, the closer we get to `G`, the farther we get from `S`. So the distance from `S` to our current position should always be increasing as we keep moving. The `g` costs are (strictly) <b>monotonic</b>.</p>
        <p>So does this mean that `f=g+h` will also monotonically increase? As we move from `n` to `n'`, the cost goes from `g(n)+h(n)` to `g(n)+c(n,a,n')+h(n')`. Canceling out the `g(n)`, we see that the path's cost will be monotonically increasing if and only if `h(n) le c(n,a,n')+h(n')`, i.e., if and only if the heuristic is consistent.</p>
        <p>So if A* search is using a consistent heuristic, then:</p>
        <ul>
          <li>all nodes that can be reached from the initial state and satisfy `f(n) lt C^(**)` will be expanded (<b>surely expanded nodes</b>)
            <ul>
              <li>this is because `f` is monotonically increasing
                <ul>
                  <li>if `f` wasn't monotonically increasing, then if we expand a node, there could be a successor with a lower `f` value, which means there was a better path to get to that successor that we missed (i.e., a node with `f(n) lt C^(**)` wasn't expanded)
                    <ul>
                      <li>but A* works by picking the nodes with the lowest `f`-values to expand, so we couldn't have missed that better path -- there's a contradiction</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>this means that all potential paths with lower costs are explored (ensuring cost-optimality and completeness I think)</li>
            </ul>
          </li>
          <li>some nodes where `f(n)=C^(**)` might be expanded</li>
          <li>nodes where `f(n) gt C^(**)` are not expanded</li>
        </ul>
        <p>These three things lead to A* with a consistent heuristic being referred to as <b>optimally efficient</b>.</p>
      </div>
      <h3>Satisficing search: Inadmissible heuristics and weighted A*</h3>
      <p>Even though A* is complete, cost-optimal, and optimally efficient, the one big problem with A* is that it expands a lot of nodes, which takes time and uses up space. We can expand fewer nodes by using an <b>inadmissible heuristic</b>, which may overestimate the estimated cost to the goal. Intuitively, if the estimated costs of some of the nodes are higher, then there will be fewer nodes that look favorable to expand. Of course, if we overestimate, then there's a chance we might miss an optimal path, but this is the price to pay for expanding fewer nodes. We might end up with suboptimal, but "good enough" solutions (<b>satisficing</b> solutions).</p>
      <p>Inadmissible heuristics are potentially more accurate. Consider a heuristic that uses a straight-line distance to estimate the distance of a road between two cities. The path between two cities is almost never a straight line, so the length of the actual path will be greater than the length of the straight line. Meaning that overestimating the straight-line distance by a little bit will make it closer to the length of the actual path.</p>
      <p>We don't actually need an inadmissible heuristic to take advantage of the faster speed and lower memory usage. We can simulate an inadmissible heuristic by adding some weight to the heuristic.</p>
      <div class="ln-box ln-center">
        <p>`f(n)=g(n)+W*h(n)`</p>
        <p>for some `W gt 1`</p>
      </div>
      <p>This is <b>weighted A* search</b>.</p>
      <div class="ln-box">
        <p>In general, if the optimal cost is `C^(**)`, weighted A* search will find a solution that costs between `C^(**)` and `W*C^(**)`. In practice, it's usually much closer to `C^(**)` than to `W*C^(**)`.</p>
      </div>
      <div class="ln-box">
        <p>Weighted A* can be seen as a generalization of some of the other search algorithms:</p>
        <table class="table">
          <tr>
            <td>A* search</td>
            <td>`g(n)+h(n)`</td>
            <td>`W=1`</td>
          </tr>
          <tr>
            <td>Uniform-cost search</td>
            <td>`g(n)`</td>
            <td>`W=0`</td>
          </tr>
          <tr>
            <td>Greedy best-first search</td>
            <td>`h(n)`</td>
            <td>`W=oo`</td>
          </tr>
          <tr>
            <td>Weighted A* search</td>
            <td>`g(n)+W*h(n)`</td>
            <td>`1 lt W lt oo`</td>
          </tr>
        </table>
      </div>
      <h3>Memory-bounded search</h3>
      <p><b>Beam search</b> keeps only `k` nodes with the best `f`-scores in the frontier. This means that we may be excluding cost-optimal paths (and even solutions), but it's the price to pay to limit the number of expanded nodes. Though for many problems, it can find near-optimal solutions.</p>
      <p><b>Iterative-deepening A* search</b> (IDA*) is a commonly used algorithm for problems with limited memory. Where iterative deepening search uses depth as the cutoff, iterative-deepening A* search uses `f`-cost as the cutoff. Specifically, the cutoff value is the smallest `f`-cost of any node that exceeded the cutoff on the previous iteration.</p>
      <p>Once the cutoff is reached, IDA* starts over from the beginning. <b>Recursive best-first search</b> (RBFS) also uses `f`-values to determine how far down a path it should go, but once the cutoff is reached, it steps back and takes another path (the one with the second-lowest `f`-value) instead of starting over.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function RECURSIVE-BEST-FIRST-SEARCH(problem)<br>&nbsp;&nbsp;solution, fvalue = RBFS(problem, NODE(problem.INITIAL), infinity)<br>&nbsp;&nbsp;return solution</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function RBFS(problem, node, f_limit)<br>&nbsp;&nbsp;if problem.IS-GOAL(node.STATE) then return node<br>&nbsp;&nbsp;successors = LIST(EXPAND(node))<br>&nbsp;&nbsp;if successors is empty then return failure, infinity<br>&nbsp;&nbsp;for each s in successors do<br>&nbsp;&nbsp;&nbsp;&nbsp;s.f = max(s.PATH-COST + h(s), node.f)<br>&nbsp;&nbsp;while true do<br>&nbsp;&nbsp;&nbsp;&nbsp;best = the node in successors with the lowest f-value<br>&nbsp;&nbsp;&nbsp;&nbsp;if best.f &gt; f_limit then return failure, best.f<br>&nbsp;&nbsp;&nbsp;&nbsp;alternative = the second lowest f-value among successors<br>&nbsp;&nbsp;&nbsp;&nbsp;result, best.f = RBFS(problem, best, min(f_limit, alternative))<br>&nbsp;&nbsp;&nbsp;&nbsp;if result != failure then return result, best_f</code></p>
        </div>
      </div>
      <h3>Bidirectional heuristic search</h3>
      <p>Bidirectional search is sometimes more efficient than unidirectional search, sometimes not. If we're using a very good heuristic, then A* search will perform well and adding bidirectional search won't add any more value. If we're using an average heuristic, bidirectional search tends to expand fewer nodes. If we're using a bad heuristic, then both directional and unidirectional search will be equally bad.</p>
      <div class="ln-box">
        <p>If we want to use bidrectional search with a heuristic, it turns out that using `f(n)=g(n)+h(n)` as the evaluation function doesn't guarantee that we'll find an optimal-cost solution. Instead, we need two different evaluation functions, one for each direction:</p>
        <div class="ln-center">
          <p>`f_F(n)=g_F(n)+h_F(n)` for going in the forward direction (initial state to goal)</p>
          <p>`f_B(n)=g_B(n)+h_B(n)` for going in the backward direction (goal to initial state)</p>
        </div>
        <p>This is a <b>front-to-end</b> search.</p>
      </div>
      <div class="ln-box">
        <p>Suppose the heuristic is admissible. Let's say that we're in the middle of running the algorithm and the forward direction is currently at node `j` and the backward direction is currently at node `t`. We can define a lower bound on the cost of a solution path `text(initial) rarr ... rarr j rarr ... rarr t rarr ... rarr text(goal)` as</p>
        <div class="ln-center">
          <p>`lb(j,t)=max(g_F(j)+g_B(t), f_F(j), f_B(t))`</p>
        </div>
        <p>The optimal cost must be at least the cost of `text(initial) rarr ... rarr j` plus the cost of `text(goal) rarr ... rarr t`.</p>
        <p>Since we're using an admissible heuristic, the estimated costs are underestimated. So the optimal cost must also be at least the cost of `text(initial) rarr ... rarr j` plus the estimated cost of `j rarr ... rarr text(goal)`. The same reasoning applies for `f_B(t)`.</p>
        <p>So for any pair of nodes `j,t` with `lb(j,t) lt C^(**)`, either `j` or `t` must be expanded because the path that goes through them is a potential optimal solution. The problem is that we don't know for sure which node is better to expand -- that part's just up to luck.</p>
        <p>Either way, this lower bound gives us an idea for an evaluation function we could use:</p>
        <div class="ln-center">
          <p>`f_2(n)=max(2g(n), g(n)+h(n))`</p>
        </div>
        <p>This function guarantees that a node with `g(n) gt C^(**)/2` will never be expanded. If `g(n) gt C^(**)/2`, then `2g(n) gt C^(**)`.</p>
        <ul>
          <li>if `2g(n) ge g(n)+h(n)`, then `f_2(n)=2g(n) gt C^(**)`</li>
          <li>if `2g(n) lt g(n)+h(n)`, then `f_2(n)=g(n)+h(n) gt 2g(n) gt C^(**)`</li>
        </ul>
        <p>So `f_2(n) gt C^(**)` for nodes with `g(n) gt C^(**)/2`. But nodes on optimal paths will have `f_2(n) le C^(**)`, so those will be expanded first.</p>
      </div>
      <div class="ln-box">
        <p>Here's another example of greedy search, A* search, and RBFS from the textbook.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/map.png">
        </div>
        <p>The map uses straight lines, but it's just a drawing. The roads aren't necessarily straight lines.</p>
        <div class="row">
          <div class="col-sm">
            <table class="table">
              <tr>
                <th>City</th>
                <th>Straight-line distance to B</th>
              </tr>
              <tr>
                <td>A</td>
                <td>366</td>
              </tr>
              <tr>
                <td>B</td>
                <td>0</td>
              </tr>
              <tr>
                <td>C</td>
                <td>160</td>
              </tr>
              <tr>
                <td>D</td>
                <td>242</td>
              </tr>
              <tr>
                <td>E</td>
                <td>161</td>
              </tr>
              <tr>
                <td>F</td>
                <td>176</td>
              </tr>
              <tr>
                <td>G</td>
                <td>77</td>
              </tr>
              <tr>
                <td>H</td>
                <td>151</td>
              </tr>
              <tr>
                <td>I</td>
                <td>226</td>
              </tr>
              <tr>
                <td>L</td>
                <td>244</td>
              </tr>
            </table>
          </div>
          <div class="col-sm">
            <table class="table">
              <tr>
                <th>City</th>
                <th>Straight-line distance to B</th>
              </tr>
              <tr>
                <td>M</td>
                <td>241</td>
              </tr>
              <tr>
                <td>N</td>
                <td>234</td>
              </tr>
              <tr>
                <td>O</td>
                <td>380</td>
              </tr>
              <tr>
                <td>P</td>
                <td>100</td>
              </tr>
              <tr>
                <td>R</td>
                <td>193</td>
              </tr>
              <tr>
                <td>S</td>
                <td>253</td>
              </tr>
              <tr>
                <td>T</td>
                <td>329</td>
              </tr>
              <tr>
                <td>U</td>
                <td>80</td>
              </tr>
              <tr>
                <td>V</td>
                <td>199</td>
              </tr>
              <tr>
                <td>Z</td>
                <td>374</td>
              </tr>
            </table>
          </div>
        </div>
        <h3>Greedy best-first search</h3>
        <p>Let's say we're starting at `A` and we want to go to `B`. If we use the <b>straight-line distance</b> heuristic, then we get the path `A ubrace(rarr)_(140) S ubrace(rarr)_(99) F ubrace(rarr)_(211) B`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/greedy_best_first_search.gif">
          <p>The numbers are the straight-line distances from that node to `B`.</p>
        </div>
        <p>The cost of that path is `450`, but that's not the path with the least cost. The path `A ubrace(rarr)_(140) S ubrace(rarr)_(80) R ubrace(rarr)_(97) P ubrace(rarr)_(101) B` has a lower cost with `418`.</p>
        <h3>A* Search</h3>
        <div class="ln-center">
          <img class="img-fluid" src="../pictures/ai/a_star_search.gif">
        </div>
        <p>Notice that `B` is found when expanding `F`, but since there's a lower cost available to consider, we don't accept the `F rarr B` path as a solution yet.</p>
        <p>A* is efficient because it <b>prunes</b> away nodes that are not necessary for finding an optimal solution. Here, `T` and `Z` are pruned away, but would've been expanded in uniform-cost or breadth-first search.</p>
        <h3>Recursive best-first search</h3>
        <div class="ln-center">
          <img class="img-fluid" src="../pictures/ai/rbfs.gif">
        </div>
        <p>Notice that when `P` is the successor with the lowest `f`-value, `P`'s `f`-value is higher than <code>f_limit</code>. So as we go back up to consider the alternative path, we update `R`'s `f`-value so we can check later if it's worth reexpanding `R`. (The same thing happens for `F`.) The `f`-value that is used as replacement is called the <b>backed-up value</b>, and it's the best `f`-value of the node's children.</p>
        <p>RBFS looks similar to A* search in that it looks for the lowest `f`-value. The difference is that RBFS only looks at the `f`-values of the current node's successors while A* search looks at all the nodes in the frontier.</p>
      </div>
      <h2 id="heuristicfunctions">Heuristic Functions</h2>
      <p>Here, we'll look at how the accuracy of a heuristic affects search performance and how we can come up with heuristics in the first place. We'll use the 8-puzzle as our main example. The 8-puzzle is a `3xx3` sliding-tile puzzle with `8` tiles and `1` blank space. We move tiles horizontally and vertically into the blank space until the goal state is reached (each move has a cost of `1`).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/8_puzzle.png">
        <p>The shortest solution has `26` moves.</p>
      </div>
      <p>Two commonly-used heuristics are</p>
      <ul>
        <li>`h_1=` the number of misplaced tiles (blank space not included)
          <ul>
            <li>for our example, `h_1=8` because all `8` tiles are misplaced</li>
            <li>this is admissible because the number of moves needed to reach a goal state will always be `ge` the number of misplaced tiles
              <ul>
                <li>if all `8` misplaced tiles need just `1` move to get to the correct spot, then the estimated cost is still not overestimated</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>`h_2=` the sum of the distances of the tiles from their goal positions
          <ul>
            <li>for our example, `h_2=3+1+2+2+2+3+3+2=18`
              <ul>
                <li>`1` is `3` squares away from its destination, `2` is `1` square away from its destination, `3` is `2` squares away from its destination, ...</li>
              </ul>
            </li>
            <li>this is sometimes called the <b>city-block distance</b> or <b>Manhattan distance</b></li>
            <li>this is admissible because the calculation of the distance doesn't take obstacles into account
              <ul>
                <li>`8` is `2` squares away from its destination, but it may actually need `ge 2` moves</li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
      <h3>The effect of heuristic accuracy on performance</h3>
      <p>One way to assess the quality of a heuristic is to look at the <b>effective branching factor</b> `b^(**)`. It can loosely be thought of as the average number of nodes that are generated at each depth. A good heuristic would have `b^(**)` close to `1`, which means that not a lot of nodes are being generated the farther we go down a path (we're fairly successful in finding good paths).</p>
      <div class="ln-box">
        <p>If A* finds a solution at depth `5` using `52` nodes, then the effective branching factor is `1.92`.</p>
      </div>
      <p>It turns out that `h_2` (the Manhattan distance) is always better than `h_1` (the number of misplaced tiles). This means that `h_2` <b>dominates</b> `h_1`. Because of this, A* search using `h_2` will never expand more nodes than A* using `h_1` (except for when there are ties).</p>
      <div class="ln-box">
        <p>Earlier we saw that every node with `f(n) lt C^(**)` will surely be expanded (assuming consistent heuristic).</p>
        <div class="ln-center">
          <p>`f(n) lt C^(**)`</p>
          <p>`g(n)+h(n) lt C^(**)`</p>
          <p>`h(n) lt C^(**)-g(n)`</p>
        </div>
        <p>So every node with `h(n) lt C^(**)-g(n)` will surely be expanded. By definition, `h_2(n) ge h_1(n)`, so every node that is surely expanded by A* with `h_2` is also surely expanded by A* with `h_1`. Mathematically, this is because `h_1(n) le h_2(n) lt C^(**)-g(n)`. But because `h_1` is small, there are more nodes that are `lt C^(**)-g(n)`, so A* with `h_1` will expand other nodes that A* with `h_2` wouldn't have expanded.</p>
        <p>So it's generally better to use a heuristic function with higher values, assuming it's consistent and it's not too computationally intensive.</p>
      </div>
      <h3>Generating heuristics from relaxed problems</h3>
      <p>So how do we come up with heuristics like `h_1` and `h_2` in the first place? (Perhaps more importantly, is it possible for a computer to come up with such a heuristic?)</p>
      <p>Let's make the 8-puzzle simpler so that tiles can be moved anywhere, even if there's another tile occupying the spot. Notice now that `h_1` would give the exact length of the shortest solution.</p>
      <p>Let's consider another simplified version where the tiles can still only move `1` square but, now, in any direction, even if the square is occupied. Notice that `h_2` would give the exact length of the shortest solution.</p>
      <p>These simplified versions, where there are fewer restrictions on the actions, are <b>relaxed problems</b>. The state-space graph of a relaxed problem is a supergraph of the original state space because the removal of restrictions allows for more possible states, resulting in more edges being added. This means that an optimal solution in the original problem is also an optimal solution in the relaxed problem. But the relaxed problem may have better solutions (this happens when the added edges provide shortcuts).</p>
      <p>That last part is how we can come up with heuristics. The cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem. (Also, if the heuristic is an exact cost for the relaxed problem, it must obey the triangle inequality, making it consistent.)</p>
      <p>So how can we come up with relaxed problems systematically?</p>
      <p>Let's describe the 8-puzzle actions as follows: a tile can move from square `X` to square `Y` if `X` is adjacent to `Y` and `Y` is blank. From this, we can generate three relaxed problems simply by removing one or both of the conditions:</p>
      <ul>
        <li>A tile can move from square `X` to square `Y` if `X` is adjacent to `Y`</li>
        <li>A tile can move from square `X` to square `Y` if `Y` is blank</li>
        <li>A tile can move from square `X` to square `Y`</li>
      </ul>
      <p>From the first one, we can derive `h_2` (the Manhattan distance) and from the third one, we can derive `h_1` (the number of misplaced tiles).</p>
      <div class="ln-box">
        <p>Ideally, relaxed problems generated by this technique should be solveable without search.</p>
      </div>
      <div class="ln-box">
        <p>If we generate a bunch of relaxed problems, we now also have a bunch of heuristics. If none of them are much better than the others, then which one should be used? One thing we can do is to pick the heuristic that gives the highest value:</p>
        <div class="ln-center">
          <p>`h(n)=max{h_1(n), ..., h_k(n)}`</p>
        </div>
        <p>But if it takes too long to compute all the heuristics, then we can also randomly pick one or use machine learning to predict which heuristic will be the best.</p>
      </div>
      <h3>Generating heuristics from subproblems: Pattern databases</h3>
      <p>Let's consider a <b>subproblem</b> of the 8-puzzle, where the goal is to get only tiles `1,2,3,4,` and the blank space into the correct spots.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/8_puzzle_subproblem.png">
      </div>
      <p>Since there are fewer tiles that have to be in a certain spot, the cost of the optimal solution of this subproblem is a lower bound on the cost of the complete problem, meaning that this could make a good admissible heuristic.</p>
      <p>For every possible subproblem, we can store the exact costs of their solutions in a <b>pattern database</b>. So when we're trying to solve the complete problem, we could look in our database for the subproblem that matches our current state and use the exact cost of the subproblem as the heuristic for the complete problem.</p>
      <p>We could have multiple databases for each subproblem type. For example, one database could be all the subproblem instances with tiles `1,2,3,4`. Another database could be all the subproblem instances with tiles `5,6,7,8`. Another database could be all the subproblem instances with tiles `2,4,6,8`. And so on. Then the heuristics from these databases can be combined by taking the maximum value. Doing this turns out to be more accurate than the Manhattan distance.</p>
      <h3>Generating heuristics with landmarks</h3>
      <p>This is kinda similar to using pattern databases, but kinda makes more sense when viewed in the context of route-finding problems.</p>
      <p>We can pick a few <b>landmark points</b> and <b>precompute</b> the optimal costs to these landmarks. More formally, for each landmark `L` and for each other vertex `v` in the graph, we compute and store `C^(**)(v,L)`. This may sound time-consuming, but it only needs to be done once and it will be infinitely useful.</p>
      <p>So if we're trying to estimate the cost to the goal, we can look at the cost to get there from a landmark.</p>
      <div class="ln-box">
        <p>The heuristic would be</p>
        <div class="ln-center">
          <p>`h_L(n)=min_(L in text(landmarks)) C^(**)(n,L)+C^(**)(L,text(goal))`</p>
        </div>
        <p>This heuristic is inadmissible though. If the optimal path is through a landmark, then this heuristic will be exact. If it isn't, then it will overestimate the optimal cost (the exact cost of a non-optimal path is greater than the exact cost of an optimal path).</p>
      </div>
      <div class="ln-box">
        <p>However, we can come up with an admissible heuristic:</p>
        <div class="ln-center">
          <p>`h_(DH)(n)=max_(L in text(landmarks)) abs(C^(**)(n,L)-C^(**)(L,text(goal)))`</p>
        </div>
        <p>This is a <b>differential heuristic</b> (differential because of the subtraction). To understand it intuitively, consider a landmark that is beyond the goal. If the goal happens to be on the optimal path to the landmark, then the path `n rarr text(goal) rarr L` minus the path `text(goal) rarr L` gives us the optimal path `n rarr text(goal)`.</p>
        <div class="ln-center">
          <img class="img-fluid ln-image-small" src="../pictures/ai/differential_heuristic.png">
        </div>
        <p>If the goal is not on the optimal path to the landmark, then we would be subtracting by a larger value, resulting in a cost `lt` the optimal cost, i.e., the cost is underestimated.</p>
        <p>Landmarks that are not beyond the goal will already underestimate the cost to the goal, but these landmarks are not likely to be considered since we're selecting the max.</p>
      </div>
      <p>A good way to decide landmark points are by looking at user requests for frequently requested landmarks.</p>
      <h3>Learning heuristics from experience</h3>
      <p>Sometimes, we can just develop an intuition for something if we do it many times. From the perspective of computers, they can pick up on patterns in past data and come up with a heuristic that seems to align with those patterns. A lot of this seems to depend on luck, so the heuristics are not expected to be admissible (much less consistent).</p>
      <div class="ln-box">
        <p>For example, the number of misplaced tiles might be helpful in predicting the actual cost. For further example, we might find that when there are `5` misplaced tiles, the average solution cost is around `14`.</p>
        <p>Another feature that might be useful is the number of pairs of adjacent tiles that are not adjacent in the goal state.</p>
        <p>If we have multiple features, we can combine them to generate a heuristic by using a linear combination of them:</p>
        <div class="ln-center">
          <p>`h(n)=c_1x_1(n)+c_2x_2(n)+...+c_kx_k(n)`</p>
          <p>where each `x_i` is a feature</p>
        </div>
        <p>Machine learning would be used to figure out the best values for the constants.</p>
      </div>
      <div class="ln-box">
        <h3>Summary of Chapter 3</h3>
        <ul>
          <li>A well-defined search problem consists of five parts: the <b>initial state</b>, a set of <b>actions</b>, a <b>transition model</b> that describes the results of those actions, a set of <b>goal states</b>, and an <b>action cost function</b>.</li>
          <li>The environment of the problem is represented by a <b>state space graph</b>. A <b>path</b> through the state space from the initial state to a goal state is a <b>solution</b>.</li>
          <li><b>Uninformed search</b> methods have access only to the problem definition. Algorithms build a search tree to find a solution but differ based on which node they expand first.
            <ul>
              <li><b>Best-first search</b> expands the node with the lowest value given by an <b>evaluation function</b>.</li>
              <li><b>Breadth-first search</b> expands the shallowest nodes first. It is complete, optimal for unit action costs, but has exponential space complexity.</li>
              <li><b>Uniform-cost search</b> expands the node with the lowest path cost, `g(n)`, and is optimal for general action costs.</li>
              <li><b>Depth-first search</b> expands the deepest unexpanded node first. It is neither complete nor optimal, but has linear space complexity. <b>Depth-limited search</b> adds a depth bound.</li>
              <li><b>Iterative deepening search</b> calls depth-first search with increasing depth limits until a goal is found. It is complete when full cycle checking is done, optimal for unit action costs, has exponential time complexity, and has linear space complexity.</li>
              <li><b>Bidirectional search</b> expands two frontiers, one around the initial state and one around the goal, stopping when the two frontiers meet.</li>
            </ul>
          </li>
          <li><b>Informed search</b> methods have access to a <b>heuristic</b> function `h(n)` that estimates the cost of a solution from `n`.
            <ul>
              <li><b>Greedy best-first search</b> expands the node with the lowest value of `h(n)`. It is not optimal but is often efficient.</li>
              <li><b>A* search</b> expands the node with the lowest value of `f(n)=g(n)+h(n)`. It is complete and optimal (if `h(n)` is admissible), but uses up a lot of space.</li>
              <li><b>IDA*</b> (iterative deepening A* search) is an iterative deepening version of A* that addresses the space complexity issue.</li>
              <li><b>RBFS</b> (recursive best-first search) is a robust, optimal search algorithm that uses limited amounts of memory.</li>
              <li><b>Beam search</b> puts a limit on the size of the frontier. This makes it incomplete and suboptimal, but it often finds reasonably good solutions and is generally faster than complete searches.</li>
              <li><b>Weighted A* search</b> expands fewer nodes but sacrifices optimality.</li>
            </ul>
          </li>
          <li>The performance of heuristic search algorithms depends on the quality of the heuristic function.</li>
          <li>Good heuristics can be obtained by relaxing the problem definition, by storing precomputed solution costs for subproblems in a pattern database, by defining landmarks, or by learning from experience.</li>
        </ul>
      </div>
      <hr>
      <p>Earlier, we assumed that the environments are episodic, single agent, fully observable, deterministic, static, discrete, and known. Now we'll start relaxing these constraints to get closer to the real world.</p>
      <hr>
      <h2 id="localsearch">Local Search and Optimization Problems</h2>
      <p>There are some search problems where we only care about what a goal state looks like (and we don't care what the path looks like). For example, the 8-queens problem is to find a valid placement of 8 queens such that no queens are attacking each other. Once we know what a solution looks like, it's trivial to reconstruct the steps to get there.</p>
      <p><b>Local search</b> algorithms work without keeping track of the paths or the states that have been reached. This means that they don't explore the whole search space systematically, i.e., they are not complete and may not find a solution. However, they use very little memory and can find reasonable solutions in problems with large or infinite state spaces.</p>
      <p>Local search also works for <b>optimization problems</b>, where the goal is to find the best state according to some <b>objective function</b>. The objective function represents what we're trying to maximize/minimize.</p>
      <h3>Hill-climbing search</h3>
      <p>Let's take a look at a sample <b>state-space landscape</b>.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/state_space_landscape.png">
      </div>
      <p>One goal might be to find the highest peak, which is the <b>global maximum</b>. This process is called <b>hill climbing</b>.</p>
      <div class="ln-box">
        <p>If the goal is to find the lowest valley (<b>global minimum</b>), then the process is called <b>gradient descent</b>.</p>
      </div>
      <p>The hill climbing search algorithm is pretty simple. It keeps moving forward until the next state it finds has a smaller value than the value of the current state.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function HILL-CLIMBING(problem)<br>&nbsp;&nbsp;current = problem.INITIAL<br>&nbsp;&nbsp;while true do<br>&nbsp;&nbsp;&nbsp;&nbsp;neighbor = a highest-valued successor state of current<br>&nbsp;&nbsp;&nbsp;&nbsp;if VALUE(neighbor) &leq; VALUE(current) then return current<br>&nbsp;&nbsp;&nbsp;&nbsp;current = neighbor</code></p>
        </div>
        <p>This particular version of hill climbing is <b>steepest-ascent hill climbing</b> because it heads in the direction of the steepest ascent.</p>
      </div>
      <div class="ln-box">
        <p>Hill climbing is sometimes also called <b>greedy local search</b> because it gets a good neighbor state without thinking ahead about where to go next.</p>
      </div>
      <p>There are some situations where the hill climbing algorithm can get stuck before finding a solution. One situation is when a <b>local maximum</b> is reached. In this situation, every possible move results in a worser state.</p>
      <p>Another situation is when a <b>plateau</b> is reached. A plateau can be a flat local maximum or a <b>shoulder</b>, where there is an uphill exit. In this situation, every possible move doesn't result in a better state (except if the plateau is a shoulder and we're right at the edge of the uphill exit).</p>
      <p>Let's look at the problem of positioning hospitals so that they are as close as possible to every house.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/plateau_example.png">
      </div>
      <p>The cost is the sum of the distances from each house to the nearest hospital.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/plateau_example_2.png">
      </div>
      <p>We can see that this is the best way to place the hospitals, but in order to achieve this position, the algorithm has to make the move below first.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/plateau_example.gif">
      </div>
      <p>Since this move doesn't result in a better state, the hill climbing algorithm won't consider that move. And as a result, the best configuration will never be reached. This is an example of a plateau.</p>
      <p>One way to get unstuck is to allow <b>sideways moves</b>, i.e., to allow the algorithm to keep moving as long as the next state isn't worse. The idea is to hope that the plateau is a shoulder. This won't work if the plateau is a local maximum though, in which case we really are stuck.</p>
      <p>Another way (kinda) is to just start over, but from a different starting point. This is <b>random-restart hill climbing</b>.</p>
      <div class="ln-box">
        <p>Interestingly, random-restart hill climbing is complete with probability `1` because it will eventually generate a goal state as the initial state.</p>
      </div>
      <div class="ln-box">
        <p><b>Stochastic hill climbing</b> chooses a random uphill move instead of the steepest uphill move.</p>
        <p><b>First-choice hill climbing</b> generates successors randomly until one of them is better than the current state.</p>
      </div>
      <h3>Simulated annealing</h3>
      <div class="ln-box">
        <p>Annealing is the process of hardening metals and glass by heating them to a high temperature and then gradually cooling them.</p>
      </div>
      <p>Sometimes, it's necessary to make worse moves that put us in a temporarily worse state so that we can get to a better position.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function SIMULATED-ANNEALING(problem, schedule)<br>&nbsp;&nbsp;current = problem.INITIAL<br>&nbsp;&nbsp;for t = 1 to infinity do<br>&nbsp;&nbsp;&nbsp;&nbsp;T = schedule(t)<br>&nbsp;&nbsp;&nbsp;&nbsp;if T = 0 then return current<br>&nbsp;&nbsp;&nbsp;&nbsp;next = a randomly selected successor of current<br>&nbsp;&nbsp;&nbsp;&nbsp;&Delta;E = VALUE(current) - VALUE(next)<br>&nbsp;&nbsp;&nbsp;&nbsp;if &Delta;E &gt; 0 then current = next<br>&nbsp;&nbsp;&nbsp;&nbsp;else current = next only with probability</code> `e^(-DeltaE//T)`</p>
        </div>
        <p><code>T</code> represents the "temperature", corresponding to the temperature described in annealing.</p>
      </div>
      <p>In the simulated-annealing algorithm, we pick moves randomly instead of picking the best move. If the move puts us in a better situation, then we'll accept it. But if the move puts us in a worse position, then we'll consider it, depending on how bad of a move it is (measured by `DeltaE`). And as time goes on, we'll be less accepting of bad moves.</p>
      <div class="ln-box">
        <p>It's called simulated annealing because the algorithm starts with a high "temperature", meaning that there's a high level of randomness in exploring the search space. As time goes on, the "temperature" gets lower, reducing the random moves.</p>
      </div>
      <h3>Local beam search</h3>
      <p><b>Local beam search</b> starts with `k` randomly generated states, generates all their successors (if one of them is a goal, then we're done), selects the best `k` successors, and repeats.</p>
      <div class="ln-box">
        <p>Local beam search kinda looks like running `k` random-restart searches in parallel. However, it actually isn't. All random-restart searches are independent of each other, while each local beam search thread passes information to each other.</p>
      </div>
      <h3>Evolutionary algorithms</h3>
      <p><b>Evolutionary algorithms</b> are based on the ideas of natural selection in biology. There is a population of individuals (states), and the fittest (highest valued) individuals produce offspring (successor states). And just like in real life, the successor states contain properties of the parent states that produced them.</p>
      <p>The process of combining the parent states to produce successor states is called <b>recombination</b>. One common way of implementing this is to split each parent at a <b>crossover point</b> and use the four resulting pieces to form two children.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/evolutionary_algorithm.png">
        <p>The numbers in the fitness function are sample fitness scores assigned by the fitness function to each state. The fitness scores are then converted to a probability.</p>
      </div>
      <p>Below is an example of how an evolutionary algorithm can be used for the 8-queens problem. We can encode each queen's positions as a number from `1-8`.</p>
      <div class="ln-center">
        <div class="row">
          <div class="col-sm">
            <img class="img-fluid" src="../pictures/ai/evolutionary_algorithm_chess_1.png">
            <p>`327|52411`</p>
          </div>
          <div class="col-sm">
            <img class="img-fluid" src="../pictures/ai/evolutionary_algorithm_chess_2.png">
            <p>`247|48552`</p>
          </div>
          <div class="col-sm">
            <img class="img-fluid" src="../pictures/ai/evolutionary_algorithm_chess_3.png">
            <p>`327|48552`</p>
          </div>
        </div>
        <p>If we use nonattacking queens as the fitness function, then the sample fitness scores are the number of nonattacking pairs of queens.</p>
      </div>
      <h2 id="localsearchincontinuousspaces">Local Search in Continuous Spaces</h2>
      <p>Most real-world problems take place in a continuous environment, not in a discrete one like the examples we've seen so far. A continuous environment can have a continuous action space (e.g., a robotic arm moving anywhere between `0` and `180` degrees), a continuous state space (e.g., the current position of a robot in a room), or both.</p>
      <div class="ln-box">
        <p>A continuous action space has an infinite branching factor, so most of the algorithms we've seen so far won't work (except for first-choice hill climbing and simulated annealing).</p>
      </div>
      <p>One way to deal with a continuous state space is to <b>discretize</b> it.</p>
      <p>For example, suppose we want to place three new airports in an area such that each city is as close as possible to its nearest airport. The state space is defined by the coordinates of the three airports: `(x_1, y_1),(x_2, y_2),(x_3, y_3)`. To discretize this, we could predefine fixed points on a grid and only allow the airports to be at those fixed points.</p>
      <p>One way to deal with a continuous action space is to choose successor states randomly by moving in a random direction by a small amount. As we move, we look at how the value of the objective function changes to see if we're going in the right direction. This is <b>empirical gradient</b> search.</p>
      <div class="ln-box">
        <p>Steepest-ascent hill climbing can be seen as a specific case of empirical gradient search when the state space is discretized.</p>
      </div>
      <div class="ln-box">
        <p>The objective function for any particular state is</p>
        <div class="ln-center">
          <p>`f(x_1, y_1, x_2, y_2, x_3, y_3) = sum_(i=1)^3sum_(c in C_i)(x_i-x_c)^2+(y_i-y_c)^2`</p>
        </div>
        <p>Note that this objective function is only correct locally, not globally. As the location of an airport moves, the closest cities to that airport change and have to be recomputed.</p>
      </div>
      <div class="ln-box">
        <p>As in the example above, the objective function can often be expressed in a math equation. This means that we can use calculus to find the steepest ascent rather than moving randomly to try and find it.</p>
        <p>For the above objective function, the gradient is calculated by</p>
        <div class="ln-center">
          <p>`grad f = ((del f)/(del x_1),(del f)/(del y_1),(del f)/(del x_2),(del f)/(del y_2),(del f)/(del x_3),(del f)/(del y_3))`</p>
        </div>
      </div>
      <h2 id="nondeterministicsearch">Search with Nondeterministic Actions</h2>
      <p>Now, we'll look at nondeterministic environments. When the environment is nondeterministic, the agent doesn't know for sure what state it will end up in after performing an action. So the agent will be thinking something like, "If I do action `a` I'll end up in state `s_2`, `s_4`, or `s_5`. The states that the agent believes are possible are called <b>belief states</b>.</p>
      <p>Instead of a sequence of states, the solution to a problem is a <b>conditional plan</b> that specifies what to do depending on the situation.</p>
      <h3>The erratic vacuum world</h3>
      <p>We're going back to our vacuum world! But this time, our vacuum is a little unpredictable. When it cleans a dirty square, it can sometimes clean an adjacent square as well. If it cleans a clean square, it can sometimes deposit dirt on it.</p>
      <h3>AND-OR search trees</h3>
      <div class="ln-center">
        <img class="img-fluid" src="../pictures/ai/and_or_search_tree.png">
        <p>Yes, the red oval is our vacuum cleaner. The states are labeled from `1-8`.</p>
      </div>
      <p>Notice that there's no single sequence of actions that can solve the problem. But this conditional plan does:</p>
      <div class="ln-center">
        <p>[Clean, if State == 5 then [Right, Clean] else []]</p>
      </div>
      <p>As the name suggests, an AND-OR search tree has OR nodes and AND nodes. <b>OR nodes</b> (represented as rectangles in the example above) are thought of as, "at this node, I can do this <em>or</em> that". <b>AND nodes</b> (represented as circles) are thought of as, "at this node, this <em>and</em> this can happen".</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function AND-OR-SEARCH(problem)<br>&nbsp;&nbsp;return OR-SEARCH(problem, problem.INITIAL, [])</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function OR-SEARCH(problem, state, path)<br>&nbsp;&nbsp;if problem.IS-GOAL(state) then return empty plan<br>&nbsp;&nbsp;if IS-CYCLE(path) return faliure<br>&nbsp;&nbsp;for each action in problem.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;plan = AND-SEARCH(problem, RESULTS(state, action), [state] + path)<br>&nbsp;&nbsp;&nbsp;&nbsp;if plan != failure then return [action] + plan<br>&nbsp;&nbsp;return failure</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function AND-SEARCH(problem, states, path)<br>&nbsp;&nbsp;for each s_i in states do<br>&nbsp;&nbsp;&nbsp;&nbsp;plan_i = OR-SEARCH(problem, s_i, path)<br>&nbsp;&nbsp;&nbsp;&nbsp;if plan_i == failure then return failure<br>&nbsp;&nbsp;return [if s_1 then plan_1 else if s_2 then plan_2 else ...if s_n-1 then plan_n-1 else plan_n]</code></p>
        </div>
        <p>This is a recursive, depth-first algorithm for AND-OR graph search.</p>
        <p>One thing to note is that we're returning failure when there's a cycle. This doesn't mean that a cycle implies no solution. It means that if there is a solution, it should exist on a path before the cycle, so there's no point in continuing to explore this path. This also ensures that the algorithm doesn't run infinitely.</p>
      </div>
      <h3>Try, try again</h3>
      <p>Forget our erratic vacuum for a moment. Let's go back to our normal vacuum, but this time, the floor is slippery. So sometimes, when the vacuum tries to move, it stays in place instead.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/cyclic_search_tree.png">
      </div>
      <p>Here, if we're in state `5` and the vacuum moves right, it can either move right or stay in place. If it stays in place, we still want it to move right. So the solution here is to keep moving right until it works.</p>
      <div class="ln-center">
        <p>[Clean, while State == 5 do Right, Clean]</p>
      </div>
      <p>This is a cyclic plan that happens to also be a solution, i.e., a <b>cyclic solution</b>.</p>
      <h2 id="partiallyobservablesearch">Search in Partially Observable Environments</h2>
      <p>Now, we'll look at what happens when the environment is partially observable. When the environment is partially observable, the agent doesn't know for sure what state it's in. So the agent will be thinking something like, "I'm either in state `s_1` or `s_3`".</p>
      <h3>Searching with no observation</h3>
      <p>If the agent has no sensors at all, then the problem that the agent is trying to solve is called a <b>sensorless problem</b>. It might seem impossible for an agent to be able to do anything without sensors, but, in fact, there are sensorless solutions that are common and useful because they don't rely on sensors working properly.</p>
      <p>So let's go back to when the vacuum was working normally. Except, the vacuum now has no sensors, so it doesn't know its own location or which squares have dirt (including the square it's currently on). But let's say the vacuum knows what all the possible states look like.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/vacuum_states.png">
      </div>
      <p>When it starts out, the vacuum's initial belief state is `{1, 2, 3, 4, 5, 6, 7, 8}`. No matter what state it's in, if the vacuum moves right, then it will be in one of the states `{2, 4, 6, 8}`. After cleaning, it will be in `{4,8}`. Then after moving left and cleaning, it will be in state `7`. So no matter what state the vacuum starts in, we can <b>coerce</b> the world into state `7` with [Right, Clean, Left, Clean].</p>
      <p>The solution to a sensorless problem is a sequence of actions, not a conditional plan. This is because the agent can't sense anything, and thus, can't tell when it needs to go with another plan.</p>
      <p>So the solutions can be found by using the informed and uninformed search algorithms from before. We just have to search in the space of belief states instead of physical states.</p>
      <div class="ln-box">
        <p>In belief-state space, the problem is fully observable because combining all the information about what states the agent could be in provides the agent with a complete picture of the environment.</p>
      </div>
      <p>If we transform a physical problem `P` (with `text(Actions)_P`, `text(Result)_P`) into a belief-state problem, we have:</p>
      <ul>
        <li>states: every possible subset of physical states
          <ul>
            <li>if `P` has `N` states, then the belief-state problem has `2^N` belief states
              <ul>
                <li>note that this is not saying that a belief state has `2^N` physical states</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>initial state: usually, the belief state consisting of all states in `P`</li>
        <li>goal test: all states in the current belief state must be a goal state</li>
        <li>action cost: to simplify things, we'll assume the cost of an action is the same in all states</li>
      </ul>
      <p>Defining the actions is a little more involved. A belief state can have multiple physical states, so there can be multiple actions available from each state. Inherently, there can be some actions that are possible in some of the states but not others. If illegal actions don't have an effect on the environment, then we can combine all the actions from each state and say that those are the available actions for the current belief state. Otherwise, we can find the actions that are common in all the states and say that those are the available actions.</p>
      <p>For example, in our vacuum world, let's say the belief state is `{s_1, s_2}`. All the possible actions from both states are to move left, move right, and clean. However, it's not really useful for a vacuum in `s_1` to move left. We've been assuming that if the vacuum can't move in a certain direction, it just stays still. So in this case, it's okay to combine all the actions and say that the available actions for `{s_1,s_2}` are to move left, move right, and clean.</p>
      <p>On the other hand, if the vacuum can fall off the edge, then we don't want it to move left if it's in `s_1`. Likewise, we don't want it to move right if it's in `s_2`. The only action in common from all states is to clean, so the available actions for the belief state in this case would be to just clean.</p>
      <div class="ln-box">
        <p>`text(Actions)(b)=uuu_(s in b) text(Actions)_P(s)` or `text(Actions)(b)=nnn_(s in b) text(Actions)_P(s)` depending on which makes sense.</p>
      </div>
      <p>Defining the transition model is also a little more involved depending on whether the actions are deterministic or not. For deterministic actions, there is one result state for each of the states in the belief state. For nondeterministic actions, it's the combination of all the possible result states for each of the states.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_deterministic.png">
        <p>The transition model for moving right in the normal vacuum world.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_nondeterministic.png">
        <p>The transition model for moving right in the slippery vacuum world.</p>
      </div>
      <div class="ln-box">
        <p>deterministic: `b' = text(Result)(b,a)={s' : s' = text(Result)_P(s,a) and s in b}`</p>
        <p>nondeterministic: `b' = text(Result)(b,a)={s' : s' in text(Results)_P(s,a) and s in b} = uuu_(s in b)text(Results)_P(s,a)`</p>
      </div>
      <p>By transforming a physical problem into a belief-state problem with these definitions, we can use the informed and uninformed search algorithms to find a solution. The operative word being "can". As mentioned earlier, there are a total of `2^N` possible belief states to search through. Furthermore, a belief state can have up to `N` states, so it can be hard to store multiple belief states in memory.</p>
      <p>An alternative is to use <b>incremental belief-state search</b> algorithms. For example, if the initial belief state is `{1,2,3,4,5,6,7,8}`, we can start by finding a solution for state `1` and see if it works for every other state. If it doesn't, then we go back to state `1`, find another solution, and repeat until the solution works for all `8` states.</p>
      <h3>Searching in partially observable environments</h3>
      <p>Now we'll upgrade our vacuum to have local sensors that can tell it which square it's on and whether it's clean or dirty. Suppose the vacuum gets the information [L, Dirty] from its sensors. From this, the vacuum can either be in state `1` or state `3`. So it's belief state will be `{1,3}`.</p>
      <p>The transition model between belief states can be seen as a three-stage process:</p>
      <ul>
        <li><b>prediction</b> stage: see what states are possible from performing on action on the current belief state</li>
        <li><b>possible percepts</b> stage: see what percepts are possible from each of the states</li>
        <li><b>update</b> stage: compute the belief states that are possible based on the possible percepts</li>
      </ul>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_deterministic_partially_observable.png">
        <p>The transition model for moving right in the normal (local sensing) vacuum world.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/transition_model_nondeterministic_partially_observable.png">
        <p>The transition model for moving right in the slippery (local sensing) vacuum world.</p>
      </div>
      <div class="ln-box">
        <p>prediction stage: `hat b = text(Result)(b,a)` (the `text(Result)` function can also be called `text(Predict)`)</p>
        <p>possible percepts stage: `text(Possible-Percepts)(hat b) = {o : o = text(Percept)(s) and s in hat b}`</p>
        <p>update stage: `text(Update)(hat b,o) = {s : o = text(Percept)(s) and s in hat b}`</p>
        <p>transition model: `text(Results)(b,a) = {b_o : b_o = text(Update)(text(Predict)(b,a), o) and o in text(Possible-Percepts)(text(Predict)(b,a))}`</p>
      </div>
      <h3>Solving partially observable problems</h3>
      <p>Because the current belief state can yield multiple belief states as the next possible states, we can treat the multiple belief states as if they were the result of a nondeterministic action and, therefore, use the AND-OR search algorithm. As a result, the solution is a conditional plan. For example, [Clean, Right, if state == 6 then Clean else []] is a solution to the search tree below.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/and_or_partially_observable.png">
        <p>Remember that the transition model returns one belief state per possible percept.</p>
      </div>
      <h2 id="onlinesearch">Online Search Agents and Unknown Environments</h2>
      <p>In this context, online isn't referring to the Internet. An <b>online search</b> agent makes an action, observes the environment, and computes the next action. As opposed to an <b>offline search</b> agent that computes a complete solution first before taking any action. Mostly everything we've seen so far has been offline search algorithms.</p>
      <p>Online search is good for (semi-)dynamic environments where the agent can't sit around doing nothing for too long. It's also good for nondeterministic environments because it allows the agent to focus on the percepts that it actually receives rather than the percepts that it can receive.</p>
      <p>On the flip side, more planning reduces the chances of things going wrong.</p>
      <hr>
        <p>We'll briefly return to deterministic and fully observable environments here.</p>
      <hr>
      <h3>Online search problems</h3>
      <p>The agent only knows the actions that are available in the current state, the cost of going to the next state (once the next available states have been computed), and whether or not the current state is a goal state. The agent cannot know the result of going to another state until it's in that state. The agent might have access to a heuristic function.</p>
      <p>An online algorithm can only generate successors for the state that it's currently in. On the other hand, offline algorithms, such as A* search, can expand a node in one part of the space and then immediately expand a node in a distant part of the space.</p>
      <p>Online explorers are vulnerable to <b>dead ends</b>, which are states from which no goal state is reachable. Dead ends are a result of <b>irreversible</b> actions, i.e., actions that prevent returning to the previous state.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/dead_end.png">
      </div>
      <p>An environment with no dead ends is <b>safely explorable</b>, which means that a goal state is reachable from every reachable state. Any environment with only reversible actions are safely explorable.</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function ONLINE-DFS-AGENT(problem, s')<br>&nbsp;&nbsp;if problem.IS-GOAL(s') then return stop<br>&nbsp;&nbsp;if s' is a new state (not in untried) then untried[s'] = problem.ACTIONS(s')<br>&nbsp;&nbsp;if s is not null then <br>&nbsp;&nbsp;&nbsp;&nbsp;result[s,a] = s'<br>&nbsp;&nbsp;&nbsp;&nbsp;add s to the front of unbacktracked[s']<br>&nbsp;&nbsp;if untried[s'] is empty then<br>&nbsp;&nbsp;&nbsp;&nbsp;if unbacktracked[s'] is empty then return stop<br>&nbsp;&nbsp;&nbsp;&nbsp;else a = an action b such that result[s',b] = POP(unbacktracked[s'])<br>&nbsp;&nbsp;else a = POP(untried[s'])<br>&nbsp;&nbsp;s = s'<br>&nbsp;&nbsp;return a</code></p>
        </div>
        <p><code>result</code>: a table mapping `(s,a)` to `s'`</p>
        <p><code>untried</code>: a table mapping `s` to a list of untried actions</p>
        <p><code>unbacktracked</code>: a table mapping `s` to a list of states never backtracked to</p>
      </div>
      <h3>Online local search</h3>
      <p>Since hill-climbing search just keeps the current state in memory, it's actually already an online search algorithm. It's not a very good one though because it can result in agents getting stuck at a local maximum. And random-restart hill climbing is not an option to get unstuck because the agent can't teleport to a new start state.</p>
      <p>A <b>random walk</b> can be used to explore an environment. The agent selects an action at random and performs it.</p>
      <div class="ln-box">
        <p>Random walks are complete when the space is finite and safely explorable.</p>
      </div>
      <p>Instead of telling the agent to do random things, we can have the agent learn about its environment as it explores it. One way to do this is to store a "current best estimate" of the cost to reach the goal from each state that has been visited.</p>
      <div class="ln-box">
        <p>The current best estimate, denoted as `H(s)`, starts out being just the heuristic `h(s)`, but it gets updated as the agent learns more.</p>
      </div>
      <p>Consider this sample state space. `(a),(b),(c),(d),(e)` are five iterations of an algorithm called learning real-time A* (<b>LRTA*</b>) search that implements this current best estimate idea. If we look at `(a)`, we see that the agent is currently stuck in a local minimum.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/lrta_star.png">
        <p>Each state is labeled the current estimated cost to reach a goal (`H(s)`). Every action costs `1`.</p>
        <p>The orange state is the agent's current location. The double-circled states are states that have had their cost estimates updated.</p>
      </div>
      <p>At `(a)`, there are two actions: move left or move right. The estimated cost to reach a goal if we move left is `1+9=10` and the estimated cost to reach a goal if we move right is `1+2=3`. So we move right. Importantly, we also update the previous state's estimated cost with the lowest one (`3` vs `10`).</p>
      <p>At `(b)`, the estimated cost to the goal if we move left is `1+3=4` and the estimated cost to the goal if we move right is `1+4=5`. We move left and update the previous state's estimated cost (`4` vs `5`).</p>
      <p>We can eventually see that we get unstuck (and "flatten out" the local minimum in the process).</p>
      <div class="ln-box">
        <p>LRTA* is complete when the environment is finite and safely explorable. However, unlike A*, it is not complete if the state space is infinite.</p>
      </div>
      <div class="ln-box">
        <h3>Summary of Chapter 4</h3>
        <ul>
          <li>Local search methods such as <b>hill climbing</b> keep only a small number of states in memory. They have been applied to optimization problems, where the idea is to find a high-scoring state without worrying about the path to that state.</li>
          <li><b>Simulated annealing</b> is a stochastic local search algorithm that returns optimal solutions when given an appropriate cooling schedule.</li>
          <li>Local search methods also apply to problems in continuous spaces. For some problems that can be represented mathematically, we can use calculus to find where the gradient is zero (to find the maximum/minimum). For other problems, we can use the empirical gradient instead, which measures the difference in fitness between two nearby points.</li>
          <li>An <b>evolutionary algorithm</b> is a stochastic hill-climbing search in which a population of states generates new states through <b>mutation</b> and <b>crossover</b>, which combines pairs of states.</li>
          <li>In <b>nondeterministic</b> environments, agents can apply AND-OR search to generate <b>contingent</b> plans that reach the goal regardless of what happens during execution.</li>
          <li>When the environment is partially observable, the <b>belief state</b> represents the set of possible states that the agent might be in.</li>
          <li>Standard search algorithms can be used to solve <b>sensorless problems</b>.</li>
          <li>Belief AND-OR search can solve partially observable problems.</li>
          <li><b>Exploration problems</b> arise when the agent has no idea about the states and actions of its environment. For safely explorable environments, <b>online search</b> agents can build a map and find a goal if one exists.</li>
          <li>Updating heuristic estimates from experience is an effective way to escape from local minima.</li>
        </ul>
      </div>
      <h2 id="constraintsatisfaction">Constraint Satisfaction Problems</h2>
      <p>Oftentimes, many problems have restrictions/conditions that need to be satisfied. For example, the rules of sudoku require that all numbers in any row, column, and square have to be different. Another general example might be that a solution needs to run in under 1 minute.</p>
      <p>More formally, a problem has a set of <b>variables</b> that represent different parts of a problem. A <b>value</b> is <b>assigned</b> to a variable. If all the variables are assigned a value, then that is a <b>complete assignment</b>. A <b>constraint satisfaction problem (CSP)</b> is solved when each variable has a value that satisifies all the constraints on the variable, i.e., the <b>solution</b> to a CSP is a <b>consistent</b> and complete assignment.</p>
      <p>If some of the variables are unassigned, then that is a <b>partial assignment</b>. A <b>partial solution</b> is a partial assignment that satisifies all the constraints.</p>
      <div class="ln-box">
        <p>`chi` is a set of variables `{X_1,...,X_n}`.</p>
        <p>`bbbD` is a set of domains `{D_1,...,D_n}`, one for each variable. These are the possible values that a variable can have. Each domain is a set of values.</p>
        <p>`bbbC` is a set of constraints that specify allowable combinations of values.</p>
        <p>For example, if `X_1` and `X_2` have the domain `{1,2,3}` and `X_1` has to be greater than `X_2`, then this is written as `(:(X_1,X_2),{(3,1),(3,2),(2,1)}:)` or as `(:(X_1,X_2),X_1 gt X_2:)`.</p>
      </div>
      <h3>Example problem: Map coloring</h3>
      <p>As trivial as this sounds, suppose we had a map of a region and we wanted to color each region red, green, or blue. But we have a constraint that no two neighboring regions can have the same color, e.g., a red region can't be next to another red region.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/map_coloring_1.png">
        <p>To spare myself the trouble of actually drawing a map, I'll represent a region as a node instead. `P` is a little island all on its own.</p>
        <p>This is actually called a <b>constraint graph</b>, where the edges connect variables that are constrained.</p>
      </div>
      <p>We formally define our CSP as:</p>
      <div class="ln-center">
        <p>`chi={H,J,K,L,M,N,P}`</p>
        <p>`D_i={text(red), text(green), text(blue)}`</p>
        <p>`bbbC={K ne H, K ne J, K ne L, K ne M, K ne N, H ne J, J ne L, L ne M, M ne N}`</p>
      </div>
      <div class="ln-box">
        <p>Because we're lazy, `K ne H` is a shortcut for `(:(K,H), K ne H:)=(:(K,H),{(text(red),text(green)),(text(red),text(blue)),(text(green),text(red)),(text(green),text(blue)),(text(blue),text(red)),(text(blue),text(green))}:)`.</p>
      </div>
      <p>One solution is `{H=text(red),J=text(green),K=text(blue),L=text(red),M=text(green),N=text(red),P=text(blue)}`.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/map_coloring_2.png">
      </div>
      <p>While it may seem like adding constraints to a problem might make it harder to solve, it actually makes things a lot easier for our algorithms. By adding constraints, the number of possible states are reduced. For example, once we've decided that we want `K` to be blue, that eliminates all the states where `H`, `J`, `L`, `M`, and `N` are blue.</p>
      <div class="ln-box">
        <p>Without the constraint, there are `3^5=243` possible states to consider (`3` colors for each of the `5` regions).</p>
        <p>With the constraint, there are only `2^5=32` possible states to consider (`2` colors for each of the `5` regions).</p>
      </div>
      <h2 id="constraintpropagation">Constraint Propagation: Inference in CSPs</h2>
      <p>This idea of reducing the number of possible states is <b>constraint propagation</b>, which means using the constraints to reduce the number of valid values for a variable, which in turn can reduce the number of valid values for other variables.</p>
      <p>Before we can begin searching for a solution, we need the graph to be consistent with the constraints. This is referred to as <b>local consistency</b>.</p>
      <h3>Node consistency</h3>
      <p>A <b>unary constraint</b> is a constraint that only affects one variable at a time. For example, `P ne text(green)`.</p>
      <p>A variable is <b>node-consistent</b> if all the values in the variable's domain satisfy the variable's unary constraints. A graph is node-consistent if every variable is node-consistent.</p>
      <h3>Arc consistency</h3>
      <p>A <b>binary constraint</b> is a constraint that affects two variables at a time. `K ne H` is a binary constraint.</p>
      <p>A variable is <b>arc-consistent</b> if all the values in the variable's domain satisfy the variable's binary constraints.</p>
      <div class="ln-box">
        <p>More specifically, a variable `X_i` is arc-consistent with `X_j` if for every value in the domain `D_i` there is some value in the domain `D_j` that satisfies the binary constraint on `X_i` and `X_j`.</p>
      </div>
      <div class="ln-box">
        <p>The most popular algorithm for creating an arc-consistent graph is called AC-3. It essentially goes through each arc (edge) and tries to reduce the size of each variable's (node's) domain.</p>
        <p>If all variables have a domain of size `1`, then the problem is solved. If there is any variable with a domain of size `0`, then there is no solution.</p>
      </div>
      <h3>Path consistency</h3>
      <p><b>Path consistency</b> makes sure three variables are consistent with each other. The name comes from the idea that the path between two variables is consistent with a third variable in the middle of the path.</p>
      <div class="ln-box">
        <p>A two-variable set `{X_i,X_j}` is path-consistent with respect to `X_m` if, for every assignment `{X_i=a,X_j=b}` (consistent with the constraints on `X_i` and `X_j`), there is an assignment to `X_m` that satisifes the constraints on `{X_i,X_m}` and `{X_m,X_j}`.</p>
      </div>
      <div class="ln-box">
        <h3>`K`-consistency</h3>
        <p>A CSP is <b>`k`-consistent</b> if, for any set of `k-1` consistent variables, a consistent value can be assigned to any `k^(th)` variable.</p>
        <p>Node consistency is `1`-consistency. Arc consistency is `2`-consistency. Path consistency is `3`-consistency.</p>
        <p>A CSP is <b>strongly `k`-consistent</b> if it is `k`-consistent, `(k-1)`-consistent, ..., `1`-consistent.</p>
      </div>
      <h3>Global constraints</h3>
      <p>A <b>global constraint</b> is a constraint that affects any number of variables at a time. It doesn't need to, but a global constraint can affect all of the variables at a time. In fact, one of the most common constraints is that each variable must have a different value (this constraint is referred to as an "Alldiff" constraint).</p>
      <p>A <b>resource constraint</b> is a constraint resulting from the limited number of resources available (this is sometimes referred to as an "Atmost" constraint). For example, suppose there are `4` tasks and only `10` people available to do them. If each task needs at least `3` people, then there is no solution.</p>
      <p>For problems that deal with larger numbers of resources, limiting the range of values a variable can have is more appropriate. This is <b>bounds propagation</b>.</p>
      <p>For example, suppose there are two airplanes where one can carry `165` people and the other can carry `385`. So the initial domains are</p>
      <div class="ln-center">
        <p>`D_1=[0,165]`</p>
        <p>`D_2=[0,385]`</p>
      </div>
      <p>Let's say our constraint is that the two flights together must carry `420` people. Using bounds propagation, we now have the domains</p>
      <div class="ln-center">
        <p>`D_1=[35,165]`</p>
        <p>`D_2=[255,385]`</p>
      </div>
      <div class="ln-box">
        <p>A CSP is <b>bounds-consistent</b> if for every variable `X` (for both the lower-bound and upper-bound values of `X`), there exists some value for `Y` that satisifies the constraint between `X` and `Y` for every variable `Y`.</p>
      </div>
      <h2 id="backtrackingsearch">Backtracking Search for CSPs</h2>
      <p>After applying the constraints to a CSP, we then have to <b>search</b> for a solution (if applying the constraints didn't already give us a solution for free).</p>
      <p>The basic idea behind backtracking search is that it repeatedly chooses an unassigned variable and tries different values in the domain of that variable. If a solution isn't found, the algorithm backtracks and tries again with different values. It's kinda like "Let's try this. Then let's try this. Then let's try this. Oops, that didn't work! Let's try this instead."</p>
      <div class="ln-box">
        <div class="ln-flex-center">
          <p><code>function BACKTRACKING-SEARCH(csp)<br>&nbsp;&nbsp;return BACKTRACK(csp, {})</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function BACKTRACK(csp, assignment)<br>&nbsp;&nbsp;if assignment is complete then return assignment<br>&nbsp;&nbsp;var = SELECT-UNASSIGNED-VARIABLE(csp, assignment)<br>&nbsp;&nbsp;for each value in ORDER-DOMAIN-VALUES(csp, var, assignment) do<br>&nbsp;&nbsp;&nbsp;&nbsp;if value is consistent with assignment then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add {var = value} to assignment<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inferences = INFERENCE(csp, var, assignment)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if inferences != failure then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add inferences to csp<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result = BACKTRACK(csp, assignment)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if result != failure then return result<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;remove inferences from csp<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;remove {var = value} from assignment<br>&nbsp;&nbsp;return failure</code></p>
        </div>
        <p><code>INFERENCE()</code> is a function that checks for consistency. <code>ORDER-DOMAIN-VALUES()</code> is a function that sorts the values in a way that we think is optimal.</p>
      </div>
      <div class="ln-box">
        <p>For a CSP with `n` variables, each with a domain size of `d`, the search tree would have a depth of `n`.</p>
        <p>At the top level, there are `n` nodes and each of them will have `d` children, so the branching factor is `nd` for the first level. At the second level, there are `n-1` nodes and each of them will have `d` children, so the branching factor is `(n-1)d` for the second level. This continues until the bottom where there is `1` node and `d` children.</p>
        <p>So the tree has `n!d^n` leaves.</p>
        <p>However, CSPs are <b>commutative</b>, meaning that the order that we assign the variables doesn't matter. For example</p>
        <div class="ln-center">
          <p>`{H=text(red),J=text(green),K=text(blue),L=text(red),M=text(green),N=text(red),P=text(blue)}`</p>
        </div>
        <p>is the same as</p>
        <div class="ln-center">
          <p>`{L=text(red),M=text(green),P=text(blue),H=text(red),J=text(green),N=text(red),K=text(blue)}`.</p>
        </div>
        <p>Since the order of the `n` variables doesn't matter for distinguishing solutions, we can divide by `n!` to remove repeated paths, resulting in the tree actually being `O(d^n)`.</p>
      </div>
      <h3>Variable and value ordering</h3>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/map_coloring_2.png">
      </div>
      <p>Going back to our map, let's say we arbitrarily start at `H` and arbitrarily choose red for it. Then we arbitrarily choose green for `J`. From these two decisions, we can see that `K` is now forced to be blue. This in turn, forces `L`, `M`, and `N` to be red, green, and red respectively.</p>
      <p>If we had instead decided to pick a color for `L` before looking at `K`, we would not have had it this easy. In fact, if we had decided `L` to be blue, there would have been no valid value for `K` and we would have had to start over.</p>
      <p>This highlights that when deciding which unassigned variable to look at next, we should look at the variable with the fewest valid values. This strategy is the <b>minimum-remaining-values</b> (MRV) heuristic. It is also called the "fail-first" heuristic because it picks a variable that is most likely to cause a failure, which allows the tree to be pruned. In our example, picking blue for `L` caused a failure, so all the possible map configurations where `L` is blue (and `H` is red and `J` is green) can be pruned away.</p>
      <p>But notice how we were still making arbitrary decisions for the first two moves. The MRV heuristic isn't really helpful in the beginning because all the variables have the same number of valid values. So another heuristic that helps us pick a variable at the start is the <b>degree heuristic</b>. It selects the variable that is involved in the most constraints with other unassigned variables. In our map, `K` has a constraint with `5` other variables (i.e. it has degree `5`), so it should be picked first. It just so happens that starting with `K` guarantees that a solution will be found right away.</p>
      <p>So we have heuristics for deciding which variable to look at first, but there is one more choice to think about and that is which value to assign first. Let's say we picked `H` to be red and `J` to be green, and we decided to look at `L` (despite knowing that looking at `K` first was better). We know that picking blue for `L` is not a good idea since it leaves no choices available for `K`, so we choose red instead.</p>
      <p>Instead of picking a value that will cause a failure, we want to pick a value that will work. More specifically, we want to pick a value that will rule out the fewest choices for the neighboring variables, giving them as much flexibility as possible. This is the <b>least-constraining-value</b> heuristic.</p>
      <div class="ln-box">
        <p>Variable selection should be fail-first and value selection should be fail-last. This is because starting at the most restrictive variable reduces the number of times we have to backtrack. If we know a variable is going to cause failures, it doesn't make sense to start somewhere else and keep hitting that failure. It's better to just prevent that failure in the first place.</p>
        <p>Value selection should be fail-last because we want to find a solution as soon as possible. We don't need to find all possible combinations of values that work; we just need to find one set of values that work.</p>
      </div>
      <h3>Interleaving search and inference</h3>
      <p>As we saw before, inference (using AC-3) can reduce the size of a variable's domain before the search starts. But we can do this during the search as well by doing <b>forward checking</b>. Whenever a value is assigned to a variable, we look at the other variables involved in a constraint with this variable and remove inconsistent values from their domains.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/map_coloring_1.png">
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/forward_checking_1.png">
      </div>
      <p>Let's say we start with `H` red. Once we have this, we know that `K` and `J` can't be red, so we remove red from their domains.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/forward_checking_2.png">
      </div>
      <p>Then we choose `L` green. Once we have this, we know that `J`, `K`, and `M` can't be green, so we remove green from their domains.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/forward_checking_3.png">
      </div>
      <p>Then we choose `N` blue. Once we have this, we know that `K` and `M` can't be blue, so we remove blue from their domains.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/forward_checking_4.png">
      </div>
      <p>At this point, we can see that there are no valid values left for `K`. So we have to backtrack and try a different value for `N`.</p>
      <h3>Intelligent backtracking: Looking backward</h3>
      <p>Whenever an inconsistency is found, the backtracking algorithm goes up one level to the previous variable and tries a different value. This is called <b>chronological backtracking</b> because the most recent decision point is revisited.</p>
      <p>This is simple, but it can be a little inefficient.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/map_coloring_3.png">
      </div>
      <p>Let's say we started with `L` red, `M` green, `N` blue, and `P` red in that order. Now we see that there is no valid color for `K`, so we backtrack and consider a different value for `P`. Unfortunately, we can see that choosing a different value for `P` won't change anything since `P` wasn't causing the problem; we would have to backtrack one more level to `N`. But the algorithm will be inefficient and try different values of `P` until none of them work and <em>then</em> backtrack to `N`.</p>
      <p>Since the set `{L=text(red),M=text(green),N=text(blue)}` was responsible for `K` not having any valid values, we call this set the <b>conflict set</b> for `K`. If we keep track of this set, we can <b>backjump</b> to the most recent assignment in the conflict set, which would be `N` in this case.</p>
      <div class="ln-box">
        <p>Forward checking prevents inconsistent situations from being reached in the first place. Doing both forward checking (or any consistency checking in general) and backjumping is redundant.</p>
      </div>
      <h3>Constraint learning</h3>
      <p>If we reach an inconsistency, we can remember what the conflict set was that led to the inconsistency so we don't run into it again. This set is aptly called a <b>no-good</b>.</p>
      <h2 id="localsearchcsp">Local Search for CSPs</h2>
      <p>The local search algorithms we saw before can be used to solve CSPS. The idea is to assign a value to every variable at the beginning, even if they're inconsistent. Then we randomly choose a variable that is violating one or more constraints and change its value to something that violates fewer constraints. And we keep doing this until we find a solution. (Or keep going forever if there is no solution. )</p>
      <p>The <b>min-conflicts</b> heuristic chooses the value that results in the minimum number of violated constraints with other variables.</p>
      <div class="ln-box">
        <h3>Summary of Chapter 6</h3>
        <ul>
          <li><b>Constraint satisfaction problems</b> (CSPs) represent a state with a set of variable/value pairs and represent the conditions for a solution by a set of constraints on the variables.</li>
          <li><b>Inference</b> techniques use the constraints to rule out certain variable assignments. Some of the inference techniques are node, arc, path, and `k`-consistency.</li>
          <li><b>Backtracking search</b> is a form of depth-first search that is commonly used to solve CSPs.</li>
          <li>The <b>minimum-remaining-values</b> and <b>degree</b> heuristics are domain-independent methods for deciding which variable to choose next in a backtracking search.</li>
          <li>The <b>least-constraining-value</b> heuristic helps in deciding which value to try first for a variable.</li>
          <li><b>Conflict-directed backjumping</b> backtracks directly to the source of the problem.</li>
          <li><b>Constraint learning</b> records the conflicts as they are encountered during search in order to avoid the same conflict later on.</li>
          <li>Local search using the <b>min-conflicts</b> heuristic is good.</li>
        </ul>
      </div>
      <hr>
      <p>Now, we'll look at multi-agent environments. In particular, <b>competitive environments</b> where two or more agents have conflicting goals. Problems in these environments are called <b>adversarial search</b> problems.</p>
      <hr>
      <h2 id="gametheory">Game Theory</h2>
      <p>Apparently, chess, Go, and poker are popular games to study in the AI community.</p>
      <h3>Two-player zero-sum games</h3>
      <p>Game theorists call these types of games deterministic, two-player, turn-taking, <b>perfect information</b>, <b>zero-sum games</b>. "Perfect information" means "fully observable" and "zero-sum" means that what is good for one player is just as bad for the other. A <b>move</b> is an "action" and a <b>position</b> is a "state".</p>
      <p>A game can be formally defined by specifying:</p>
      <ul>
        <li>the <b>initial state</b>, which describes how the game is set up at the start
          <ul>
            <li>`S_0`</li>
          </ul>
        </li>
        <li>the player whose turn it is to move in state `s`
          <ul>
            <li>`text(TO-MOVE)(s)`</li>
          </ul>
        </li>
        <li>the set of legal moves in state `s`
          <ul>
            <li>`text(ACTIONS)(s)`</li>
          </ul>
        </li>
        <li>the <b>transition model</b>, which defines the state resulting from taking action `a` in state `s`
          <ul>
            <li>`text(RESULT)(s,a)`</li>
          </ul>
        </li>
        <li>a <b>terminal test</b>, which is true when the game is over
          <ul>
            <li>states where the game has ended are called <b>terminal states</b></li>
            <li>`text(IS-TERMINAL)(s)`</li>
          </ul>
        </li>
        <li>a <b>utility function</b>, which assigns numerical values to the outcome of the game
          <ul>
            <li>in chess, a win is `1`, a loss is `0`, and a draw is `1/2`</li>
            <li>`text(UTILITY)(s,p)`</li>
          </ul>
        </li>
      </ul>
      <div class="ln-box">
        <p>Chess is considered a "zero-sum" game even though the sum of the outcomes is `1`. However, if we imagine that each player "paid `1/2`" to play, then the winner wins `1/2` and the loser loses `1/2`.</p>
      </div>
      <p>Instead of using a <b>search tree</b> like the one we saw earlier, we'll be using a <b>game tree</b>, which takes into account that there are multiple players. And speaking of players, we'll be calling our two players MAX and MIN.</p>
      <p>Here's an example of a (partial) game tree for tic-tac-toe:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/game_tree.png">
      </div>
      <h2 id="optimaldecisionsingames">Optimal Decisions in Games</h2>
      <p>Since each player is responding to each other's moves, the strategy for both players is a conditional plan (if that player does this, I'll do this).</p>
      <div class="ln-box">
        <p>In games that have a binary outcome (e.g., win or lose), we could use AND-OR search to generate the conditional plan. For games with multiple outcomes, we need something a little more general.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/two_ply_game_tree_1.png">
      </div>
      <p>Consider the simple game above where MAX is trying to reach one of the bottom-row nodes with the highest value and MIN is trying to have MAX end up at the node with the lowest value (depending on MAX's first move). MAX moves first and gets to pick a path `a_1`, `a_2`, or `a_3`. Then it is MIN's turn to decide which bottom-row node MAX ends up at.</p>
      <p>For example, MAX might want to reach the node with `14`, so MAX will choose `a_3`. At this point, MIN has three options `d_1`, `d_2`, `d_3`. Since MIN wants MAX to have the lowest score possible, MIN will choose `d_3`, so MAX will end up with a score of `2`.</p>
      <p>That strategy didn't really go so well for MAX, so let's look at another one. This time, MAX wants `12`, so MAX will choose `a_1`. MIN will then choose `b_1` so that MAX ends up with `3`, the lowest out of the three choices.</p>
      <p>We can see that `3` is the highest score MAX can hope to get if MIN plays optimally. And if you took the time to verify that, then you'll notice that you played the game backwards (starting from the bottom and moving up) to check.</p>
      <p>This is essentially the idea behind the <b>minimax search</b> algorithm.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/two_ply_game_tree_2.png">
        <p>`B` has a minimax value of `3` because if MAX chooses `a_1`, the highest score MAX can get is `3` if MIN plays optimally.</p>
      </div>
      <p>Each node is assigned a <b>minimax value</b> that represents how "good" that position/move is for a player, assuming the other player plays optimally. In this game, MAX wants to choose the node with the maximum minimax value and MIN wants to choose the node with the minimum minimax value.</p>
      <p>The minimax value of the terminal nodes are just the values defined by the game's `text(UTILITY)` function (for terminal nodes, their utility is the score we get when the game ends). Then the minimax values are propagated up the game tree using the same reasoning we used to analyze the game above.</p>
      <div class="ln-box">
        <h3>The minimax search algorithm</h3>
        <p>This is a recursive search algorithm that goes all the way down to the leaves and then <b>backs up</b> the minimax values up the tree as the recursion unwinds.</p>
        <div class="ln-flex-center">
          <p><code>function MINIMAX-SEARCH(game, state)<br>&nbsp;&nbsp;player = game.TO-MOVE(state)<br>&nbsp;&nbsp;value, move = MAX-VALUE(game, state)<br>&nbsp;&nbsp;return move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MAX-VALUE(game, state)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = -infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MIN-VALUE(game, game.RESULT(state, a))<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &gt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MIN-VALUE(game, state)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MAX-VALUE(game, game.RESULT(state, a))<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &lt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <p>Let `m` be the maximum depth of the tree and assume there are `b` legal moves at each point.</p>
        <p>The time complexity of minimax search is `O(b^m)` and the space complexity is `O(bm)` (the reasoning for the space complexity is the same as the reasoning for the space complexity of depth-first search; this is a depth-first algorithm after all).</p>
      </div>
      <h3>Optimal decisions in multiplayer games</h3>
      <p>To create game trees for games with more than two players, we assign a group of values (one value for each player) to each node. Consider a game (similar to the one before) where all players are trying to get to the node that will give them (individually) the highest score possible.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/three_ply_game_tree_1.png">
      </div>
      <p>Let's say we have three players `A`, `B`, and `C`. So each node will have a group of minimax values `(a,b,c)` where `a` is the minimax value for `A`, `b` is the minimax value for `B`, and `c` is the minimax value for `C`. `A` is trying to get to the node with the highest `a` value, `B` is trying to get to the node with the highest `b` value, and `C` is trying to get to the node with the highest `c` value.</p>
      <p>Suppose `A` wants to get to the node `(7,4,1)`, so `A` moves left. At this point, `B` would also want to get to the node `(7,4,1)`, so `B` moves right. From the remaining options, `C` wants to get to the node `(6,1,2)`, so `C` moves left. So the final score for all three players is `A:6`, `B:1`, `C:2`.</p>
      <p>This is the game tree assuming everyone plays optimally:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/three_ply_game_tree_2.png">
        <p>`A`'s best move is left since the minimax value of the left node `(1)` is greater than the minimax value of the right node `(0)`.</p>
      </div>
      <p>Again, the terminal nodes have their minimax values defined by the game's `text(UTILITY)` function, and the values are passed up the tree depending on which player's turn it is.</p>
      <h3>Alpha-Beta Pruning</h3>
      <p>We can actually sometimes deduce the minimax value of a node without having to fully expand it. This means that we can optimize the algorithm by not examining all the nodes. If we can deduce that it is not optimal for a player to go down a certain path, we can skip it, which resultingly <b>prunes</b> the tree.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/alpha_beta_pruning_1.png">
      </div>
      <p>Suppose we're running the algorithm and we've already discovered the minimax value of `B`. And now when we're calculating the minimax value of `C`, we find that its first child has a value of `2`. We don't need to look at the minimax values of the other two children because the minimax value of `C` will be at most `2`. If the other two children have minimax values higher than `2`, then MIN will move to the node with minimax value `2`. If the other two children have minimax values lower than `2`, then MIN will move to those nodes, making MAX's score even lower. No matter what those two children of `C` are, it's not in MAX's best interest to move to `C` because moving to `B` is a better option.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/alpha_beta_pruning_2.png">
      </div>
      <p>We do however, need to explore all of `D`'s children though. The minimax values of the first two children are `14` and `5`, both of which are higher than `3`, so MAX would still want to go down this path. It's not until we look at the last child that we can determine that this path is not optimal for MAX.</p>
      <div class="ln-box ln-center">
        <p>`text(MINIMAX)(text(root)) = max(min(3,12,8),min(2,x,y),min(14,5,2))`</p>
        <p>`= max(3,min(2,x,y),2)`</p>
        <p>`= max(3,z,2)` where `z = min(2,x,y) le 2`</p>
        <p>`=3`</p>
      </div>
      <div class="ln-box">
        <p>It is called <b>alpha-beta pruning</b> because `alpha` and `beta` are used as bounds to determine whether a part of the tree can be pruned. `alpha` is the highest minimax value found so far and `beta` is the lowest minimax value found so far.</p>
        <div class="ln-flex-center">
          <p><code>function ALPHA-BETA-SEARCH(game, state)<br>&nbsp;&nbsp;player = game.TO-MOVE(state)<br>&nbsp;&nbsp;value, move = MAX-VALUE(game, state, -infinity, infinity)<br>&nbsp;&nbsp;return move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MAX-VALUE(game, state, &alpha;, &beta;)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = -infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MIN-VALUE(game, game.RESULT(state, a), &alpha;, &beta;)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &gt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&alpha; = MAX(&alpha;, v)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v &ge; &beta; then return v, move<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <div class="ln-flex-center">
          <p><code>function MIN-VALUE(game, state, &alpha;, &beta;)<br>&nbsp;&nbsp;if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null<br>&nbsp;&nbsp;v = infinity<br>&nbsp;&nbsp;for each a in game.ACTIONS(state) do<br>&nbsp;&nbsp;&nbsp;&nbsp;v2, a2 = MAX-VALUE(game, game.RESULT(state, a), &alpha;, &beta;)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v2 &lt; v then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v, move = v2, a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&beta; = MIN(&beta;, v)<br>&nbsp;&nbsp;&nbsp;&nbsp;if v &le; &alpha; then return v, move<br>&nbsp;&nbsp;return v, move</code></p>
        </div>
        <p>Notice that <code>if v &le; &alpha; then return v, move</code> is the reasoning we used to stop exploring `C`. `alpha` is `3` (the highest value we've seen so far) and `v` is `2`. Since `2 le 3`, we're gonna return `2`, i.e., we're assigning the minimax value of `C` to `2`.</p>
      </div>
      <h3>Move ordering</h3>
      <p>Notice that if the last child of `D` (the one with minimax value `2`) had been generated first, then we could've pruned `D`'s other children. This means that, if possible, we should try to examine successors that allow us to prune the tree.</p>
      <p>A move that prunes a tree when examined is called a <b>killer move</b>. Examining those moves first is called the killer move heuristic. It's not examining the node and then finding out that it prunes the tree; it's knowing ahead of time that examining that node will prune the tree. This information comes from past experience. Maybe we have a collection of winning moves and we find a move at our current state in that collection.</p>
      <div class="ln-box">
        <p>In the best case, alpha-beta would only need to examine `O(b^(m//2))` nodes, compared to minimax's best case of `O(b^m)`. When we prune a tree, we're kinda removing half of it.</p>
      </div>
      <h2 id="heuristicalphabetasearch">Heuristic Alpha-Beta Tree Search</h2>
      <p>Even with alpha-beta pruning and good move ordering, there are still too many states to explore for games like chess. One way to deal with this is to use a <b>Type A strategy</b>, which considers all possible moves until a certain depth. Then it uses a heuristic function to estimate the utility of the rest of the states.</p>
      <div class="ln-box">
        <p>A Type A strategy searches a wide but shallow part of the tree. Because of this, a Type A strategy is not good for games with large branching factors, like Go.</p>
        <p>A <b>Type B strategy</b> ignores moves that look bad and only searches moves that look promising. This results in searching a deep but narrow part of the tree.</p>
      </div>
      <p>There's a <b>cutoff test</b> to determine when to stop searching and start estimating. This effectively treats nonterminal nodes as if they were terminal nodes.</p>
      <h3>Evaluation functions</h3>
      <p>A heuristic evaluation function gives us an estimate of a state's expected utility.</p>
      <div class="ln-box">
        <p>`text(EVAL)(s,p)=text(UTILITY)(s,p)` for terminal states.</p>
        <p>`text(UTILITY)(text(loss),p) le text(EVAL)(s,p) le text(UTILITY)(text(win),p)` for nonterminal states.</p>
      </div>
      <p>Evaluation functions work by calculating various <b>features</b> of the state. The information from those features is used to assign a numerical value to that state. A good evaluation function should not take too long to compute (obviously). More importantly, the evaluation function should be strongly correlated with the actual chances of winning.</p>
      <p>For example, one feature we could use is the number of pieces that each player (white vs black) has remaining. If there's a move that results in white having more pieces, then that state should have a high evaluation function value for white.</p>
      <p>Of course, simply having more pieces doesn't necessarily mean it's better. It depends on what types of pieces they are. For example, a queen is much more valuable than having eight pawns and no queen. We take things like this into account by adding weight to certain features that are more important than other features.</p>
      <div class="ln-box">
        <p>Mathematically, this kind of evaluation function is called a <b>weighted linear function</b>.</p>
        <div class="ln-center">
          <p>`text(EVAL)(s) = w_1f_1(s)+w_2f_2(s)+...+w_nf_n(s)`</p>
          <p>where each `f_i` is a feature, such as "number of white bishops"</p>
        </div>
        <p>However, using a linear function only works when the features are independent of each other, which isn't the case in chess. Bishops are generally worth more when there are fewer pieces in the game, i.e., number of bishops and number of pieces remaining are not independent.</p>
      </div>
      <h3>Cutting off search</h3>
      <div class="ln-box">
        <p>To implement this, we can modify <code>ALPHA-BETA-SEARCH</code> by replacing</p>
        <div class="ln-center">
          <p><code>if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null</code></p>
        </div>
        <p>with</p>
        <div class="ln-center">
          <p><code>if game.IS-CUTOFF(state, depth) then return game.EVAL(state, player), null</code></p>
        </div>
      </div>
      <p>Using an evaluation function that just counts the number of pieces is very simple and can lead to errors. For example, suppose the algorithm stops at the cutoff and finds a state where black has more pieces than white, so black moves to that state. But then white moves to capture black's queen! If the algorithm had been able to look ahead, then it would've known that it was not a favorable position after all.</p>
      <p>Positions where a pending move can wildly swing the evaluation are called nonquiescent positions. The evaluation function should be used only for <b>quiescent</b> positions, and search should continue until quiescent positions are reached.</p>
      <h3>Forward pruning</h3>
      <p>Alpha-beta pruning prunes parts of the tree that were for sure bad moves. <b>Forward pruning</b> prunes parts of the tree that look like they have bad moves.</p>
      <div class="ln-box">
        <p>Forward pruning is a Type B strategy.</p>
      </div>
      <p>One way to implement forward pruning is to use <b>beam search</b>, which considers a "beam" of `n` moves. Of course, there's a chance that we miss the the best move by doing this.</p>
      <h2 id="montecarlosearch">Monte Carlo Tree Search</h2>
      <p><b>Monte Carlo tree search (MCTS)</b> goes through many <b>simulations</b> to determine which moves are good. Basically, the algorithm plays out many games of "what if I do this and what if they do this?" and sees which choices led to wins and losses. So instead of using a heuristic evaluation function to assign values to states, it uses an average utility score (defined by the number of wins/losses and games) to assign values to states.</p>
      <div class="ln-box">
        <p>A <b>pure Monte Carlo search</b> does `N` simulations from the current state of the game and keeps track of which moves (from the current position) has the highest win percentage.</p>
      </div>
      <p>While it's not possible to know exactly how an opponent is going to respond to a move in a simulation, there are almost always certain moves that are better than others, and we can create <b>playout policies</b> that tell the algorithm what these moves are so they can prioritize those moves in the simulations.</p>
      <p>Let's suppose we have this tree for a game played by pink and brown. Each fraction in the node is the number of simulations won over the number of simulations played. Our current state is the pink `37//100`, so from here, there are `3` possible moves we can consider. One move won `60` out of the `79` simulations; the second move won `1` out of `10` simluations; and the third move won `2` out of `11` simulations.</p>
      <p>It makes sense to choose the move that had the highest win percentage, but this doesn't necessarily mean the other two moves are bad. This is because we only had `10` and `11` simluations for those moves; it's possible that they could have performed even better if more simluations had been played out.</p>
      <div class="ln-box">
        <p>Likewise, the move with the highest win percentage doesn't always mean it's the best move to make either. Winning `2` out of `3` simulations isn't enough information to know if the move is actually good or not.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/monte_carlo_1.png">
      </div>
      <p>This decision about which state to go to next is decided by a <b>selection policy</b>, i.e., the selection policy tells the algorithm which state to go to next. There are two factors that influence the selection policy: <b>exploitation</b> and <b>exploration</b>. Exploitation is choosing states that have performed well (e.g., choosing the move with the highest win percentage) and exploration is choosing states that have had only a few simulations (to gather more information and to take a chance on it turning out to be a good move).</p>
      <p>So now the search has progressed three moves later and our current state is at the brown `27//35`. It's time to run a simulation. (The moves that are played in this simulation are decided by the playout policy.)</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/monte_carlo_2.png">
      </div>
      <p>Let's say that the simulation resulted in a loss for pink. Now we have to update the tree with this new information. This step is called <b>back-propagation</b>.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/monte_carlo_3.png">
      </div>
      <p>To summarize the process:</p>
      <ul>
        <li><b>Selection</b>: keep choosing moves (guided by the selection policy) until we reach a leaf</li>
        <li><b>Expansion</b>: generate a new child</li>
        <li><b>Simulation</b>: play out a simulation (guided by the playout policy)</li>
        <li><b>Back-propagation</b>: update all the search tree nodes with the new information</li>
      </ul>
      <div class="ln-box">
        <p>One very effective selection policy is called "upper confidence bounds applied to trees" (<b>UCT</b>). It uses a formula (<b>UCB1</b>) to rank each possible move.</p>
        <div class="ln-center">
          <p>`text(UCB1)(n)=(U(n))/(N(n))+C*sqrt((logN(text(PARENT)(n)))/(N(n)))`</p>
          <p>`U(n)` is the total utility (e.g., number of wins) of all simulations that went through node `n`.</p>
          <p>`N(n)` is the number of simulations through node `n`.</p>
          <p>I think `text(PARENT)(n)` is the number of times `n`'s parent node has been selected for exploration.</p>
          <p>`C` is a constant that balances exploration and exploitation.</p>
        </div>
        <p>`(U(n))/(N(n))` is the exploitation term and `sqrt((logN(text(PARENT)(n)))/(N(n)))` is the exploration term.</p>
      </div>
      <h2 id="stochasticgames">Stochastic Games</h2>
      <p>Stochastic games are ones that involve randomness and luck. Like games that involve rolling dice. In backgammon, the possible moves that a player can make is determined by the result of the dice rolls. So we can't really build the game tree the same way we did before because we don't know what the possible moves will be.</p>
      <p>We have to include <b>chance nodes</b>, which represent a possible outcome from the random aspect of the game. So if a die can have `6` different values, then there will be `6` chance nodes for each value of the die and then we list all the possible moves for each chance node. If we're rolling two dice, then there will be a chance node for every possible pair of values.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/chance_nodes.png">
        <p>For a chance node, there is a branch for each possible outcome of rolling two dice. The probability of rolling a `1` and a `1` is `1/36`. The probability of rolling a `1` and a `2` is `1/18`.</p>
      </div>
      <p>Since we don't know for sure that a chance node will be reached, they don't have definite minimax values. Instead, each chance node has an <b>expectiminimax value</b>, which is the sum of the minimax values of each action weighted by the probability of that action (this is the <b>expected value</b> of the chance node).</p>
      <div class="ln-box">
        <p>The expectiminimax value of a non-chance node is its minimax value.</p>
      </div>
      <h3>Evaluation functions for games of chance</h3>
      <p>Recall that in heuristic alpha-beta search, we kept searching until a certain point, then used a heuristic function to estimate the rest of the minimax values. For stochastic games, we can do the same, but we need a slightly different heuristic function.</p>
      <p>Let's go back to the game where MAX is trying to reach the node with the highest value and MIN is trying to have MAX end up at the node with the lowest value. Suppose we have this game tree:</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/stochastic_game_tree_1.png">
        <p>The left chance node's expectiminimax value is `2*0.9+3*0.1=2.1`.</p>
      </div>
      <p>We can see that the left chance node has a higher expectiminimax value, so MAX should go left (move `a_1`). We can also see that this is the optimal move for MAX since moving right means that there is a `90%` chance that MIN will be able to go to the node with `1`.</p>
      <p>But let's look at what happens if the heuristic function had assigned different values to each of the nodes at the cutoff point.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/stochastic_game_tree_2.png">
      </div>
      <p>Now, the right chance node has a higher expectiminimax value, so MAX will go right (move `a_2`). But we can see that this is not the optimal choice because there is a `90%` chance that MIN will be able to go to the node with `1`.</p>
      <p>From this, we can see that the heuristic function must also take into account the probabilities when calculating the utility. (I might go into more detail on this? Let's see where the class goes.)</p>
      <h2 id="partiallyobservablegames">Partially Observable Games</h2>
      <h3>Kriegspiel: Partially observable chess</h3>
      <p><b>Kriegspiel</b> is an interesting version of chess where each player has their own board, and each player's board has only their own pieces. Each player proposes a move and a referee (who can see both players' boards) tells them if the move is legal or illegal. If the move is illegal, that player keeps proposing moves until a legal one is found. If the move is legal, the referee announces captures, checks, and mates if any of those happened.</p>
      <p>Using belief states helps here. In the context of games, a <b>belief state</b> is the set of all board states that are logically possible based on the moves that have happened so far. With this, we can use the AND-OR search algorithm we saw before.</p>
      <div class="ln-box">
        <p>Instead of branches for each possible move the opponent might make, we have branches for every possible percept sequence that might be received.</p>
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/kriegspiel.png">
        <p>(I'm including a picture from the textbook because this is too complicated to draw myself. )</p>
      </div>
      <div class="ln-box">
        <p>While it's important to move pieces to the right places, playing optimally can be predictable, giving the opponent information. Sometimes, making random moves is optimal because it introduces an element of unpredictability that the opponent may not be prepared for.</p>
      </div>
      <div class="ln-box">
        <h3>Summary of Chapter 5</h3>
        <ul>
          <li>A game can be defined by the <b>initial state</b> (how the board is set up), the legal <b>actions</b> in each state, the <b>result</b> of each action, a <b>terminal test</b> (which says when the game is over), and a <b>utility function</b> that applies to terminal states to say who won and what the final score is.</li>
          <li>In two-player, discrete, deterministic, turn-taking zero-sum games with <b>perfect information</b>, the <b>minimax</b> algorithm can select optimal moves by a depth-first enumeration of the game tree.</li>
          <li>The <b>alpha-beta</b> search algorithm computes the same optimal move as minimax, but achieves much greater efficiency by eliminating subtrees that are provably irrelevant.</li>
          <li>Usually, it is not possible to consider the whole game tree (even with alpha-beta), so we need to cut the search off at some point and apply a heuristic <b>evaluation function</b> that estimates the utility of a state.</li>
          <li>An alternative called <b>Monte Carlo tree search</b> (MCTS) evaluates states not by applying a heuristic function, but by playing out the game all the way to the end and using the rules of the game to see who won. Since the moves during the <b>playout</b> may not have been optimal moves, the process is repeated multiple times and the evaluation is the average of the results.</li>
          <li>Many game programs precompute tables of best moves in the opening and endgame so that they can look up a move rather than search for one.</li>
          <li>Games of chance can be handled by <b>expectiminimax</b>, an extension to the minimax algorithm that evaluates a <b>chance node</b> by taking the average utility of all its children, weighted by the probability of each child.</li>
          <li>In games of <b>imperfect information</b>, like poker, optimal play requires reasoning about the current and future <b>belief states</b> of each player. A simple approximation can be obtained by averaging the value of an action over each possible configuration of missing information.</li>
        </ul>
      </div>
      <h2 id="knowledgebasedagents">Knowledge-Based Agents</h2>
      <p>So far, all we've seen are problem-solving agents. As powerful as they are, they aren't able to reason or adapt to new information.</p>
      <p>Knowledge-based agents are able to use <b>logic</b> to make decisions. They have a <b>knowledge base</b> of <b>sentences</b>, which are the facts that are known by the agent. They use the sentences in their knowledge base to come up with new sentences (conclusions). This process is called <b>inference</b>.</p>
      <h2 id="thewumpusworld">The Wumpus World</h2>
      <p>Here, we'll go over an example of how a knowledge-based agent works.</p>
      <p>The wumpus world is a cave with a monster (wumpus), a pile of gold, and a bunch of bottomless pits. The goal of the agent is to find the gold while avoiding the pits and the wumpus. The wumpus can be killed with an arrow, but the agent has only one arrow to use.</p>
      <div class="ln-box">
        <p>The wumpus world is partially observable, single-agent, deterministic, sequential, static, and discrete.</p>
        <p>PEAS description:</p>
        <ul>
          <li>Performance measure: `+1000` for escaping with the gold, `-1000` for falling into a pit or being eaten by the wumpus, `-1` for each action taken, `-10` for using an arrow</li>
          <li>Environment: A `n xx n` grid of rooms. The locations of the gold and the wumpus are chosen randomly. Each square (other than the start) can be a pit with some probability (say `0.2`)</li>
          <li>Actuators: the agent can move forward, turn left, turn right, grab the gold, shoot an arrow, and climb out of the cave</li>
          <li>Sensors: the agent can perceive a stench (from the wumpus), a breeze (from a pit), a glitter (from the gold), a bump (when walking into a wall), and a scream (when the wumpus is killed)
            <ul>
              <li>the stench and a breeze are detected when the agent is on a square next to a wumpus or a pit</li>
            </ul>
          </li>
        </ul>
      </div>
      <p>Let's say our agent (denoted as `A`) starts in the lower-left corner. It doesn't detect a stench or a breeze so the two squares next to it are safe.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_world_1.png">
      </div>
      <p>After moving right, the agent detects a breeze. The breeze could be coming from the square above or from the right. (Bear with me here, the agent can't lick its finger to see which direction the breeze is blowing.) There is only one square that is for sure safe, so the agent decides to go back and move there.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_world_2.png">
      </div>
      <p>Here, the agent detects a stench. Theoretically, the stench could be coming from the square above <em>or</em> from the right. But since the agent didn't detect a stench in the previous state (image above), it knows the wumpus can't be in the square to the right.</p>
      <p>Also, since the agent is not detecting a breeze, that means the square to the right is not a pit.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_world_3.png">
      </div>
      <p>Avoiding the wumpus, the agent finds the gold!</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_world_4.png">
      </div>
      <h2 id="logic">Logic</h2>
      <p>In order for a knowledge-based agent to be able to reason and make inferences, it needs to understand logic.</p>
      <p>A sentence has a <b>truth</b> value, i.e., it can either be true or false. Its truth value depends on the world (<b>model</b>) it's in.</p>
      <div class="ln-box">
        <p>The sentence `x+y=4` is true in a world where `x=2,y=2` and false in a world where `x=1,y=1`.</p>
      </div>
      <p>If a sentence `alpha` is true in model `m`, then `m` <b>satisfies</b> `alpha` (or `m` <b>is a model of</b> `alpha`).</p>
      <p>The truth value of a sentence can depend on the truth value of another one. We say that a sentence can follow logically from another sentence.</p>
      <div class="ln-box">
        <p>The sentence `x=0` entails the sentence `xy=0`, i.e., the sentence `xy=0` follows logically from `x=0`.</p>
      </div>
      <div class="ln-box">
        <p>`alpha` entails `beta` is denoted as `alpha |== beta`.</p>
        <p>The formal definition of entailment is: `alpha |== beta` if and only if, in every model in which `alpha` is true, `beta` is also true.</p>
        <div class="ln-center">
          <p>`alpha |== beta iff M(alpha) sube M(beta)`</p>
          <p>(`M(alpha)` is the set of models where `alpha` is true.)</p>
        </div>
      </div>
      <p>Let's see how this can be applied to the wumpus world. Suppose the agent is at `[2,1]` (the second step when it detected a breeze).</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_world_2.png">
      </div>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_entailment_1.png">
      </div>
      <p>`KB` is the agent's knowledge base. There are three possible models based on the knowledge that it feels a breeze on its current square (either the pit is in the square above, to the right, or both).</p>
      <p>Let `alpha_1` be the sentence, "There is no pit in `[1,2]`." There are four possible models based on this sentence.</p>
      <p>Notice that in every model in which `KB` is true, `alpha_1` is also true. That is, if you pick any model in `KB` and say "There is no pit in `[1,2]`", the sentence is true. This means that `KB` entails `alpha_1`, and the agent can conclude that there is no pit in `[1,2]`.</p>
      <p>Now let `alpha_2` be the sentence, "There is no pit in `[2,2]`."</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_entailment_2.png">
      </div>
      <p>There are some models in `KB` where if we say "There is no pit in `[2,2]`", the sentence is false. So `KB` does not entail `alpha_2`, meaning that the agent cannot conclude that there is no pit in `[2,2]`.</p>
      <p>Whether you realized it or not, we were running an algorithm to check entailment. The inference algorithm is called <b>model checking</b>.</p>
      <p>An inference algorithm is <b>sound</b> or <b>truth-preserving</b> if it derives only entailed sentences. That is, the results of running the algorithm are all sentences that follow logically. An unsound algorithm makes things up.</p>
      <div class="ln-box">
        <p>If it's provable, it's true.</p>
      </div>
      <p>An inference algorithm is <b>complete</b> if it can derive any sentence that is entailed. That is, all sentences that follow logically can be derived using the algorithm.</p>
      <div class="ln-box">
        <p>If it's true, it's provable.</p>
      </div>
      <h2 id="propositionallogic">Propositional Logic: A Very Simple Logic</h2>
      <h3>Syntax</h3>
      <p>A <b>proposition</b> is a sentence (that can be true or false). We use a symbol (typically a letter) to represent a proposition. For example, `W_(1,3)` can be the symbol for "there is a wumpus at `[1,3]`" and `P_(3,1)` can be the symbol for "there is a pit at `[3,1]`."</p>
      <p>We can use <b>logical connectives</b> to turn sentences into <b>complex sentences</b>. The most common connectives are</p>
      <ul>
        <li>`not` (not). `not W_(1,3)` (the <b>negation</b> of `W_(1,3)`) means "there is no wumpus at `[1,3]`."</li>
        <li>`^^` (and). `W_(1,3) ^^ P_(3,1)` means "there is a wumpus at `[1,3]` and there is a pit at `[3,1]`."
          <ul>
            <li>a sentence of this form is called a <b>conjunction</b></li>
          </ul>
        </li>
        <li>`vv` (or). `W_(1,3) vv P_(3,1)` means "there is a wumpus at `[1,3]` or there is a pit at `[3,1]`."
          <ul>
            <li>a sentence of this form is called a <b>disjunction</b></li>
          </ul>
        </li>
        <li>`implies` (implies). `W_(1,3) implies not W_(2,2)` means "if there is a wumpus at `[1,3]`, then there is no wumpus at `[2,2]`."
          <ul>
            <li>a sentence of this form is called an <b>implication</b></li>
          </ul>
        </li>
        <li>`iff` (if and only if). `W_(1,3) iff not W_(2,2)` means "there is a wumpus at `[1,3]` if and only if there is no wumpus at `[2,2]`."
          <ul>
            <li>a sentence of this form is called a <b>biconditional</b></li>
          </ul>
        </li>
      </ul>
      <div class="ln-box">
        <p>An <b>atomic sentence</b> is a sentence without any connectives; it's just a single symbol.</p>
        <p>A <b>literal</b> is an atomic sentence.</p>
        <p><b>Complementary literals</b> are literals where one is the negation of the other.</p>
      </div>
      <div class="ln-box">
        <p>A <b>clause</b> is a disjunction of literals.</p>
      </div>
      <h3>Semantics</h3>
      <p>The semantics of propositional logic defines the rules for determining the truth of a sentence. Here's a <b>truth table</b> that defines the connectives:</p>
      <table class="table">
        <tr>
          <td>`P`</td>
          <td class="border-end">`Q`</td>
          <td>`not P`</td>
          <td>`P^^Q`</td>
          <td>`PvvQ`</td>
          <td>`P implies Q`</td>
          <td>`P iff Q`</td>
        </tr>
        <tr>
          <td>false</td>
          <td class="border-end">false</td>
          <td>true</td>
          <td>false</td>
          <td>false</td>
          <td>true</td>
          <td>true</td>
        </tr>
        <tr>
          <td>false</td>
          <td class="border-end">true</td>
          <td>true</td>
          <td>false</td>
          <td>true</td>
          <td>true</td>
          <td>false</td>
        </tr>
        <tr>
          <td>true</td>
          <td class="border-end">false</td>
          <td>false</td>
          <td>false</td>
          <td>true</td>
          <td>false</td>
          <td>false</td>
        </tr>
        <tr>
          <td>true</td>
          <td class="border-end">true</td>
          <td>false</td>
          <td>true</td>
          <td>true</td>
          <td>true</td>
          <td>true</td>
        </tr>
      </table>
      <h3>A simple knowledge base</h3>
      <p>Using all of this, we can build a knowledge base for the wumpus world. For the symbols, we have:</p>
      <ul>
        <li>`P_(x,y)` is true if there is a pit in `[x,y]`</li>
        <li>`W_(x,y)` is true if there is a wumpus in `[x,y]`</li>
        <li>`B_(x,y)` is true if there is a breeze in `[x,y]`</li>
        <li>`S_(x,y)` is true if there is a stench in `[x,y]`</li>
        <li>`L_(x,y)` is true if the agent is in `[x,y]`</li>
      </ul>
      <p>With these symbols, we can create sentences that describe what the agent saw (and knows) in the wumpus world. (Each sentence will be labeled with `R_i`. The `R_i`s themselves are not symbols.)</p>
      <ul>
        <li>There is no pit in `[1,1]`
          <ul>
            <li>`R_1`: `not P_(1,1)`</li>
          </ul>
        </li>
        <li>A square is breezy if and only if there is a pit in a neighboring square
          <ul>
            <li>`R_2`: `B_(1,1) iff (P_(1,2) vv P_(2,1))`</li>
            <li>`R_3`: `B_(2,1) iff (P_(1,1) vv P_(2,2) vv P_(3,1))`</li>
          </ul>
        </li>
        <li>There is no breeze in `[1,1]`
          <ul>
            <li>`R_4`: `not B_(1,1)`</li>
          </ul>
        </li>
        <li>There is a breeze in `[2,1]`
          <ul>
            <li>`R_5`: `B_(2,1)`</li>
          </ul>
        </li>
      </ul>
      <h3>A simple inference procedure</h3>
      <p>We can now perform the model-checking algorithm a little more formally. The relevant symbols are `B_(1,1),B_(2,1),P_(1,1),P_(1,2),P_(2,1),P_(2,2),P_(3,1)`. A model is an assignment of true or false to each symbol.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_entailment_1.png">
      </div>
      <div class="ln-box">
        <p>Notice that in all the models in `KB`, statements `R_1,R_2,R_3,R_4,R_5` are all true. `KB` is true if `R_1` through `R_5` are true because those statements make up the knowledge base.</p>
        <p>In fact, a systematic way to generate all the models in the knowledge base is to consider all the possible assignments to each symbol that make `R_1` through `R_5` true.</p>
      </div>
      <p>For example, here are the assignments for each of the three models in `KB` from top to bottom:</p>
      <ul>
        <li>`B_(1,1)=F,B_(2,1)=T,P_(1,1)=F,P_(1,2)=F,P_(2,1)=F,P_(2,2)=F,P_(3,1)=T`</li>
        <li>`B_(1,1)=F,B_(2,1)=T,P_(1,1)=F,P_(1,2)=F,P_(2,1)=F,P_(2,2)=T,P_(3,1)=F`</li>
        <li>`B_(1,1)=F,B_(2,1)=T,P_(1,1)=F,P_(1,2)=F,P_(2,1)=F,P_(2,2)=T,P_(3,1)=T`</li>
      </ul>
      <p>Can `not P_(1,2)` (there is no pit in `[1,2]`) be derived from `KB`? Well, in all three models, `P_(1,2)` is false, meaning there is no pit in `[1,2]`. So yes, `not P_(1,2)` is entailed by `KB`.</p>
      <h2 id="propositionaltheoremproving">Propositional Theorem Proving</h2>
      <p>If there are a lot of models in the knowledge base, then it's not efficient to check each model to determine entailment.</p>
      <p><b>Theorem proving</b> is a method of deriving a sentence by applying the rules of inference to the sentences in the knowledge base.</p>
      <div class="ln-box">
        <p>Two sentences are <b>logically equivalent</b> if they are true in the same set of models. This is denoted as `alpha -= beta`.</p>
        <p>Two sentences are logically equivalent if and only if each of them entails the other:</p>
        <div class="ln-center">
          <p>`alpha -= beta` if and only if `alpha |== beta` and `beta |== alpha`</p>
        </div>
        <p>The <b>deduction theorem</b> says `alpha |== beta` if and only if the sentence `alpha implies beta` is true in <em>all</em>* models. This means we can decide if `alpha |== beta` by checking that `alpha implies beta` is true in every model.</p>
        <p>*A sentence that is true in all models is <b>valid</b>. Valid sentences are also called <b>tautologies</b>. `P vv not P` is an example.</p>
      </div>
      <div class="ln-box">
        <p>It might seem that `-=` and `iff` are the same. The subtle difference between them is that `-=` is used to make claims about sentences while `iff` is used to make sentences.</p>
      </div>
      <div class="ln-box">
        <p>A sentence is <b>satisfiable</b> if it is true in at least one model.</p>
      </div>
      <div class="ln-box">
        <p>One way of proving `alpha |== beta` is to show that `(alpha ^^ not beta)` is unsatisfiable.</p>
      </div>
      <h3>Inference and proofs</h3>
      <p>To derive a sentence, we have to apply <b>inference rules</b> to come up with a <b>proof</b>.</p>
      <p><b>Modus Ponens</b> is a rule that says if we have the sentence `alpha implies beta` and we know `alpha` is true, then we can conclude `beta` is true.</p>
      <div class="ln-center">
        <p>`(alpha implies beta, alpha)/beta`</p>
        <p>this notation means that the stuff on the top is true, and using that, we can conclude the stuff on the bottom</p>
      </div>
      <div class="ln-box">
        <p>If there is a breeze, then there is a pit nearby. There is a breeze.</p>
        <p>Therefore, there is a pit nearby.</p>
      </div>
      <p><b>And-Elimination</b> is a rule that says if we have a conjunction that is true, then both of the individual sentences by themselves are true.</p>
      <div class="ln-center">
        <p>`(alpha ^^ beta)/alpha`</p>
      </div>
      <div class="ln-box">
        <p>There is a wumpus ahead and the wumpus is alive.</p>
        <p>There is a wumpus ahead.</p>
      </div>
      <div class="ln-box">
        <p>Here are some logical equivalences (these can also be thought of as inference rules):</p>
        <div class="ln-center">
          <table class="table">
            <tr>
              <td>`(alpha ^^ beta) -= (beta ^^ alpha)`</td>
              <td>commutativity of `^^`</td>
            </tr>
            <tr>
              <td>`(alpha vv beta) -= (beta vv alpha)`</td>
              <td>commutativity of `vv`</td>
            </tr>
            <tr>
              <td>`((alpha ^^ beta) ^^ gamma) -= (alpha ^^ (beta ^^ gamma))`</td>
              <td>associativity of `^^`</td>
            </tr>
            <tr>
              <td>`((alpha vv beta) vv gamma) -= (alpha vv (beta vv gamma))`</td>
              <td>associativity of `vv`</td>
            </tr>
            <tr>
              <td>`not(not alpha) -= alpha`</td>
              <td>double-negation elimination</td>
            </tr>
            <tr>
              <td>`(alpha implies beta) -= (not beta implies not alpha)`</td>
              <td>contraposition</td>
            </tr>
            <tr>
              <td>`(alpha implies beta) -= (not alpha vv beta)`</td>
              <td>implication elimination</td>
            </tr>
            <tr>
              <td>`(alpha iff beta) -= ((alpha implies beta) ^^ (beta implies alpha))`</td>
              <td>biconditional elimination</td>
            </tr>
            <tr>
              <td>`not (alpha ^^ beta) -= (not alpha vv not beta)`</td>
              <td>De Morgan's Law</td>
            </tr>
            <tr>
              <td>`not (alpha vv beta) -= (not alpha ^^ not beta)`</td>
              <td>De Morgan's Law</td>
            </tr>
            <tr>
              <td>`(alpha ^^ (beta vv gamma)) -= ((alpha ^^ beta) vv (alpha ^^ gamma))`</td>
              <td>distributivity of `^^` over `vv`</td>
            </tr>
            <tr>
              <td>`(alpha vv (beta ^^ gamma)) -= ((alpha vv beta) ^^ (alpha vv gamma))`</td>
              <td>distributivity of `vv` over `^^`</td>
            </tr>
          </table>
        </div>
      </div>
      <p>With these inference rules, we can prove a sentence is true without having to check each model in the knowledge base. Let's see how we can prove `not P_(1,2)` using these rules.</p>
      <p>We have these sentences in our knowledge base:</p>
      <ul>
        <li>`R_1`: `not P_(1,1)`</li>
        <li>`R_2`: `B_(1,1) iff (P_(1,2) vv P_(2,1))`</li>
        <li>`R_3`: `B_(2,1) iff (P_(1,1) vv P_(2,2) vv P_(3,1))`</li>
        <li>`R_4`: `not B_(1,1)`</li>
        <li>`R_5`: `B_(2,1)`</li>
      </ul>
      <p>The only sentence that involves `P_(1,2)` is `R_2`, so we'll start there.</p>
      <p>Apply biconditional elimination to `R_2` to get</p>
      <div class="ln-center">
        <p>`R_6`: `(B_(1,1) implies (P_(1,2) vv P_(2,1))) ^^ ((P_(1,2) vv P_(2,1)) implies B_(1,1))`</p>
      </div>
      <p>Apply And-Elimination to `R_6` to get</p>
      <div class="ln-center">
        <p>`R_7`: `((P_(1,2) vv P_(2,1)) implies B_(1,1))`</p>
      </div>
      <p>Use the logical equivalence for contrapositives to get</p>
      <div class="ln-center">
        <p>`R_8`: `(not B_(1,1) implies not (P_(1,2) vv P_(2,1)))`</p>
      </div>
      <p>Apply Modus Ponens with `R_8` and `R_4` to get</p>
      <div class="ln-center">
        <p>`R_9`: `not (P_(1,2) vv P_(2,1))`</p>
      </div>
      <p>Apply De Morgan's rule to get</p>
      <div class="ln-center">
        <p>`R_10`: `not P_(1,2) ^^  not P_(2,1)`</p>
      </div>
      <p>Apply And-Elimination to `R_10` to get</p>
      <div class="ln-center">
        <p>`R_11`: `not P_(1,2)`</p>
      </div>
      <h3>Proof by resolution</h3>
      <p><b>Resolution</b> is another inference rule. It can be loosely thought of as a "cancellation", like in math. Let's say we have two sentences `alpha vv beta` and `not alpha`. Then the `alpha` and `not alpha` "cancel out" (the technical term is "resolves with") and we can conclude `beta`.</p>
      <div class="ln-box">
        <p>There is a pit or a wumpus ahead. There is no pit ahead.</p>
        <p>There is a wumpus ahead.</p>
      </div>
      <p>Let's see how we can use resolution to prove that there is a pit in `[3,1]`.</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_world_3.png">
      </div>
      <p>We'll add some more sentences to the knowledge base:</p>
      <ul>
        <li>`R_12`: `not B_(1,2)`</li>
        <li>`R_13`: `B_(1,2) iff (P_(1,1) vv P_(2,2) vv P_(1,3))`</li>
      </ul>
      <p>Using the exact same rules in the proof above, we can derive</p>
      <ul>
        <li>`R_14`: `not P_(2,2)`</li>
        <li>`R_15`: `not P_(1,3)`</li>
      </ul>
      <p>We can use biconditional elimination and And-Elimination  on `R_3` (which is `B_(2,1) iff (P_(1,1) vv P_(2,2) vv P_(3,1))`) to get</p>
      <div class="ln-center">
        <p>`B_(2,1) implies (P_(1,1) vv P_(2,2) vv P_(3,1))`</p>
      </div>
      <p>Then we can use Modus Ponens with `R_5` (which is `B_(2,1)`) to get</p>
      <div class="ln-center">
        <p>`R_16`: `P_(1,1) vv P_(2,2) vv P_(3,1)`</p>
      </div>
      <p>Now we can use our new resolution rule. We have `not P_(1,1)` (from `R_1`), which resolves with the `P_(1,1)` in `R_16` to get</p>
      <div class="ln-center">
        <p>`R_17`: `P_(2,2) vv P_(3,1)`</p>
      </div>
      <p>We also have `not P_(2,2)` (from `R_14`), which resolves with the `P_(2,2)` in `R_17` to get</p>
      <div class="ln-center">
        <p>`R_18`: `P_(3,1)`</p>
      </div>
      <h4>Conjunctive normal form</h4>
      <p>The resolution rule only works for clauses (a.k.a. disjunctions of literals) though. So how is this helpful for sentences that aren't clauses? Well, it turns out that every sentence is logically equivalent to a conjunction of clauses. That is, any sentence can be converted into a combination of clauses.</p>
      <p>A sentence that is a conjunction of clauses is in <b>conjunctive normal form</b> or <b>CNF</b>.</p>
      <p>Let's look at how to convert `B_(1,1) iff (P_(1,2) vv P_(2,1))` into CNF.</p>
      <div class="ln-center">
        <p>Biconditional elimination:</p>
        <p>`(B_(1,1) implies (P_(1,2) vv P_(2,1)) ^^ ((P_(1,2) vv P_(2,1)) implies B_(1,1))`</p>
      </div>
      <div class="ln-center">
        <p>Implication elimination:</p>
        <p>`(not B_(1,1) vv P_(1,2) vv P_(2,1)) ^^ (not (P_(1,2) vv P_(2,1)) vv B_(1,1))`</p>
      </div>
      <div class="ln-center">
        <p>De Morgan's rule*:</p>
        <p>`(not B_(1,1) vv P_(1,2) vv P_(2,1)) ^^ ((not P_(1,2) ^^ not P_(2,1)) vv B_(1,1))`</p>
      </div>
      <div class="ln-center">
        <p>Distributivity of `vv` over `^^`<sup></sup>:</p>
        <p>`(not B_(1,1) vv P_(1,2) vv P_(2,1)) ^^ (not P_(1,2) vv B_(1,1)) ^^ (not P_(2,1) vv B_(1,1))`</p>
      </div>
      <div class="ln-box">
        <p>*CNF requires `not` to appear only in literals, so we need to keep going here.</p>
        <p><sup></sup>Each clause joined by an `^^` needs to be a literal or a disjunction, so we need to keep going here.</p>
      </div>
      <div class="ln-box">
        <p>Why is it important that any sentence can be turned into a conjunction of clauses? It's because resolution leads to a complete inference procedure for all of propositional logic. A resolution-based theorem prover can, for any sentences `alpha` and `beta`, decide whether `alpha |== beta`.</p>
      </div>
      <h4>A resolution algorithm</h4>
      <p>Resolution algorithms are based on the idea of proving a contradiction. Recall that proving `KB |== alpha` is the same as proving `(KB ^^ not alpha)` is unsatisfiable. So if we assume `(KB ^^ not alpha)` is true and arrive at a contradiction, then `KB |== alpha`.</p>
      <div class="ln-flex-center">
        <p><code>function PL-RESOLUTION(KB, &alpha;)<br>&nbsp;&nbsp;clauses = the set of clauses in the CNF representation of KB &and; &alpha;<br>&nbsp;&nbsp;new = {}<br>&nbsp;&nbsp;while true do<br>&nbsp;&nbsp;&nbsp;&nbsp;for each pair of clauses C_i, C_j in clauses do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;resolvents = PL-RESOLVE(C_i, C_j)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if resolvents contains the empty clause then return true<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;new = new &cup; resolvents<br>&nbsp;&nbsp;&nbsp;&nbsp;if new &sube; clauses then return false<br>&nbsp;&nbsp;&nbsp;&nbsp;clauses = clauses &cup; new</code></p>
      </div>
      <div class="ln-center">
        <p><code>KB</code>: a sentence in the knowledge base</p>
        <p><code>&alpha;</code>: the sentence we're trying to prove</p>
      </div>
      <p>First, `(KB ^^ not alpha)` is converted into CNF. Then the resolution rule is applied to the resulting clauses. Each pair that contains complementary literals is resolved to produce a new clause, which is added to <code>new</code>.</p>
      <p>This keeps going until one of two things happens:</p>
      <ul>
        <li>there are no new clauses that can be added, which means `KB` does not entail `alpha`</li>
        <li>two clauses resolve to yield the empty clause, which means `KB` does entail `alpha`
          <ul>
            <li>if the empty clause is yielded, then there is a contradiction</li>
          </ul>
        </li>
      </ul>
      <p>Let's go back to the first step in the wumpus world and look at this algorithm in action. The agent is in `[1,1]` and there is no breeze. How do we use resolution to prove there is no pit in `[1,2]`?</p>
      <div class="ln-center">
        <img class="img-fluid ln-image-small" src="../pictures/ai/wumpus_world_1.png">
      </div>
      <p>The relevant knowledge base is</p>
      <div class="ln-center">
        <p>`KB=R_2 ^^ R_4 = (B_(1,1) iff (P_(1,2) vv P_(2,1)) ^^ not B_(1,1))`</p>
      </div>
      <p>and `alpha` is</p>
      <div class="ln-center">
        <p>`alpha = not P_(1,2)`</p>
      </div>
      <p>When we convert `(KB ^^ not alpha)` into CNF, we get</p>
      <div class="ln-center">
        <p>`(not B_(1,1) vv P_(1,2) vv P_(2,1)) ^^ (not P_(1,2) vv B_(1,1)) ^^ (not P_(2,1) vv B_(1,1)) ^^ not B_(1,1) ^^ P_(1,2)`</p>
      </div>
      <p>This gives us five clauses. For each pair of clauses, we apply resolution to get new clauses. For example,</p>
      <div class="ln-center">
        <p>`not P_(2,1) vv B_(1,1)` with `not B_(1,1) vv P_(1,2) vv P_(2,1)` results in `P_(1,2) vv P_(2,1) vv not P_(2,1)`.</p>
        <p>`not P_(1,2) vv B_(1,1)` with `not B_(1,1)` results in `not P_(1,2)`.</p>
      </div>
      <p>We can see that if we were to continue, we would eventually apply the resolution rule to `P_(1,2)` and `not P_(1,2)` to get the empty clause. This means that `alpha = not P_(1,2)` is proven.</p>
      <h3>Horn clauses and definite clauses</h3>
      <p>A <b>definite clause</b> is a disjunction of literals where exactly one of them is positive. For example, `alpha vv not beta vv not gamma` is a definite clause.</p>
      <div class="ln-box">
        <p>A <b>Horn clause</b> is a disjunction of literals where at most one of them is positive.</p>
      </div>
      <p>So what's the point of definite clauses? It turns out that having a knowledge base of definite clauses allows us to use another set of inference algorithms called <b>forward-chaining</b> and <b>backward-chaining</b>.</p>
      <h3>Forward and backward chaining</h3>
      <p>Suppose we had the implication `(alpha ^^ beta) implies gamma`. Forward chaining says that if we know `alpha` and `beta` are true, then we can add `gamma` to the knowledge base, i.e., we can conclude that `gamma` is true. (Forward chaining is essentially an implementation of Modus Ponens.)</p>
      <p>Let's say we had these statements in the knowledge base:</p>
      <div class="ln-center">
        <p>`A`</p>
        <p>`B`</p>
        <p>`A ^^ B implies L`</p>
        <p>`A ^^ P implies L`</p>
        <p>`B ^^ L implies M`</p>
        <p>`L ^^ M implies P`</p>
        <p>`P implies Q`</p>
      </div>
      <p>and let's say we wanted to prove that `Q` is true.</p>
      <p>With forward chaining, we start with the facts that we know are true (which are `A` and `B` in this case) and see if we can reach the goal.</p>
      <ol>
        <li>`A` and `B` give us `L`</li>
        <li>`B` and `L` give us `M`</li>
        <li>`L` and `M` give us `P`</li>
        <li>`P` gives us `Q`</li>
      </ol>
      <p>Backward chaining is kind of the same thing, just with a different mindset. With backward chaining, we start with what we want to prove (`Q` in this case) and see if we have enough information to prove it.</p>
      <ol>
        <li>If we want to prove `Q`, we need to show `P` is true.</li>
        <li>In order to show `P` is true, we need to show `L` and `M` are true.</li>
        <li>In order to show `M` is true, we need to show `B` and `L` are true.
          <ul>
            <li>Well, we already have `B`, so we just need `L`.</li>
          </ul>
        </li>
        <li>In order to show `L` is true, we need to show `A` and `B` are true.
          <ul>
            <li>Well, we already have `A` and `B`, so we can get `L`.</li>
          </ul>
        </li>
        <li>Now that we have `B` and `L`, we can get `M`.</li>
        <li>Now that we have `L` and `M`, we can get `P`.</li>
        <li>Now that we have `P`, we can get `Q`.</li>
      </ol>
      <p>But wait, forward chaining and backward chaining are algorithms for implications. What do they have to do with definite clauses?</p>
      <p>Definite clauses can be converted into implications. Recall implication elimination:</p>
      <div class="ln-center">
        <p>`(alpha implies beta) -= (not alpha vv beta)`</p>
      </div>
      <p>We can use this (in the other direction) to convert definite clauses into implications.</p>
      <div class="ln-box">
        <p>How about definite clauses with more than two literals, like `not alpha vv not beta vv gamma`?</p>
        <p>Loosely, the conversion process can be thought of as "the negative literal goes before the implication (with the `not` removed) and the positive literal goes after the implication". Applying this, we get:</p>
        <div class="ln-center">
          <p>`alpha implies gamma`</p>
          <p>`beta implies gamma`</p>
        </div>
        <p>If we think about it*, we can conclude that both `alpha` <em>and</em> `beta` implies `gamma`.</p>
        <div class="ln-center">
          <p>`alpha ^^ beta implies gamma`</p>
        </div>
        <p>*For peace of mind, we can create a truth table for `((alpha implies gamma) ^^ (beta implies gamma)) implies ((alpha ^^ beta) implies gamma)` and it should be true in every row.</p>
      </div>
      <div class="ln-box">
        <h3>Summary of Chapter 7</h3>
        <ul>
          <li>Intelligent agents need knowledge about the world in order to reach good decisions.</li>
          <li>Knowledge is contained in agents in the form of <b>sentences</b> in a <b>knowledge representation language</b> that are stored in a <b>knowledge base</b>.</li>
          <li>A knowledge-based agent is composed of a knowledge base and an inference mechanism. It operates by storing sentences about the world in its knowledge base, using the inference mechanism to infer new sentences, and using these sentences to decide what action to take.</li>
          <li>A representation language is defined by its <b>syntax</b>, which specifies the structure of sentences, and its <b>semantics</b>, which defines the <b>truth</b> of each sentence in each <b>possible world</b> or <b>model</b>.</li>
          <li>The relationship of <b>entailment</b> between sentences is crucial to our understanding of reasoning. A sentence `alpha` entails another sentence `beta` if `beta` is true in all worlds where `alpha` is true. Equivalent definitions include the <b>validity</b> of the sentence `alpha implies beta` and the <b>unsatisfiability</b> of the sentence `alpha ^^ not beta`.</li>
          <li>Inference is the process of deriving new sentences from old ones. <b>Sound</b> inference algorithms derive only sentences that are entailed; <b>complete</b> algorithms derive all sentences that are entailed.</li>
          <li><b>Propositional logic</b> is a simple language consisting of <b>proposition symbols</b> and <b>logical connectives</b>. It can handle propositions that are known to be true, known to be false, or completely unknown.</li>
          <li>The set of possible models, given a fixed propositional vocabulary, is finite, so entailment can be checked by enumerating models. Efficient <b>model-checking</b> inference algorithms for propositional logic include backtracking and local search methods and can often solve large problems quickly.</li>
          <li><b>Inference rules</b> are patterns of sound inference that can be used to find proofs. The <b>resolution</b> rule yields a complete inference algorithm for knowledge bases that are expressed in <b>conjunctive normal form</b>. <b>Forward chaining</b> and <b>backward chaining</b> are very natural reasoning algorithms for knowledge bases in <b>Horn form</b>.</li>
          <li><b>Local search</b> methods such as `text(WALKSAT)` can be used to find solutions. Such algorithms are sound but not complete.</li>
          <li>Logical <b>state estimation</b> involves maintaining a logical sentence that describes the set of possible states consistent with the observation history. Each update step requires inference using the transition model of the environment, which is built from <b>successor-state axioms</b> that specify how each <b>fluent</b> changes.</li>
          <li>Decisions within a logical agent can be made by SAT solving: finding possible models specifying future action sequences that reach the goal. This approach works only for fully observable or sensorless environments.</li>
          <li>Propositional logic does not scale to environments of unbounded size because it lacks the expressive power to deal concisely with time, space, and universal patterns of relationships among objects.</li>
        </ul>
      </div>
      <h2 id="firstorderlogic">First-Order Logic</h2>
      <p>As much as we can do with propositional logic, there are some limitations to it. The biggest limitation is that we need a lot of propositions to represent some basic ideas.</p>
      <p>For example, to represent the idea that there is a breeze next to a pit, we use</p>
      <div class="ln-center">
        <p>`B_(1,1) iff (P_(1,2) vv P_(2,1))`</p>
        <p>`B_(2,1) iff (P_(1,1) vv P_(3,1) vv P_(2,2))`</p>
        <p>`B_(3,1) iff (P_(2,1) vv P_(3,2) vv P_(4,1))`</p>
        <p>`vdots`</p>
        <p>and so on for every square</p>
      </div>
      <p>To represent the idea that there is exactly one wumpus, we use</p>
      <div class="ln-center">
        <p>`W_(1,1) vv W_(1,2) vv W_(1,3) vv ...`</p>
        <p>(there is at least one wumpus)</p>
      </div>
      <p>and</p>
      <div class="ln-center">
        <p>`not W_(1,1) vv not W_(1,2)`</p>
        <p>`not W_(1,1) vv not W_(1,3)`</p>
        <p>`not W_(1,1) vv not W_(1,4)`</p>
        <p>`vdots`</p>
        <p>(there is at most one wumpus)</p>
      </div>
      <p>First-order logic solves this problem by dealing with <b>objects</b> and <b>relations</b> instead of propositions. For the wumpus world, objects would be things like squares, pits, and wumpuses. Relations would be <b>properties</b> or descriptions of those objects, like "is breezy", "is adjacent to", and "shoots".</p>
      <div class="ln-box">
        <p>Some relations can be thought of as <b>functions</b>.</p>
      </div>
      <h2 id="syntaxandsemanticsoffirstorderlogic">Syntax and Semantics of First-Order Logic</h2>
      <h3>Symbols and interpretations</h3>
      <p><b>Constant symbols</b> are used to represent objects and <b>predicate symbols</b> are used to represent relations. Some examples of constant symbols are `text(Wumpus)`, `text(Square)`, and `text(Agent)`. Some examples of predicate symbols are `text(Breezy)` (for "is breezy"), `text(Pit)` (for "is a pit"), and `text(Adjacent)` (for "is adjacent to").</p>
      <h3>Atomic sentences</h3>
      <p>We combine constant symbols and predicate symbols to form <b>atomic sentences</b>.</p>
      <p>`text(Alive(Wumpus))` is a sentence that stands for "The wumpus is alive", and this sentence can be either true or false.</p>
      <h3>Complex sentences</h3>
      <p>We can use the same connectives we saw in propositional logic here as well. For example, `not text(Alive(Wumpus)) implies not text(HasArrow(Agent))`.</p>
      <h3>Quantifiers</h3>
      <p>Now that we have the concept of objects, we can make general statements about a whole collection of objects (instead of enumerating each object one by one like we did earlier) using <b>quantifiers</b>.</p>
      <h4>Universal quantification</h4>
      <p>The <b>universal quantifier</b> `AA` is pronounced "for all". A sentence using this quantifier looks like</p>
      <div class="ln-center">
        <p>`AA x` `text(Breezy)(x) implies text(Pit)(x)`</p>
        <p>"For all objects `x`, if `x` is breezy, then `x` is a pit."</p>
        <p>[This sentence is not true in our wumpus world since the neighboring squares (not the pit itself) are breezy.]
      </div>
      <p>The `x` is a <b>variable</b>.</p>
      <h4>Existential quantification</h4>
      <p>The <b>existential quantifier</b> `EE` is pronounced "there exists".</p>
      <div class="ln-center">
        <p>`EE x` `text(Pit)(x)`</p>
        <p>"There exists an object `x` such that `x` is a pit."</p>
      </div>
      <h2 id="usingfirstorderlogic">Using First-Order Logic</h2>
      <h3>The wumpus world</h3>
      <p>The agent can perceive a stench (from the wumpus), a breeze (from a pit), a glitter (from the gold), a bump (when walking into a wall), and a scream (when the wumpus is killed). An example of a percept sentence then could look like</p>
      <div class="ln-center">
        <p>`text(Percept)([text(Stench), text(Breeze), text(Glitter), text(None), text(None)], 4)`</p>
        <p>(The `4` refers to the time at which the percept was perceived by the agent.)</p>
      </div>
      <p>To transform the percept into a fact about the wumpus world, we can do things like</p>
      <div class="ln-center">
        <p>`AA t,s,g,w,c` `text(Percept)([s, text(Breeze), g, w, c], t) implies text(Breeze(t))`</p>
        <p>For all objects `t,s,g,w,c`, if a breeze was perceived at time `t`, then there is a breeze at time `t`.</p>
        <p>`AA t,s,b,w,c` `text(Percept)([s, b, text(None), w, c], t) implies not text(Glitter(t))`</p>
        <p>For all objects `t,s,b,w,c`, if no glitter was perceived at time `t`, then there is no glitter at time `t`.</p>
      </div>
      <p>We can implement simple reflex behavior with</p>
      <div class="ln-center">
        <p>`AA t` `text(Glitter)(t) implies text(BestAction)(text(Grab), t)`</p>
        <p>If there is a glitter at time `t`, then the best action is to grab (the gold) at time `t`.</p>
      </div>
      <p>We can describe the environment concisely with</p>
      <div class="ln-center">
        <p>`AA x,y,a,b` `text(Adjacent)text(()[x,y], [a,b]text()) iff (x = a ^^ (y = b - 1 vv y = b + 1)) vv (y = b ^^ (x = a -1 vv x = a + 1))`</p>
        <p>A square at position `(x,y)` is adjacent to a square at position `(a,b)` if and only if the square is above/below it or to the left/right of it.</p>
        <p>`text(Wumpus)`</p>
        <p>There is a wumpus.</p>
      </div>
      <p>If we have a function `text(At)(x,s,t)` to mean object `x` is at square `s` at time `t`, then we can say things like</p>
      <div class="ln-center">
        <p>`AA s,t` `text(At)(text(Agent), s, t) ^^ Breeze(t) implies Breezy(s)`</p>
        <p>If the agent detects a breeze on square `s` at time `t`, then square `s` is breezy.</p>
        <p>`AA s` `text(Breezy)(s) iff EE r` `text(Adjacent)(r,s) ^^ text(Pit)(r)`</p>
        <p>A square is breezy if and only if there exists a pit next to it.</p>
      </div>
      <div class="ln-box">
        <h3>Summary of Chapter 8</h3>
        <ul>
          <li>Knowledge representation languages should be declarative, compositional, expressive, context independent, and unambiguous.</li>
          <li>Logics differ in their <b>ontological commitments</b> and <b>epistemological commitments</b>. While propositional logic commits only to the existence of facts, first-order logic commits to the existence of objects and relations and thereby gains expressive power, appropriate for domains such as the wumpus world and electronic circuits.</li>
          <li>Both propositional logic and first-order logic share a difficulty in representing vague propositions. This difficulty limits their applicability in domains that require personal judgments, like politics or cuisine.</li>
          <li>The syntax of first-order logic builds on that of propositional logic. It adds terms to represent objects, and has universal and existential quantifiers to construct assertions about all or some of the possible values of the quantified variables.</li>
          <li>A <b>possible world</b>, or <b>model</b>, for first-order logic includes a set of objects and an <b>interpretation</b> that maps constant symbols to objects, predicate symbols to relations among objects, and function symbols to functions on objects.</li>
          <li>An atomic sentence is true only when the relation named by the predicate holds between the objects named by the terms. <b>Extended interpretations</b>, which map quantifier variables to objects in the model, define the truth of quantified sentences.</li>
          <li>Developing a knowledge base in first-order logic requires a careful process of analyzing the domain, choosing a vocabulary, and encoding the axioms required to support the desired inferences.</li>
        </ul>
      </div>
      <h2 id="inferenceinfirstorderlogic">Propositional vs. First-Order Inference</h2>
      <p>So how do we prove statements when these new quantifiers are involved? Well, we can convert sentences in first-order logic into sentences in propositional logic by eliminating the quantifiers.</p>
      <p>Eliminating universal quantifiers is fairly simple; we just replace the variables with objects. For example, let's say we have the sentence "All greedy kings are evil.":</p>
      <div class="ln-center">
        <p>`AA x` `text(King)(x) ^^ text(Greedy)(x) implies text(Evil)(x)`</p>
      </div>
      <p>Then we can create new sentences for all people that exist:</p>
      <div class="ln-center">
        <p>`text(King)(text(John)) ^^ text(Greedy)(text(John)) implies text(Evil)(text(John))`</p>
        <p>`text(King)(text(Richard)) ^^ text(Greedy)(text(Richard)) implies text(Evil)(text(Richard))`</p>
        <p>`vdots`</p>
      </div>
      <p>This is <b>Universal Instantiation</b>.</p>
      <p><b>Existential Instantiation</b> is similar except we replace the variables with a new constant symbol. For example, the sentence</p>
      <div class="ln-center">
        <p>`EE x` `text(Crown)(x) ^^ text(OnHead)(x, text(John))`</p>
      </div>
      <p>becomes</p>
      <div class="ln-center">
        <p>`text(Crown)(C_1) ^^ text(OnHead)(C_1, text(John))`</p>
      </div>
      <p>by replacing `x` with `C_1`, where `C_1` is a symbol that represents an object.</p>
      <div class="ln-box">
        <p>The new constant symbol is called a <b>Skolem constant</b>.</p>
      </div>
      <h3>Reduction to propositional inference</h3>
      <p>Then we replace predicate symbols (that have no variables) with proposition symbols. So `text(King)(text(John))` becomes something like `text(JohnIsKing)`.</p>
      <p>This whole process is called <b>propositionalization</b>.</p>
      <h2 id="forwardchaining">Forward Chaining</h2>
      <h3>First-order definite clauses</h3>
      <p>Like we saw before, a definite clause in first-order logic is a disjunction of literals where exactly one of the literals is positive (they can be in the form of an implication after conversion). Existential quantifiers are not allowed, but universal quantifiers are. So `text(King)(x) ^^ text(Greedy)(x) implies text(Evil)(x)` is a definite clause, where the `AA x` is implicit.</p>
      <p>Let's look at the following problem: The law says that it is a crime for an American to sell weapons to hostile nations. The country Nono, an enemy of America, has some missiles, and all of its missiles were sold to it by Colonel West, who is American. We can use forward chaining to prove that West is a criminal.</p>
      <p>First we populate the knowledge base with definite clauses. "... it is a crime for an American to sell weapons to hostile nations" becomes</p>
      <div class="ln-center">
        <p>`text(American)(x) ^^ text(Weapon)(y) ^^ text(Sells)(x,y,z) ^^ text(Hostile)(z) implies text(Criminal)(x)`</p>
      </div>
      <p>"Nono ... has some missles" is `EE x` `text(Owns)(text(Nono), x) ^^ text(Missle)(x)`. Using Existential Instantiation, we get</p>
      <div class="ln-center">
        <p>`text(Owns)(text(Nono),M_1) ^^ text(Missle)(M_1)`</p>
      </div>
      <p>"All of its missles were sold to it by Colonel West" becomes</p>
      <div class="ln-center">
        <p>`text(Missle)(x) ^^ text(Owns)(text(Nono), x) implies text(Sells)(text(West), x, text(Nono))`</p>
      </div>
      <p>"West, who is an American" becomes</p>
      <div class="ln-center">
        <p>`text(American)(text(West))`</p>
      </div>
      <p>"The country Nono, an enemy of America" becomes</p>
      <div class="ln-center">
        <p>`text(Enemy)(text(Nono), text(America))`</p>
      </div>
      <p>We also need to establish that a missle is a weapon:</p>
      <div class="ln-center">
        <p>`text(Missle)(x) implies text(Weapon)(x)`</p>
      </div>
      <p>and an enemy of America is hostile:</p>
      <div class="ln-center">
        <p>`text(Enemy)(x, text(America)) implies text(Hostile)(x)`</p>
      </div>
      <h3>A simple forward-chaining algorithm</h3>
      <p>To prove that West is a criminal, we need to show `text(American)(text(West)) ^^ text(Weapon)(M_1) ^^ text(Sells)(text(West), M_1, text(Nono)) ^^ text(Hostile)(text(Nono)) implies text(Criminal)(text(West))`.</p>
      <ul>
        <li>`text(American)(text(West))` is given</li>
        <li>`text(Weapon)(M_1)`
          <ul>
            <li>`text(Owns)(text(Nono),M_1) ^^ text(Missle)(M_1)` gives us `text(Missle)(M_1)`</li>
            <li>`text(Missle)(M_1) implies text(Weapon)(M_1)` gives us `text(Weapon)(M_1)`</li>
          </ul>
        </li>
        <li>`text(Sells)(text(West), M_1, text(Nono))`
          <ul>
            <li>`text(Owns)(text(Nono),M_1) ^^ text(Missle)(M_1)` gives us `text(Sells)(text(West), M_1, text(Nono))`</li>
          </ul>
        </li>
        <li>`text(Hostile)(text(Nono))`
          <ul>
            <li>`text(Enemy)(text(Nono), text(America))` and `text(Enemy)(x, text(America)) implies text(Hostile)(x)` give us `text(Hostile)(text(Nono))`</li>
          </ul>
        </li>
      </ul>
      <p>Now that we have shown the four premises are true, we can use forward chaining to get `text(Criminal)(text(West))`.</p>
      <h2 id="resolution">Resolution</h2>
      <h3>Conjunctive normal form for first-order logic</h3>
      <p>Like we saw before, a sentence in <b>conjunctive normal form</b> (CNF) is a conjunction of clauses (recall that a clause is a disjunction of literals). The literals can contain variables, which are assumed to be universally quantified.</p>
      <p>The sentence `AA x,y,z` `text(American)(x) ^^ text(Weapon)(y) ^^ text(Sells)(x,y,z) ^^ text(Hostile)(z) implies text(Criminal)(x)` becomes</p>
      <div class="ln-center">
        <p>`not text(American)(x) vv not text(Weapon)(y) vv not text(Sells)(x,y,z) vv not text(Hostile)(z) vv text(Criminal)(x)`</p>
      </div>
      <p>Every sentence of first-order logic can be converted into a CNF sentence.</p>
      <p>Converting a sentence to CNF is similar to the propositional case; we just need to handle the quantifiers and the variables.</p>
    </div>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>
</html>
